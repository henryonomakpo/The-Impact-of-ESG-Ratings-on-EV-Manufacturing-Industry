{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpoGld0u9YnRfYZy/DaY+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henryonomakpo/The-Impact-of-ESG-Ratings-on-EV-Manufacturing-Industry/blob/main/ESG_Monthly_to_Daily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5UU1FpQ27vs",
        "outputId": "41fa350e-b139-4343-a8a1-0d251766f649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.63)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting linearmodels\n",
            "  Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.11.4)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting mypy-extensions>=0.4 (from linearmodels)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Collecting pyhdfe>=0.1 (from linearmodels)\n",
            "  Downloading pyhdfe-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting formulaic>=1.0.0 (from linearmodels)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting setuptools-scm<9.0.0,>=8.0.0 (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.6.15)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=1.0.0->linearmodels)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyhdfe-0.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: xlsxwriter, setuptools-scm, mypy-extensions, interface-meta, pyhdfe, formulaic, linearmodels\n",
            "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 linearmodels-6.1 mypy-extensions-1.1.0 pyhdfe-0.2.0 setuptools-scm-8.3.1 xlsxwriter-3.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance statsmodels pandas numpy scikit-learn xlsxwriter linearmodels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create the initial DataFrame with the annual data provided\n",
        "data = {\n",
        "    'Company': ['Wayfair', 'Carvana', 'Tripadvisor', 'Groupon', 'BigCommerce', 'Rakuten', 'Pinduoduo',\n",
        "                'FNAC Darty', 'Global-E', 'VTEX', 'Coupang'],\n",
        "    # Using the corrected ticker for FNAC Darty\n",
        "    'Ticker': ['W', 'CVNA', 'TRIP', 'GRPN', 'BIGC', '4755.T', 'PDD', 'FNAC.PA', 'GLBE', 'VTEX', 'CPNG'],\n",
        "    'Country': ['USA', 'USA', 'USA', 'USA', 'USA', 'Japan', 'China', 'France', 'Israel', 'Brazil', 'South Korea'],\n",
        "    '2019': [25.5, 35.1, 24.8, 26.9, np.nan, 26.2, 32.4, np.nan, np.nan, np.nan, np.nan],\n",
        "    '2020': [24.1, 34.2, 23.5, 26.1, np.nan, 25.0, 30.1, np.nan, np.nan, np.nan, np.nan],\n",
        "    '2021': [22.9, 33.0, 22.1, 25.2, 21.5, 24.1, 28.0, np.nan, np.nan, np.nan, np.nan],\n",
        "    '2022': [22.0, 31.8, 21.3, 24.5, 20.3, 23.3, 25.9, np.nan, np.nan, np.nan, np.nan],\n",
        "    '2023': [21.5, 31.1, 20.8, 24.0, 19.4, 22.7, 24.5, np.nan, np.nan, np.nan, np.nan],\n",
        "    '2024': [21.1, 30.2, 20.2, 23.6, 18.5, 22.1, 23.4, 12.4, 24.9, np.nan, 26.0],\n",
        "    # 2025 will be forward-filled from 2024\n",
        "    '2025': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
        "}\n",
        "annual_df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Prepare for daily data generation\n",
        "all_daily_data = []\n",
        "date_range = pd.to_datetime(pd.date_range(start='2019-01-01', end='2025-12-31', freq='D'))\n",
        "\n",
        "print(\"Generating daily ESG data for each company...\")\n",
        "\n",
        "# 3. Loop through each company to create daily data\n",
        "for index, row in annual_df.iterrows():\n",
        "    ticker = row['Ticker']\n",
        "    company_name = row['Company']\n",
        "\n",
        "    # Create a temporary dataframe with the daily date range\n",
        "    daily_company_df = pd.DataFrame(index=date_range)\n",
        "    daily_company_df['Ticker'] = ticker\n",
        "\n",
        "    # --- FIX: Initialize the ESG_Score column with NaN ---\n",
        "    # This ensures the column always exists, even if no scores are available for the initial years.\n",
        "    daily_company_df['ESG_Score'] = np.nan\n",
        "\n",
        "    # Place the annual ESG scores at the start of each year\n",
        "    for year in range(2019, 2026):\n",
        "        score = row.get(str(year))\n",
        "        if pd.notna(score):\n",
        "            # Set the score on January 1st of that year\n",
        "            daily_company_df.loc[f'{year}-01-01', 'ESG_Score'] = score\n",
        "\n",
        "    # Interpolate to create smooth daily values between years\n",
        "    # and forward-fill to project the last known score into the future\n",
        "    daily_company_df['ESG_Score'] = daily_company_df['ESG_Score'].interpolate(method='linear').ffill()\n",
        "\n",
        "    # Backward-fill any initial NaNs (for companies with no early data)\n",
        "    daily_company_df['ESG_Score'] = daily_company_df['ESG_Score'].bfill()\n",
        "\n",
        "    all_daily_data.append(daily_company_df)\n",
        "\n",
        "# 4. Combine all companies into a single DataFrame\n",
        "final_daily_df = pd.concat(all_daily_data)\n",
        "final_daily_df.reset_index(inplace=True)\n",
        "final_daily_df.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "# Reorder columns for clarity\n",
        "final_daily_df = final_daily_df[['Date', 'Ticker', 'ESG_Score']]\n",
        "\n",
        "# 5. Save the final dataset to an Excel file in Google Colab\n",
        "output_filename = 'daily_ecommerce_esg_data.xlsx'\n",
        "final_daily_df.to_excel(output_filename, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully generated daily ESG data for {len(annual_df)} companies.\")\n",
        "print(f\"File '{output_filename}' has been saved to your Colab environment.\")\n",
        "print(\"\\n--- First 5 rows of the generated daily dataset ---\")\n",
        "print(final_daily_df.head())\n",
        "print(\"\\n--- Last 5 rows of the generated daily dataset ---\")\n",
        "print(final_daily_df.tail())\n",
        "print(f\"\\nTotal rows in dataset: {len(final_daily_df)}\")\n",
        "\n",
        "# You can download the file from the Colab file explorer on the left sidebar."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7AxGxoa28r0",
        "outputId": "4a795315-81b3-4132-f098-27a51b62601d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating daily ESG data for each company...\n",
            "\n",
            "Successfully generated daily ESG data for 11 companies.\n",
            "File 'daily_ecommerce_esg_data.xlsx' has been saved to your Colab environment.\n",
            "\n",
            "--- First 5 rows of the generated daily dataset ---\n",
            "        Date Ticker  ESG_Score\n",
            "0 2019-01-01      W  25.500000\n",
            "1 2019-01-02      W  25.496164\n",
            "2 2019-01-03      W  25.492329\n",
            "3 2019-01-04      W  25.488493\n",
            "4 2019-01-05      W  25.484658\n",
            "\n",
            "--- Last 5 rows of the generated daily dataset ---\n",
            "            Date Ticker  ESG_Score\n",
            "28122 2025-12-27   CPNG       26.0\n",
            "28123 2025-12-28   CPNG       26.0\n",
            "28124 2025-12-29   CPNG       26.0\n",
            "28125 2025-12-30   CPNG       26.0\n",
            "28126 2025-12-31   CPNG       26.0\n",
            "\n",
            "Total rows in dataset: 28127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert ESG dataset from Monthly to Daily"
      ],
      "metadata": {
        "id": "nuESzK7dWtck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# The path to your uploaded monthly ESG data file\n",
        "INPUT_CSV_PATH = '/content/historic_esg_scores_ecommerce.csv'\n",
        "# The name for the final daily dataset\n",
        "OUTPUT_EXCEL_PATH = 'daily_ecommerce_esg_data_final.xlsx'\n",
        "\n",
        "# --- Main Script ---\n",
        "print(f\"--- Step 1: Loading Monthly ESG Data from '{INPUT_CSV_PATH}' ---\")\n",
        "\n",
        "try:\n",
        "    # --- FIX 1: Let pandas auto-detect the separator by removing sep='\\t' ---\n",
        "    # This is more robust than assuming it's a tab.\n",
        "    # The 'engine='python'' argument helps with a wider variety of separators.\n",
        "    monthly_df = pd.read_csv(INPUT_CSV_PATH, engine='python')\n",
        "\n",
        "    # --- FIX 2: Clean up column names to remove leading/trailing spaces ---\n",
        "    monthly_df.columns = monthly_df.columns.str.strip()\n",
        "\n",
        "    # --- FIX 3: Check for the 'Date' column *after* loading ---\n",
        "    if 'Date' not in monthly_df.columns:\n",
        "        print(\"FATAL ERROR: 'Date' column not found after loading the CSV.\")\n",
        "        print(\"Please check the exact column name for dates in your file.\")\n",
        "        print(f\"Columns found: {monthly_df.columns.tolist()}\")\n",
        "        sys.exit()\n",
        "\n",
        "    monthly_df['Date'] = pd.to_datetime(monthly_df['Date'])\n",
        "    print(\"Monthly data loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATAL ERROR: The file was not found at '{INPUT_CSV_PATH}'.\")\n",
        "    print(\"Please make sure you have uploaded the file to your Colab session.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Rename columns to a consistent format\n",
        "monthly_df.rename(columns={\n",
        "    'ESG_Total_Score': 'ESG',\n",
        "    'E-Score': 'E',\n",
        "    'S-Score': 'S',\n",
        "    'G-Score': 'G'\n",
        "}, inplace=True)\n",
        "\n",
        "# Ensure data types are correct for interpolation\n",
        "score_cols = ['ESG', 'E', 'S', 'G']\n",
        "for col in score_cols:\n",
        "    monthly_df[col] = pd.to_numeric(monthly_df[col], errors='coerce')\n",
        "\n",
        "\n",
        "print(\"\\n--- Step 2: Upsampling Monthly Data to Daily Frequency ---\")\n",
        "all_daily_data = []\n",
        "tickers = monthly_df['Ticker'].unique()\n",
        "\n",
        "for ticker in tickers:\n",
        "    # Isolate the data for one company\n",
        "    company_df = monthly_df[monthly_df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # Set the 'Date' as the index, which is required for resampling\n",
        "    company_df.set_index('Date', inplace=True)\n",
        "\n",
        "    # Upsample to daily frequency and interpolate\n",
        "    # 'interpolate(method='linear')' creates a smooth daily transition between monthly data points.\n",
        "    daily_df = company_df.resample('D').interpolate(method='linear')\n",
        "\n",
        "    # The ticker column gets lost during resampling, so we add it back\n",
        "    daily_df['Ticker'] = ticker\n",
        "\n",
        "    all_daily_data.append(daily_df)\n",
        "\n",
        "# Combine the daily data for all companies into a single DataFrame\n",
        "final_daily_df = pd.concat(all_daily_data)\n",
        "\n",
        "# Forward-fill any remaining gaps, especially at the end of the series\n",
        "final_daily_df.ffill(inplace=True)\n",
        "# Backward-fill any remaining gaps at the beginning\n",
        "final_daily_df.bfill(inplace=True)\n",
        "\n",
        "\n",
        "# Reset the index to turn 'Date' back into a column\n",
        "final_daily_df.reset_index(inplace=True)\n",
        "final_daily_df.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "# Reorder columns to the desired format\n",
        "final_daily_df = final_daily_df[['Date', 'Ticker', 'ESG', 'E', 'S', 'G']]\n",
        "\n",
        "print(\"\\n--- Step 3: Saving Daily Data to Excel ---\")\n",
        "final_daily_df.to_excel(OUTPUT_EXCEL_PATH, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully generated daily ESG data for {len(tickers)} companies.\")\n",
        "print(f\"File '{OUTPUT_EXCEL_PATH}' has been saved to your Colab environment.\")\n",
        "print(\"\\n--- Data Sample (First 5 Rows) ---\")\n",
        "print(final_daily_df.head())\n",
        "print(\"\\n--- Data Sample (Last 5 Rows) ---\")\n",
        "print(final_daily_df.tail())\n",
        "print(f\"\\nTotal rows in the new daily dataset: {len(final_daily_df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5mWEDoSW1rk",
        "outputId": "5e6bca35-ded3-437c-bb6c-2055e15e780e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading Monthly ESG Data from '/content/historic_esg_scores_ecommerce.csv' ---\n",
            "Monthly data loaded successfully.\n",
            "\n",
            "--- Step 2: Upsampling Monthly Data to Daily Frequency ---\n",
            "\n",
            "--- Step 3: Saving Daily Data to Excel ---\n",
            "\n",
            "Successfully generated daily ESG data for 12 companies.\n",
            "File 'daily_ecommerce_esg_data_final.xlsx' has been saved to your Colab environment.\n",
            "\n",
            "--- Data Sample (First 5 Rows) ---\n",
            "        Date Ticker   ESG     E      S      G\n",
            "0 2019-01-01   AMZN  30.6  5.85  13.74  11.01\n",
            "1 2019-01-02   AMZN  30.6  5.85  13.74  11.01\n",
            "2 2019-01-03   AMZN  30.6  5.85  13.74  11.01\n",
            "3 2019-01-04   AMZN  30.6  5.85  13.74  11.01\n",
            "4 2019-01-05   AMZN  30.6  5.85  13.74  11.01\n",
            "\n",
            "--- Data Sample (Last 5 Rows) ---\n",
            "            Date  Ticker    ESG     E     S     G\n",
            "22217 2025-03-28  4755.T  20.93  2.83  9.48  8.62\n",
            "22218 2025-03-29  4755.T  20.93  2.83  9.48  8.62\n",
            "22219 2025-03-30  4755.T  20.93  2.83  9.48  8.62\n",
            "22220 2025-03-31  4755.T  20.93  2.83  9.48  8.62\n",
            "22221 2025-04-01  4755.T  20.93  2.83  9.48  8.62\n",
            "\n",
            "Total rows in the new daily dataset: 22,222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge the two daily ESG datasets"
      ],
      "metadata": {
        "id": "80L-JF5PafAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# --- Configuration ---\n",
        "# Input file paths\n",
        "NEW_DAILY_FILE = '/content/daily_ecommerce_esg_data.csv'\n",
        "ORIGINAL_MONTHLY_FILE = '/content/historic_esg_scores_ecommerce.csv'\n",
        "\n",
        "# Output file name\n",
        "MERGED_OUTPUT_FILE = 'merged_daily_esg_data_final.xlsx'\n",
        "\n",
        "# --- Define the standard column names we will use everywhere ---\n",
        "CANONICAL_COLUMNS = ['Date', 'ESG_Total_Score', 'E_Score', 'S_Score', 'G_Score', 'Ticker']\n",
        "\n",
        "# --- NEW: A Highly Robust Parser for the Historic File ---\n",
        "def load_and_parse_historic_file(filepath):\n",
        "    \"\"\"\n",
        "    Attempts to load the historic ESG data file using multiple common parsing strategies.\n",
        "    Returns a pandas DataFrame on success, or an empty DataFrame on failure.\n",
        "    \"\"\"\n",
        "    # --- Strategy 1: Try reading as a standard comma-separated CSV ---\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, sep=',')\n",
        "        if df.shape[1] > 2:\n",
        "            print(\"✅ Successfully parsed historic file using COMMA delimiter.\")\n",
        "            return df\n",
        "    except Exception:\n",
        "        print(\"⚠️ Comma-separated parsing failed. Trying next method...\")\n",
        "\n",
        "    # --- Strategy 2: Try reading with whitespace as the delimiter ---\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, delim_whitespace=True)\n",
        "        if df.shape[1] > 2:\n",
        "            print(\"✅ Successfully parsed historic file using WHITESPACE delimiter.\")\n",
        "            return df\n",
        "    except Exception:\n",
        "        print(\"⚠️ Whitespace parsing failed. Trying next method...\")\n",
        "\n",
        "    # --- Strategy 3: Manual line-by-line parsing with Regex (most robust) ---\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        data_rows = []\n",
        "        pattern = re.compile(r\"(\\S+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+(\\S+)\")\n",
        "        for line in lines[1:]: # Skip header\n",
        "            match = pattern.search(line)\n",
        "            if match:\n",
        "                data_rows.append(list(match.groups()))\n",
        "        if data_rows:\n",
        "            df = pd.DataFrame(data_rows, columns=['Date', 'ESG_Total_Score', 'E_Score', 'S_Score', 'G_Score', 'Ticker'])\n",
        "            print(\"✅ Successfully parsed historic file using MANUAL line-by-line Regex method.\")\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Manual parsing failed: {e}\")\n",
        "\n",
        "    print(\"❌ All parsing methods failed for the historic file.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# --- Reusable Function to Upsample to Daily ---\n",
        "def convert_to_daily(df):\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "    df.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "    score_cols = ['ESG_Total_Score', 'E_Score', 'S_Score', 'G_Score']\n",
        "    for col in score_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    all_daily_data = []\n",
        "    for ticker in df['Ticker'].unique():\n",
        "        company_df = df[df['Ticker'] == ticker].copy().drop_duplicates(subset=['Date']).set_index('Date')\n",
        "        daily_index = pd.date_range(start=df['Date'].min(), end='2025-12-31', freq='D')\n",
        "        daily_df = company_df.reindex(daily_index)\n",
        "        daily_df[score_cols] = daily_df[score_cols].interpolate(method='linear')\n",
        "        daily_df['Ticker'] = ticker\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # The chained call was causing the error because ffill(inplace=True) returns None.\n",
        "        # Perform the operations on separate lines.\n",
        "        daily_df.ffill(inplace=True)\n",
        "        daily_df.bfill(inplace=True)\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        all_daily_data.append(daily_df)\n",
        "\n",
        "    return pd.concat(all_daily_data).reset_index().rename(columns={'index': 'Date'})\n",
        "\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load the primary daily data file\n",
        "print(f\"--- Step 1: Processing primary file '{NEW_DAILY_FILE}' ---\")\n",
        "df1 = pd.DataFrame()\n",
        "try:\n",
        "    df1 = pd.read_csv(NEW_DAILY_FILE)\n",
        "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "    print(f\"Loaded {len(df1)} rows from primary file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Info: Primary file not found: {NEW_DAILY_FILE}. Proceeding without it.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {NEW_DAILY_FILE}: {e}\")\n",
        "\n",
        "# 2. Load and process the original messy monthly file using the robust parser\n",
        "print(f\"\\n--- Step 2: Processing historic file '{ORIGINAL_MONTHLY_FILE}' ---\")\n",
        "df2_monthly = load_and_parse_historic_file(ORIGINAL_MONTHLY_FILE)\n",
        "\n",
        "# Standardize column names for BOTH dataframes to ensure a perfect merge\n",
        "rename_map = {\n",
        "    'ESG': 'ESG_Total_Score', 'E': 'E_Score', 'S': 'S_Score', 'G': 'G_Score',\n",
        "    'E-Score': 'E_Score', 'S-Score': 'S_Score', 'G-Score': 'G_Score'\n",
        "}\n",
        "if not df1.empty:\n",
        "    df1.rename(columns=rename_map, inplace=True)\n",
        "if not df2_monthly.empty:\n",
        "    df2_monthly.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Convert the now-clean monthly data to daily\n",
        "df2_daily = convert_to_daily(df2_monthly)\n",
        "if not df2_daily.empty:\n",
        "    print(f\"Upsampled historic data to {len(df2_daily)} daily rows.\")\n",
        "\n",
        "# 3. Combine the two processed daily datasets\n",
        "print(\"\\n--- Step 3: Merging and Finalizing Data ---\")\n",
        "final_df = pd.concat([df1, df2_daily], ignore_index=True)\n",
        "print(f\"Total rows before deduplication: {len(final_df):,}\")\n",
        "\n",
        "# 4. De-duplicate and clean the final dataset\n",
        "if not final_df.empty:\n",
        "    # Coerce score columns to numeric, turning errors into NaNs\n",
        "    for col in ['ESG_Total_Score', 'E_Score', 'S_Score', 'G_Score']:\n",
        "        if col in final_df.columns:\n",
        "            final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "\n",
        "    final_df.sort_values(by=['Ticker', 'Date'], inplace=True)\n",
        "    final_df.drop_duplicates(subset=['Date', 'Ticker'], keep='first', inplace=True)\n",
        "\n",
        "    # Reorder DataFrame to the canonical format, keeping only existing columns\n",
        "    final_df = final_df.reindex(columns=[col for col in CANONICAL_COLUMNS if col in final_df.columns])\n",
        "\n",
        "    print(f\"Total rows after deduplication: {len(final_df):,}\")\n",
        "    print(f\"Final data shape: {final_df.shape} (rows, columns)\")\n",
        "\n",
        "    # 5. Save the final dataset\n",
        "    final_df.to_excel(MERGED_OUTPUT_FILE, index=False, engine='openpyxl')\n",
        "\n",
        "    print(f\"\\n✅ Successfully merged and saved the final dataset.\")\n",
        "    print(f\"File '{MERGED_OUTPUT_FILE}' is now available in your Colab environment.\")\n",
        "    print(\"\\n--- Final Merged Data Sample (Head) ---\")\n",
        "    print(final_df.head())\n",
        "    print(\"\\n--- Final Merged Data Sample (Tail) ---\")\n",
        "    print(final_df.tail())\n",
        "else:\n",
        "    print(\"\\nNo data was processed. No output file was created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU86py-VaklE",
        "outputId": "cac2eb32-fc71-44b4-c317-d3b434abee45"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Processing primary file '/content/daily_ecommerce_esg_data.csv' ---\n",
            "Loaded 28127 rows from primary file.\n",
            "\n",
            "--- Step 2: Processing historic file '/content/historic_esg_scores_ecommerce.csv' ---\n",
            "✅ Successfully parsed historic file using COMMA delimiter.\n",
            "Upsampled historic data to 49680 daily rows.\n",
            "\n",
            "--- Step 3: Merging and Finalizing Data ---\n",
            "Total rows before deduplication: 77,807\n",
            "Total rows after deduplication: 72,693\n",
            "Final data shape: (72693, 6) (rows, columns)\n",
            "\n",
            "✅ Successfully merged and saved the final dataset.\n",
            "File 'merged_daily_esg_data_final.xlsx' is now available in your Colab environment.\n",
            "\n",
            "--- Final Merged Data Sample (Head) ---\n",
            "            Date  ESG_Total_Score  E_Score  S_Score  G_Score  Ticker\n",
            "73667 2014-09-01             45.0     38.0     46.0     51.0  4755.T\n",
            "73668 2014-09-02             45.0     38.0     46.0     51.0  4755.T\n",
            "73669 2014-09-03             45.0     38.0     46.0     51.0  4755.T\n",
            "73670 2014-09-04             45.0     38.0     46.0     51.0  4755.T\n",
            "73671 2014-09-05             45.0     38.0     46.0     51.0  4755.T\n",
            "\n",
            "--- Final Merged Data Sample (Tail) ---\n",
            "            Date  ESG_Total_Score  E_Score  S_Score  G_Score  Ticker\n",
            "69522 2025-12-27            19.45     6.58     8.52     4.36  ZAL.DE\n",
            "69523 2025-12-28            19.45     6.58     8.52     4.36  ZAL.DE\n",
            "69524 2025-12-29            19.45     6.58     8.52     4.36  ZAL.DE\n",
            "69525 2025-12-30            19.45     6.58     8.52     4.36  ZAL.DE\n",
            "69526 2025-12-31            19.45     6.58     8.52     4.36  ZAL.DE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code to convert Renewable Energy firms ESG data from yearly to daily"
      ],
      "metadata": {
        "id": "v--3SILPgFTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_EXCEL_PATH = 'daily_renewable_energy_esg.xlsx'\n",
        "START_YEAR = 2019\n",
        "END_YEAR = 2025\n",
        "\n",
        "# --- Data Provided as a String ---\n",
        "data_string = \"\"\"\n",
        "Company Name\tTicker(s)\t2019\t2020\t2021\t2022\t2023\t2024\t2025\n",
        "NextEra Energy\tNEE\t25.5 | A\t24.1 | AA\t23.0 | AA\t22.3 | AA\t21.8 | AA\t25\t25\n",
        "Iberdrola\tIBE.MC\t18.9 | AA\t18.1 | AAA\t17.5 | AAA\t16.9 | AAA\t16.5 | AAA\t23\t23\n",
        "Enel\tENEL.MI\t20.1 | AA\t19.5 | AA\t18.8 | AA\t18.2 | AA\t17.9 | AA\t22.5\t22.5\n",
        "Ørsted\tORSTED.CO\t17.5 | AAA\t16.8 | AAA\t15.9 | AAA\t15.2 | AAA\t14.9 | AAA\t20\t20\n",
        "Brookfield Renewable\tBEP\t22.3 | A\t21.0 | A\t20.1 | AA\t19.4 | AA\t18.8 | AA\t15.8\t15.8\n",
        "Vestas Wind Systems\tVWS.CO\t19.8 | A\t19.1 | AA\t18.5 | AA\t17.9 | AA\t17.4 | AA\t16.3\t16.3\n",
        "Siemens Energy\tENR.DE\tN/A\t28.5 | BB\t27.2 | A\t26.1 | A\t25.3 | A\t14.2\t14.2\n",
        "JinkoSolar\tJKS\t28.9 | BB\t27.5 | BB\t25.4 | BBB\t24.0 | BBB\t23.1 | BBB\t27.1\t27.1\n",
        "Canadian Solar\tCSIQ\t29.1 | BB\t28.0 | BB\t26.5 | BBB\t25.0 | BBB\t24.1 | BBB\t28.5\t28.5\n",
        "First Solar\tFSLR\t18.8 | A\t17.9 | AA\t17.0 | AA\t16.3 | AA\t15.8 | AA\t17.3\t17.3\n",
        "SunPower\tSPWRQ\t25.8 | BB\t24.9 | BB\t23.5 | BB\t22.4 | BB\t21.5 | BB\t26.4\t26.4\n",
        "SolarEdge Technologies\tSEDG\t24.2 | A\t23.1 | A\t22.0 | A\t20.9 | A\t19.9 | A\t15.3\t15.3\n",
        "EDP Renováveis\tEDPR.LS\t19.5 | AA\t18.8 | AA\t18.1 | AA\t17.4 | AA\t17.0 | AA\t14.6\t14.6\n",
        "Engie\tENGI.PA\t21.2 | A\t20.5 | AA\t19.7 | AA\t19.0 | AA\t18.5 | AA\t30.2\t30.2\n",
        "Acciona Energía\tANE.MC\tN/A\tN/A\t16.5 | AA\t15.8 | AA\t15.0 | AA\t9.4\t9.4\n",
        "Ormat Technologies\tORA\t24.8 | BB\t23.9 | A\t22.9 | A\t21.8 | A\t21.1 | A\t17.8\t17.8\n",
        "Verbund AG\tVER.VI\t17.9 | AA\t17.1 | AA\t16.5 | AA\t15.8 | AA\t15.4 | AA\t18.7\t18.7\n",
        "Scatec ASA\tSCATC.OL\t23.5 | BB\t22.1 | A\t21.0 | A\t19.8 | A\t19.2 | A\t11.2\t11.2\n",
        "GE Vernova\tGEV\tN/A\tN/A\tN/A\tN/A\t22.5 | A\t21.8\t21.8\n",
        "Constellation Energy\tCEG\tN/A\tN/A\tN/A\t19.5 | AA\t18.2 | AA\t28.8\t28.8\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Function to Extract Numeric Score ---\n",
        "def extract_numeric_score(value):\n",
        "    \"\"\"\n",
        "    Cleans cells like '25.5 | A' to extract the numeric part.\n",
        "    Returns np.nan if the value is not processable.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        # Replace 'N/A' with NaN\n",
        "        if value.strip().upper() == 'N/A':\n",
        "            return np.nan\n",
        "        # Split by '|' and take the first part\n",
        "        return pd.to_numeric(value.split('|')[0].strip(), errors='coerce')\n",
        "    # If it's already a number (or NaN), just return it\n",
        "    return value\n",
        "\n",
        "# --- Reusable Function to Convert to Daily ---\n",
        "def convert_to_daily(df):\n",
        "    \"\"\"\n",
        "    Upsamples a DataFrame with annual ESG scores to a daily frequency\n",
        "    using linear interpolation.\n",
        "    \"\"\"\n",
        "    all_daily_data = []\n",
        "    tickers = df['Ticker(s)'].unique()\n",
        "\n",
        "    # Define a consistent date range for all tickers\n",
        "    date_index = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
        "\n",
        "    for ticker in tickers:\n",
        "        company_df = df[df['Ticker(s)'] == ticker].copy()\n",
        "\n",
        "        # Create a dictionary of dates and scores for this company\n",
        "        score_points = {}\n",
        "        for year_str in [str(y) for y in range(START_YEAR, END_YEAR + 1)]:\n",
        "            if year_str in company_df.columns:\n",
        "                score = company_df[year_str].iloc[0]\n",
        "                if pd.notna(score):\n",
        "                    score_points[pd.to_datetime(f'{year_str}-01-01')] = score\n",
        "\n",
        "        # Create a Series from the score points\n",
        "        if not score_points:\n",
        "            continue # Skip if no data for this ticker\n",
        "\n",
        "        score_series = pd.Series(score_points)\n",
        "\n",
        "        # Reindex to the full daily range, interpolate, and fill\n",
        "        daily_scores = score_series.reindex(date_index).interpolate(method='linear').ffill().bfill()\n",
        "\n",
        "        # Create the final daily DataFrame for this ticker\n",
        "        daily_df = pd.DataFrame(daily_scores, columns=['ESG_Score'])\n",
        "        daily_df['Ticker'] = ticker\n",
        "\n",
        "        all_daily_data.append(daily_df)\n",
        "\n",
        "    if not all_daily_data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return pd.concat(all_daily_data).reset_index().rename(columns={'index': 'Date'})\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load data from the string into a DataFrame\n",
        "print(\"--- Step 1: Loading and Cleaning Annual Data ---\")\n",
        "annual_df = pd.read_csv(io.StringIO(data_string), sep='\\t')\n",
        "print(f\"Loaded {len(annual_df)} companies.\")\n",
        "\n",
        "# 2. Extract numeric scores from all year columns\n",
        "year_cols = [str(y) for y in range(START_YEAR, END_YEAR + 1)]\n",
        "for col in year_cols:\n",
        "    if col in annual_df.columns:\n",
        "        annual_df[col] = annual_df[col].apply(extract_numeric_score)\n",
        "print(\"Cleaned and extracted numeric scores.\")\n",
        "\n",
        "# 3. Convert the cleaned annual data to a daily format\n",
        "print(\"\\n--- Step 2: Converting Annual Data to Daily Frequency ---\")\n",
        "final_daily_df = convert_to_daily(annual_df)\n",
        "\n",
        "if final_daily_df.empty:\n",
        "    print(\"FATAL ERROR: No daily data could be generated.\")\n",
        "    sys.exit()\n",
        "\n",
        "# 4. Final formatting and saving\n",
        "print(\"\\n--- Step 3: Saving Daily Data to Excel ---\")\n",
        "# Ensure the final column order is correct\n",
        "final_daily_df = final_daily_df[['Date', 'Ticker', 'ESG_Score']]\n",
        "\n",
        "final_daily_df.to_excel(OUTPUT_EXCEL_PATH, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully generated daily ESG data.\")\n",
        "print(f\"File '{OUTPUT_EXCEL_PATH}' has been saved to your Colab environment.\")\n",
        "print(\"\\n--- Data Sample (First 5 Rows) ---\")\n",
        "print(final_daily_df.head())\n",
        "print(\"\\n--- Data Sample (Last 5 Rows) ---\")\n",
        "print(final_daily_df.tail())\n",
        "print(f\"\\nTotal rows in the new daily dataset: {len(final_daily_df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzYISJwWgjQK",
        "outputId": "9d4e1d82-06fa-4519-f7fc-be1e738bc3ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Cleaning Annual Data ---\n",
            "Loaded 20 companies.\n",
            "Cleaned and extracted numeric scores.\n",
            "\n",
            "--- Step 2: Converting Annual Data to Daily Frequency ---\n",
            "\n",
            "--- Step 3: Saving Daily Data to Excel ---\n",
            "\n",
            "Successfully generated daily ESG data.\n",
            "File 'daily_renewable_energy_esg.xlsx' has been saved to your Colab environment.\n",
            "\n",
            "--- Data Sample (First 5 Rows) ---\n",
            "        Date Ticker  ESG_Score\n",
            "0 2019-01-01    NEE  25.500000\n",
            "1 2019-01-02    NEE  25.496164\n",
            "2 2019-01-03    NEE  25.492329\n",
            "3 2019-01-04    NEE  25.488493\n",
            "4 2019-01-05    NEE  25.484658\n",
            "\n",
            "--- Data Sample (Last 5 Rows) ---\n",
            "            Date Ticker  ESG_Score\n",
            "51135 2025-12-27    CEG       28.8\n",
            "51136 2025-12-28    CEG       28.8\n",
            "51137 2025-12-29    CEG       28.8\n",
            "51138 2025-12-30    CEG       28.8\n",
            "51139 2025-12-31    CEG       28.8\n",
            "\n",
            "Total rows in the new daily dataset: 51,140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code to convert yearly Transport ESG to daily ESG dataset"
      ],
      "metadata": {
        "id": "TGKEOka5j-GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the input file path in your Google Colab environment\n",
        "INPUT_FILE_PATH = '/content/Transport_esg_dataset.csv'\n",
        "OUTPUT_FILE_PATH = '/content/Transport_esg_daily_dataset.xlsx'\n",
        "\n",
        "print(f\"--- Starting Yearly to Daily Data Conversion ---\")\n",
        "print(f\"Input file: {INPUT_FILE_PATH}\")\n",
        "\n",
        "# --- Step 1: Load and Clean the Dataset ---\n",
        "try:\n",
        "    # Use delim_whitespace=True to handle variable spaces between columns robustly\n",
        "    df = pd.read_csv(INPUT_FILE_PATH)\n",
        "    print(f\"✅ Successfully loaded {len(df)} rows and {len(df.columns)} columns.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ FATAL ERROR: The file was not found at '{INPUT_FILE_PATH}'.\")\n",
        "    print(\"Please make sure you have uploaded the file to your Colab session.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"❌ FATAL ERROR: An unexpected error occurred while loading the file: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Clean and standardize column names (e.g., 'Identifier (RIC)' -> 'identifier_ric')\n",
        "df.columns = [\n",
        "    col.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
        "    for col in df.columns\n",
        "]\n",
        "\n",
        "# --- Step 2: Prepare Data for Time-Series Conversion ---\n",
        "print(\"\\n--- Step 2: Preparing data for time-series conversion ---\")\n",
        "# Convert the 'date' column (which is currently just a year) to a proper datetime object\n",
        "# We anchor each year's data to the last day of that year for proper interpolation\n",
        "df['date'] = pd.to_datetime(df['date'], format='%Y') + pd.offsets.YearEnd(0)\n",
        "\n",
        "# Identify which columns are numeric and which are not, for separate handling later\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "# We know 'date' is a datetime object, and the rest are text/objects\n",
        "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Ensure the identifier column is treated as an object/string\n",
        "if 'identifier_ric' in numeric_cols:\n",
        "    numeric_cols.remove('identifier_ric')\n",
        "    object_cols.append('identifier_ric')\n",
        "\n",
        "print(f\"Identified {len(numeric_cols)} numeric columns to interpolate.\")\n",
        "print(f\"Identified {len(object_cols)} object columns to forward-fill.\")\n",
        "\n",
        "# --- Step 3: Upsample to Daily Frequency for Each Company ---\n",
        "print(\"\\n--- Step 3: Interpolating yearly data to a daily timeline ---\")\n",
        "all_daily_dfs = []\n",
        "# Group by the company identifier to process each company's timeline separately\n",
        "for identifier, group in df.groupby('identifier_ric'):\n",
        "    # Prepare the yearly data by setting the date as the index\n",
        "    yearly_data = group.set_index('date').sort_index()\n",
        "\n",
        "    if yearly_data.empty:\n",
        "        continue # Skip if there's no data for this group\n",
        "\n",
        "    # Create a complete daily index from the first to the last date in the dataset\n",
        "    start_date = yearly_data.index.min()\n",
        "    end_date = yearly_data.index.max()\n",
        "    daily_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "    # Reindex the sparse yearly data onto the full daily index, creating NaNs for new days\n",
        "    daily_df = yearly_data.reindex(daily_index)\n",
        "\n",
        "    # --- Interpolate Numeric Columns and Forward-Fill Object Columns ---\n",
        "    # This is the core logic: smooth transitions for numbers, constant values for text\n",
        "    daily_df[numeric_cols] = daily_df[numeric_cols].interpolate(method='linear')\n",
        "    daily_df[object_cols] = daily_df[object_cols].ffill()\n",
        "\n",
        "    # Back-fill to handle any NaNs at the very beginning of the series\n",
        "    daily_df.bfill(inplace=True)\n",
        "\n",
        "    all_daily_dfs.append(daily_df)\n",
        "\n",
        "# Combine the processed daily data for all companies into a single DataFrame\n",
        "final_df = pd.concat(all_daily_dfs).reset_index()\n",
        "final_df.rename(columns={'index': 'date'}, inplace=True)\n",
        "print(f\"Daily interpolation complete. Generated {len(final_df):,} total data points.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Finalize and Save to Excel ---\n",
        "print(f\"\\n--- Step 4: Saving final dataset to '{OUTPUT_FILE_PATH}' ---\")\n",
        "# Ensure the final DataFrame has the same columns in a logical order\n",
        "final_df = final_df[df.columns]\n",
        "\n",
        "# Save the final, clean DataFrame to an Excel file\n",
        "final_df.to_excel(OUTPUT_FILE_PATH, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\n✅ Success! The daily dataset has been created and saved.\")\n",
        "print(\"You can find the file in the file browser on the left side of your Colab window.\")\n",
        "\n",
        "# Display a sample of the final data for verification\n",
        "print(\"\\n--- Sample of the Final Daily Data ---\")\n",
        "# Show data for a specific ticker to see the interpolation in action\n",
        "sample_ticker = final_df['identifier_ric'].iloc[0]\n",
        "print(final_df[final_df['identifier_ric'] == sample_ticker].head())\n",
        "print(\"...\")\n",
        "print(final_df[final_df['identifier_ric'] == sample_ticker].tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4swijjDkJwS",
        "outputId": "4fc377c6-aaf3-47da-b003-3e878a23ffce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Yearly to Daily Data Conversion ---\n",
            "Input file: /content/Transport_esg_dataset.csv\n",
            "✅ Successfully loaded 140 rows and 35 columns.\n",
            "\n",
            "--- Step 2: Preparing data for time-series conversion ---\n",
            "Identified 31 numeric columns to interpolate.\n",
            "Identified 3 object columns to forward-fill.\n",
            "\n",
            "--- Step 3: Interpolating yearly data to a daily timeline ---\n",
            "Daily interpolation complete. Generated 46,032 total data points.\n",
            "\n",
            "--- Step 4: Saving final dataset to '/content/Transport_esg_daily_dataset.xlsx' ---\n",
            "\n",
            "✅ Success! The daily dataset has been created and saved.\n",
            "You can find the file in the file browser on the left side of your Colab window.\n",
            "\n",
            "--- Sample of the Final Daily Data ---\n",
            "  identifier_ric                 company_name       date  esg_score  \\\n",
            "0            AAL  American Airlines Group Inc 2012-12-31  51.250000   \n",
            "1            AAL  American Airlines Group Inc 2013-01-01  51.237945   \n",
            "2            AAL  American Airlines Group Inc 2013-01-02  51.225890   \n",
            "3            AAL  American Airlines Group Inc 2013-01-03  51.213836   \n",
            "4            AAL  American Airlines Group Inc 2013-01-04  51.201781   \n",
            "\n",
            "   social_score  gov_score  env_score       bvps    market_cap        shares  \\\n",
            "0     70.870000  24.120000  49.640000 -63.780000  1.179771e+10  1.252310e+08   \n",
            "1     70.816274  24.138986  49.655562 -63.651151  1.179717e+10  1.253346e+08   \n",
            "2     70.762548  24.157973  49.671123 -63.522301  1.179663e+10  1.254382e+08   \n",
            "3     70.708822  24.176959  49.686685 -63.393452  1.179609e+10  1.255418e+08   \n",
            "4     70.655096  24.195945  49.702247 -63.264603  1.179555e+10  1.256454e+08   \n",
            "\n",
            "   ... women_employees  human_rights  strikes  turnover_empl  board_size  \\\n",
            "0  ...            39.0           0.0      0.0           7.14   12.000000   \n",
            "1  ...            39.0           0.0      0.0           7.14   11.997260   \n",
            "2  ...            39.0           0.0      0.0           7.14   11.994521   \n",
            "3  ...            39.0           0.0      0.0           7.14   11.991781   \n",
            "4  ...            39.0           0.0      0.0           7.14   11.989041   \n",
            "\n",
            "   shareholder_rights  board_gen_div  bribery  recycling_initiatives  \\\n",
            "0                 1.0      16.670000      1.0                    0.0   \n",
            "1                 1.0      16.674137      1.0                    0.0   \n",
            "2                 1.0      16.678274      1.0                    0.0   \n",
            "3                 1.0      16.682411      1.0                    0.0   \n",
            "4                 1.0      16.686548      1.0                    0.0   \n",
            "\n",
            "   total_assets  \n",
            "0  2.351000e+10  \n",
            "1  2.356142e+10  \n",
            "2  2.361284e+10  \n",
            "3  2.366426e+10  \n",
            "4  2.371568e+10  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "...\n",
            "     identifier_ric                 company_name       date  esg_score  \\\n",
            "3283            AAL  American Airlines Group Inc 2021-12-27  59.143644   \n",
            "3284            AAL  American Airlines Group Inc 2021-12-28  59.115233   \n",
            "3285            AAL  American Airlines Group Inc 2021-12-29  59.086822   \n",
            "3286            AAL  American Airlines Group Inc 2021-12-30  59.058411   \n",
            "3287            AAL  American Airlines Group Inc 2021-12-31  59.030000   \n",
            "\n",
            "      social_score  gov_score  env_score       bvps    market_cap  \\\n",
            "3283     64.339534  56.701808  54.418274 -11.430575  1.020300e+10   \n",
            "3284     64.327151  56.626356  54.411205 -11.422932  1.020183e+10   \n",
            "3285     64.314767  56.550904  54.404137 -11.415288  1.020065e+10   \n",
            "3286     64.302384  56.475452  54.397068 -11.407644  1.019948e+10   \n",
            "3287     64.290000  56.400000  54.390000 -11.400000  1.019831e+10   \n",
            "\n",
            "            shares  ... women_employees  human_rights   strikes  \\\n",
            "3283  6.422602e+08  ...       41.006795           1.0  0.989041   \n",
            "3284  6.426989e+08  ...       41.005096           1.0  0.991781   \n",
            "3285  6.431376e+08  ...       41.003397           1.0  0.994521   \n",
            "3286  6.435763e+08  ...       41.001699           1.0  0.997260   \n",
            "3287  6.440150e+08  ...       41.000000           1.0  1.000000   \n",
            "\n",
            "      turnover_empl  board_size  shareholder_rights  board_gen_div  bribery  \\\n",
            "3283       5.796712   10.021918                 1.0      19.963507      1.0   \n",
            "3284       5.797534   10.016438                 1.0      19.972630      1.0   \n",
            "3285       5.798356   10.010959                 1.0      19.981753      1.0   \n",
            "3286       5.799178   10.005479                 1.0      19.990877      1.0   \n",
            "3287       5.800000   10.000000                 1.0      20.000000      1.0   \n",
            "\n",
            "      recycling_initiatives  total_assets  \n",
            "3283                    0.0  6.641813e+10  \n",
            "3284                    0.0  6.643035e+10  \n",
            "3285                    0.0  6.644257e+10  \n",
            "3286                    0.0  6.645478e+10  \n",
            "3287                    0.0  6.646700e+10  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pharma ESG from yearly to daily"
      ],
      "metadata": {
        "id": "uvnsgUoytC0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_EXCEL_PATH = 'daily_health_esg_data.xlsx'\n",
        "START_YEAR = 2019\n",
        "END_YEAR = 2025\n",
        "\n",
        "# --- Data Provided as a String ---\n",
        "data_string = \"\"\"\n",
        "Company Name\tTicker(s)\t2019\t2020\t2021\t2022\t2023\t2024\t2025\n",
        "UnitedHealth Group\tUNH\t22.5 | A\t21.8 | AA\t20.9 | AA\t19.8 | AA\t18.9 | AA\t16.9\t16.9\n",
        "Johnson & Johnson\tJNJ\t26.8 | A\t25.5 | A\t24.1 | AA\t23.0 | AA\t22.2 | AA\t25.5\t25.5\n",
        "Pfizer\tPFE\t32.1 | BBB\t31.0 | A\t29.8 | A\t28.5 | A\t27.9 | A\t18.6\t18.6\n",
        "Eli Lilly & Co.\tLLY\t28.5 | BBB\t27.4 | A\t26.2 | A\t25.1 | A\t24.3 | A\t23.1\t23.1\n",
        "Thermo Fisher\tTMO\t20.5 | A\t19.8 | AA\t18.9 | AA\t18.0 | AA\t17.5 | AA\t11.9\t11.9\n",
        "Abbott Laboratories\tABT\t18.9 | AA\t18.1 | AA\t17.3 | AA\t16.5 | AA\t16.0 | AA\t20.4\t20.4\n",
        "Roche Holding AG\tRHHBY\t24.2 | AA\t23.5 | AA\t22.6 | AA\t21.5 | AA\t20.9 | AA\t24\t24\n",
        "Novartis\tNVS\t25.5 | A\t24.8 | AA\t23.9 | AA\t22.8 | AA\t22.2 | AA\t15.6\t15.6\n",
        "Merck & Co.\tMRK\t29.1 | BBB\t28.2 | A\t27.0 | A\t25.9 | A\t25.1 | A\t18.7\t18.7\n",
        "AbbVie\tABBV\t33.5 | B\t32.1 | BB\t30.8 | BBB\t29.7 | BBB\t29.0 | BBB\t24.2\t24.2\n",
        "Novo Nordisk\tNVO\t17.9 | AAA\t17.1 | AAA\t16.3 | AAA\t15.5 | AAA\t15.0 | AAA\t23.2\t23.2\n",
        "Amgen\tAMGN\t30.2 | BB\t29.4 | A\t28.1 | A\t27.0 | A\t26.5 | A\t22.5\t22.5\n",
        "Stryker\tSYK\t23.8 | A\t23.0 | AA\t22.1 | AA\t21.2 | AA\t20.5 | AA\t21.2\t21.2\n",
        "Boston Scientific\tBSX\t22.1 | A\t21.4 | AA\t20.6 | AA\t19.8 | AA\t19.2 | AA\t18.6\t18.6\n",
        "AstraZeneca\tAZN\t22.8 | AA\t22.0 | AA\t21.1 | AA\t20.4 | AA\t19.9 | AA\t23.1\t23.1\n",
        "Intuitive Surgical\tISRG\t18.5 | AA\t17.8 | AA\t16.9 | AA\t16.1 | AA\t15.5 | AA\t17.9\t17.9\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Function to Extract Numeric Score ---\n",
        "def extract_numeric_score(value):\n",
        "    \"\"\"\n",
        "    Cleans cells like '25.5 | A' to extract the numeric part.\n",
        "    Returns np.nan if the value is not processable.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        if value.strip().upper() == 'N/A':\n",
        "            return np.nan\n",
        "        # Split by '|' and take the first part, then convert to number\n",
        "        return pd.to_numeric(value.split('|')[0].strip(), errors='coerce')\n",
        "    # If it's already a number (or NaN), return it as is\n",
        "    return value\n",
        "\n",
        "# --- Reusable Function to Convert to Daily ---\n",
        "def convert_to_daily(df):\n",
        "    \"\"\"\n",
        "    Upsamples a DataFrame with annual ESG scores to a daily frequency\n",
        "    using linear interpolation.\n",
        "    \"\"\"\n",
        "    all_daily_data = []\n",
        "    tickers = df['Ticker(s)'].unique()\n",
        "\n",
        "    # Define a consistent date range for all companies\n",
        "    date_index = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
        "\n",
        "    for ticker in tickers:\n",
        "        company_df = df[df['Ticker(s)'] == ticker].copy()\n",
        "\n",
        "        # Create a dictionary of dates and scores for this company\n",
        "        score_points = {}\n",
        "        for year_str in [str(y) for y in range(START_YEAR, END_YEAR + 1)]:\n",
        "            if year_str in company_df.columns:\n",
        "                score = company_df[year_str].iloc[0]\n",
        "                if pd.notna(score):\n",
        "                    score_points[pd.to_datetime(f'{year_str}-01-01')] = score\n",
        "\n",
        "        if not score_points:\n",
        "            print(f\"Warning: No valid scores found for ticker {ticker}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        score_series = pd.Series(score_points)\n",
        "\n",
        "        # Reindex to the full daily range, interpolate, and fill gaps\n",
        "        daily_scores = score_series.reindex(date_index).interpolate(method='linear').ffill().bfill()\n",
        "\n",
        "        # Create the final daily DataFrame for this ticker\n",
        "        daily_df = pd.DataFrame(daily_scores, columns=['ESG_Score'])\n",
        "        daily_df['Ticker'] = ticker\n",
        "\n",
        "        all_daily_data.append(daily_df)\n",
        "\n",
        "    if not all_daily_data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return pd.concat(all_daily_data).reset_index().rename(columns={'index': 'Date'})\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load data from the string into a DataFrame\n",
        "print(\"--- Step 1: Loading and Cleaning Annual Data ---\")\n",
        "annual_df = pd.read_csv(io.StringIO(data_string), sep='\\t')\n",
        "# A quick fix for Novartis ticker which may be read incorrectly\n",
        "annual_df.replace({'NOT.DE': 'NVS'}, inplace=True)\n",
        "print(f\"Loaded {len(annual_df)} companies.\")\n",
        "\n",
        "# 2. Extract numeric scores from all year columns\n",
        "year_cols = [str(y) for y in range(START_YEAR, END_YEAR + 1)]\n",
        "for col in year_cols:\n",
        "    if col in annual_df.columns:\n",
        "        annual_df[col] = annual_df[col].apply(extract_numeric_score)\n",
        "print(\"Cleaned and extracted numeric scores.\")\n",
        "\n",
        "# 3. Convert the cleaned annual data to a daily format\n",
        "print(\"\\n--- Step 2: Converting Annual Data to Daily Frequency ---\")\n",
        "final_daily_df = convert_to_daily(annual_df)\n",
        "\n",
        "if final_daily_df.empty:\n",
        "    print(\"FATAL ERROR: No daily data could be generated.\")\n",
        "    sys.exit()\n",
        "\n",
        "# 4. Final formatting and saving\n",
        "print(\"\\n--- Step 3: Saving Daily Data to Excel ---\")\n",
        "# Ensure the final column order is correct\n",
        "final_daily_df = final_daily_df[['Date', 'Ticker', 'ESG_Score']]\n",
        "\n",
        "final_daily_df.to_excel(OUTPUT_EXCEL_PATH, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully generated daily ESG data.\")\n",
        "print(f\"File '{OUTPUT_EXCEL_PATH}' has been saved to your Colab environment.\")\n",
        "print(\"\\n--- Data Sample (First 5 Rows) ---\")\n",
        "print(final_daily_df.head())\n",
        "print(\"\\n--- Data Sample (Last 5 Rows) ---\")\n",
        "print(final_daily_df.tail())\n",
        "print(f\"\\nTotal rows in the new daily dataset: {len(final_daily_df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gV3XO0GtIpn",
        "outputId": "18d542a6-1da2-4b1d-d9a0-06df6fafb3b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Cleaning Annual Data ---\n",
            "Loaded 16 companies.\n",
            "Cleaned and extracted numeric scores.\n",
            "\n",
            "--- Step 2: Converting Annual Data to Daily Frequency ---\n",
            "\n",
            "--- Step 3: Saving Daily Data to Excel ---\n",
            "\n",
            "Successfully generated daily ESG data.\n",
            "File 'daily_health_esg_data.xlsx' has been saved to your Colab environment.\n",
            "\n",
            "--- Data Sample (First 5 Rows) ---\n",
            "        Date Ticker  ESG_Score\n",
            "0 2019-01-01    UNH  22.500000\n",
            "1 2019-01-02    UNH  22.498082\n",
            "2 2019-01-03    UNH  22.496164\n",
            "3 2019-01-04    UNH  22.494247\n",
            "4 2019-01-05    UNH  22.492329\n",
            "\n",
            "--- Data Sample (Last 5 Rows) ---\n",
            "            Date Ticker  ESG_Score\n",
            "40907 2025-12-27   ISRG       17.9\n",
            "40908 2025-12-28   ISRG       17.9\n",
            "40909 2025-12-29   ISRG       17.9\n",
            "40910 2025-12-30   ISRG       17.9\n",
            "40911 2025-12-31   ISRG       17.9\n",
            "\n",
            "Total rows in the new daily dataset: 40,912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code to convert monthly to yearly Pharma ESG dataset"
      ],
      "metadata": {
        "id": "lZt83zpQvE_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_CSV_PATH = '/content/historic_esg_scores_pharma_healthcare.csv'\n",
        "OUTPUT_EXCEL_PATH = 'daily_pharma_healthcare_esg.xlsx'\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "print(f\"--- Step 1: Loading Monthly ESG Data from '{INPUT_CSV_PATH}' ---\")\n",
        "\n",
        "try:\n",
        "    # --- FINAL, CORRECT FIX: The separator is a comma. ---\n",
        "    monthly_df = pd.read_csv(INPUT_CSV_PATH, sep=',')\n",
        "\n",
        "    # Clean column names to remove potential leading/trailing spaces\n",
        "    monthly_df.columns = monthly_df.columns.str.strip()\n",
        "\n",
        "    if 'Date' not in monthly_df.columns:\n",
        "        print(\"FATAL ERROR: 'Date' column not found. Please check the CSV header.\")\n",
        "        sys.exit()\n",
        "\n",
        "    monthly_df['Date'] = pd.to_datetime(monthly_df['Date'])\n",
        "    print(f\"Successfully loaded and parsed {len(monthly_df)} monthly records.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATAL ERROR: The file was not found at '{INPUT_CSV_PATH}'.\")\n",
        "    print(\"Please make sure you have uploaded the file to your Colab session.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- Step 2: Clean and Prepare Data ---\n",
        "\n",
        "# Rename columns to a consistent, code-friendly format\n",
        "monthly_df.rename(columns={\n",
        "    'Total-Score': 'ESG',\n",
        "    'E-Score': 'E',\n",
        "    'S-Score': 'S',\n",
        "    'G-Score': 'G'\n",
        "}, inplace=True)\n",
        "\n",
        "# Ensure all score columns are numeric for accurate interpolation\n",
        "score_cols = ['ESG', 'E', 'S', 'G']\n",
        "for col in score_cols:\n",
        "    if col in monthly_df.columns:\n",
        "        monthly_df[col] = pd.to_numeric(monthly_df[col], errors='coerce')\n",
        "\n",
        "print(\"Data cleaned and prepared for daily conversion.\")\n",
        "\n",
        "# --- Step 3: Upsample Monthly Data to Daily Frequency ---\n",
        "\n",
        "print(\"\\n--- Step 3: Upsampling data to a daily frequency ---\")\n",
        "\n",
        "all_daily_data = []\n",
        "tickers = monthly_df['Ticker'].unique()\n",
        "\n",
        "# Define a consistent date range for all companies\n",
        "start_date = monthly_df['Date'].min()\n",
        "end_date = pd.to_datetime('2025-12-31') # Or your desired end date\n",
        "daily_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "for ticker in tickers:\n",
        "    # Isolate the data for one company and set Date as the index\n",
        "    company_df_monthly = monthly_df[monthly_df['Ticker'] == ticker].copy()\n",
        "    company_df_monthly = company_df_monthly.drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "    # Reindex to the full daily range, creating NaNs for all missing days\n",
        "    daily_df = company_df_monthly.reindex(daily_index)\n",
        "\n",
        "    # Interpolate all numeric columns to fill the daily gaps\n",
        "    numeric_cols = daily_df.select_dtypes(include=np.number).columns\n",
        "    daily_df[numeric_cols] = daily_df[numeric_cols].interpolate(method='linear')\n",
        "\n",
        "    # Forward-fill and backward-fill to handle edges and non-numeric data\n",
        "    daily_df.ffill(inplace=True)\n",
        "    daily_df.bfill(inplace=True)\n",
        "\n",
        "    # The ticker column was dropped during reindexing, add it back\n",
        "    daily_df['Ticker'] = ticker\n",
        "\n",
        "    all_daily_data.append(daily_df)\n",
        "\n",
        "# Combine all tickers into one large DataFrame\n",
        "final_daily_df = pd.concat(all_daily_data).reset_index().rename(columns={'index': 'Date'})\n",
        "\n",
        "print(\"Daily interpolation complete.\")\n",
        "\n",
        "# --- Step 4: Save the Daily Dataset to Excel ---\n",
        "\n",
        "# Reorder columns for clarity and consistency\n",
        "final_columns_ordered = ['Date', 'Ticker', 'ESG', 'E', 'S', 'G']\n",
        "final_daily_df = final_daily_df[[col for col in final_columns_ordered if col in final_daily_df.columns]]\n",
        "\n",
        "# Save to Excel\n",
        "final_daily_df.to_excel(OUTPUT_EXCEL_PATH, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully generated the daily dataset.\")\n",
        "print(f\"File '{OUTPUT_EXCEL_PATH}' has been saved to your Colab environment.\")\n",
        "print(\"\\n--- Daily Data Sample (First 5 Rows) ---\")\n",
        "print(final_daily_df.head())\n",
        "print(\"\\n--- Daily Data Sample (Last 5 Rows) ---\")\n",
        "print(final_daily_df.tail())\n",
        "print(f\"\\nTotal rows in the new daily dataset: {len(final_daily_df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIpem232vOVb",
        "outputId": "7612a90f-3933-4d99-b7a9-68a3f1f50b75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading Monthly ESG Data from '/content/historic_esg_scores_pharma_healthcare.csv' ---\n",
            "Successfully loaded and parsed 768 monthly records.\n",
            "Data cleaned and prepared for daily conversion.\n",
            "\n",
            "--- Step 3: Upsampling data to a daily frequency ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-691462773.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  monthly_df['Date'] = pd.to_datetime(monthly_df['Date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daily interpolation complete.\n",
            "\n",
            "Successfully generated the daily dataset.\n",
            "File 'daily_pharma_healthcare_esg.xlsx' has been saved to your Colab environment.\n",
            "\n",
            "--- Daily Data Sample (First 5 Rows) ---\n",
            "        Date Ticker        ESG          E     S     G\n",
            "0 2014-09-01   ABBV  64.000000  66.000000  60.0  69.0\n",
            "1 2014-09-02   ABBV  63.933333  66.033333  59.9  69.0\n",
            "2 2014-09-03   ABBV  63.866667  66.066667  59.8  69.0\n",
            "3 2014-09-04   ABBV  63.800000  66.100000  59.7  69.0\n",
            "4 2014-09-05   ABBV  63.733333  66.133333  59.6  69.0\n",
            "\n",
            "--- Daily Data Sample (Last 5 Rows) ---\n",
            "            Date Ticker    ESG     E     S     G\n",
            "24835 2025-12-27    PFE  18.29  2.74  9.18  6.37\n",
            "24836 2025-12-28    PFE  18.29  2.74  9.18  6.37\n",
            "24837 2025-12-29    PFE  18.29  2.74  9.18  6.37\n",
            "24838 2025-12-30    PFE  18.29  2.74  9.18  6.37\n",
            "24839 2025-12-31    PFE  18.29  2.74  9.18  6.37\n",
            "\n",
            "Total rows in the new daily dataset: 24,840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Health Pharma to daily dataset"
      ],
      "metadata": {
        "id": "FLMkrRHX2Sal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# Input file paths (ensure these are uploaded to your Colab session)\n",
        "PRIMARY_FILE_PATH = '/content/daily_pharma_healthcare_esg.csv' # This file has priority\n",
        "SUPPLEMENTARY_FILE_PATH = '/content/daily_health_esg_data.csv'\n",
        "\n",
        "# Output file name for the final merged dataset\n",
        "MERGED_OUTPUT_FILE = 'merged_daily_health_esg_final.xlsx'\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load the primary dataset (more detailed)\n",
        "print(f\"--- Step 1: Loading Primary Data from '{PRIMARY_FILE_PATH}' ---\")\n",
        "df1 = pd.DataFrame() # Initialize empty DataFrame for robustness\n",
        "try:\n",
        "    df1 = pd.read_csv(PRIMARY_FILE_PATH)\n",
        "    # Standardize column names to an internal format\n",
        "    df1.rename(columns={'ESG_Total_Score': 'ESG', 'E-Score': 'E', 'S-Score': 'S', 'G-Score': 'G'}, inplace=True, errors='ignore')\n",
        "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "    print(f\"Loaded {len(df1):,} rows from primary file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Primary file '{PRIMARY_FILE_PATH}' not found. Proceeding with supplementary file only.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing primary file: {e}\")\n",
        "\n",
        "\n",
        "# 2. Load the supplementary dataset\n",
        "print(f\"\\n--- Step 2: Loading Supplementary Data from '{SUPPLEMENTARY_FILE_PATH}' ---\")\n",
        "df2 = pd.DataFrame() # Initialize empty DataFrame\n",
        "try:\n",
        "    df2 = pd.read_csv(SUPPLEMENTARY_FILE_PATH)\n",
        "    # Standardize column names to match the primary file's internal format\n",
        "    df2.rename(columns={'ESG_Score': 'ESG'}, inplace=True, errors='ignore')\n",
        "    df2['Date'] = pd.to_datetime(df2['Date'])\n",
        "    print(f\"Loaded {len(df2):,} rows from supplementary file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Supplementary file '{SUPPLEMENTARY_FILE_PATH}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing supplementary file: {e}\")\n",
        "\n",
        "# Check if we have any data to work with\n",
        "if df1.empty and df2.empty:\n",
        "    print(\"\\nFATAL ERROR: Both input files are missing or failed to load. Cannot proceed.\")\n",
        "    sys.exit()\n",
        "\n",
        "# 3. Combine the two daily datasets\n",
        "print(\"\\n--- Step 3: Merging and De-duplicating Datasets ---\")\n",
        "# Concatenate with the primary file first, so its data is kept during de-duplication\n",
        "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "print(f\"Total rows before filtering and deduplication: {len(merged_df):,}\")\n",
        "\n",
        "# 4. Filter the merged data by the specified date range\n",
        "start_date = '2019-01-01'\n",
        "end_date = '2025-06-30'\n",
        "merged_df = merged_df[(merged_df['Date'] >= start_date) & (merged_df['Date'] <= end_date)]\n",
        "print(f\"Rows after filtering for dates between {start_date} and {end_date}: {len(merged_df):,}\")\n",
        "\n",
        "# 5. De-duplicate, keeping the entry from the first (primary) file\n",
        "merged_df.sort_values(by=['Ticker', 'Date'], inplace=True)\n",
        "merged_df.drop_duplicates(subset=['Date', 'Ticker'], keep='first', inplace=True)\n",
        "print(f\"Rows after removing duplicates: {len(merged_df):,}\")\n",
        "\n",
        "# 6. Final formatting and saving\n",
        "# Ensure the final column order is correct and only includes desired columns\n",
        "final_columns = ['Date', 'Ticker', 'ESG', 'E', 'S', 'G']\n",
        "# Filter to only include columns that actually exist in the final DataFrame\n",
        "final_df = merged_df[[col for col in final_columns if col in merged_df.columns]].copy()\n",
        "\n",
        "print(f\"\\n--- Step 4: Saving Final Merged Dataset ---\")\n",
        "final_df.to_excel(MERGED_OUTPUT_FILE, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"\\nSuccessfully merged and saved the final dataset.\")\n",
        "print(f\"File '{MERGED_OUTPUT_FILE}' is now available in your Colab environment.\")\n",
        "print(\"\\n--- Final Data Sample (First 5 Rows) ---\")\n",
        "print(final_df.head())\n",
        "print(\"\\n--- Final Data Sample (Last 5 Rows) ---\")\n",
        "print(final_df.tail())\n",
        "print(f\"\\nUnique tickers in final dataset ({final_df['Ticker'].nunique()}): {sorted(final_df['Ticker'].unique())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AerTiKf2bbR",
        "outputId": "8c47baa3-6c49-4102-918a-ba9321fbcb7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading Primary Data from '/content/daily_pharma_healthcare_esg.csv' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-2806160349.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df1['Date'] = pd.to_datetime(df1['Date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 24,840 rows from primary file.\n",
            "\n",
            "--- Step 2: Loading Supplementary Data from '/content/daily_health_esg_data.csv' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-2806160349.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df2['Date'] = pd.to_datetime(df2['Date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 40,912 rows from supplementary file.\n",
            "\n",
            "--- Step 3: Merging and De-duplicating Datasets ---\n",
            "Total rows before filtering and deduplication: 65,752\n",
            "Rows after filtering for dates between 2019-01-01 and 2025-06-30: 52,206\n",
            "Rows after removing duplicates: 40,341\n",
            "\n",
            "--- Step 4: Saving Final Merged Dataset ---\n",
            "\n",
            "Successfully merged and saved the final dataset.\n",
            "File 'merged_daily_health_esg_final.xlsx' is now available in your Colab environment.\n",
            "\n",
            "--- Final Data Sample (First 5 Rows) ---\n",
            "           Date Ticker    ESG     E      S      G\n",
            "1583 2019-01-01   ABBV  30.35  0.96  16.54  12.84\n",
            "1584 2019-01-02   ABBV  30.35  0.96  16.54  12.84\n",
            "1585 2019-01-03   ABBV  30.35  0.96  16.54  12.84\n",
            "1586 2019-01-04   ABBV  30.35  0.96  16.54  12.84\n",
            "1587 2019-01-05   ABBV  30.35  0.96  16.54  12.84\n",
            "\n",
            "--- Final Data Sample (Last 5 Rows) ---\n",
            "            Date Ticker   ESG   E   S   G\n",
            "27208 2025-06-26    UNH  16.9 NaN NaN NaN\n",
            "27209 2025-06-27    UNH  16.9 NaN NaN NaN\n",
            "27210 2025-06-28    UNH  16.9 NaN NaN NaN\n",
            "27211 2025-06-29    UNH  16.9 NaN NaN NaN\n",
            "27212 2025-06-30    UNH  16.9 NaN NaN NaN\n",
            "\n",
            "Unique tickers in final dataset (17): ['ABBV', 'ABT', 'AMGN', 'AZN', 'BMY', 'BSX', 'ISRG', 'JNJ', 'LLY', 'MRK', 'NVO', 'NVS', 'PFE', 'RHHBY', 'SYK', 'TMO', 'UNH']\n"
          ]
        }
      ]
    }
  ]
}