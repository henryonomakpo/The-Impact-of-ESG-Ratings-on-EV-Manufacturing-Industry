{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu2PFzdZV9ThlU62/l2FkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henryonomakpo/The-Impact-of-ESG-Ratings-on-EV-Manufacturing-Industry/blob/main/Panel_Regression_ESG_data_for_Transportation%2C_3PL_providers%2C_and_Courier_service_sector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required libraries, yfinance, statsmodels, pandas, numpy, scikit-learn, xlsxwriter, linearmodels\n",
        "!pip install yesg\n",
        "!pip install yfinance\n",
        "!pip install statsmodels\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install xlsxwriter\n",
        "!pip install linearmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-X8hNeUto7p",
        "outputId": "e0539d5d-7692-4c1a-c093-b194c73fff14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yesg\n",
            "  Downloading yesg-2.1.1.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: yesg\n",
            "  Building wheel for yesg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yesg: filename=yesg-2.1.1-py3-none-any.whl size=6105 sha256=b174c59a864eca96b8dd0a568c66b5e0c87a4641ed7d61b7a48cac2585b52579\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/8d/48/f5e8ff0315a46301e15c68371e297b460b33e1c846117725bc\n",
            "Successfully built yesg\n",
            "Installing collected packages: yesg\n",
            "Successfully installed yesg-2.1.1\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.56)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.3\n",
            "Collecting linearmodels\n",
            "  Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.15.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.14.4)\n",
            "Collecting mypy-extensions>=0.4 (from linearmodels)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Collecting pyhdfe>=0.1 (from linearmodels)\n",
            "  Downloading pyhdfe-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting formulaic>=1.0.0 (from linearmodels)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting setuptools-scm<9.0.0,>=8.0.0 (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=1.0.0->linearmodels)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n",
            "Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyhdfe-0.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: setuptools-scm, mypy-extensions, interface-meta, pyhdfe, formulaic, linearmodels\n",
            "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 linearmodels-6.1 mypy-extensions-1.1.0 pyhdfe-0.2.0 setuptools-scm-8.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch ESG data for Transportation, 3PL providers, and Courier service sector"
      ],
      "metadata": {
        "id": "MejjxhSNtO-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKIZCEBstOEJ",
        "outputId": "b02dfee0-072a-403b-e803-80b1f61d56b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ESG Data Fetching Script for Global Transportation Firms ---\n",
            "\n",
            "Attempting to mount Google Drive at /content/drive...\n",
            "Failed to mount Google Drive: mount failed\n",
            "Output CSV will be saved locally as 'historic_esg_scores_global_transportation.csv'.\n",
            "\n",
            "Tickers to fetch ESG data for (15 total): ['UPS', 'FDX', 'DPSGY', 'AMKBY', 'KHNGY', 'DSDVY', 'UNP', 'CNI', 'CP', 'CSX', 'XPO', 'ODFL', 'JBHT', 'ZIM', 'JYD']\n",
            "Starting ESG data download loop...\n",
            "WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\n",
            "Note: ESG data is typically published annually, so expect one data point per year if available.\n",
            "  -> Processing: UPS (United Parcel Service)\n",
            "    -> Success: Found 128 historic ESG data points for UPS (United Parcel Service).\n",
            "  -> Processing: FDX (FedEx Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for FDX (FedEx Corporation).\n",
            "  -> Processing: DPSGY (Deutsche Post DHL Group)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for DPSGY (Deutsche Post DHL Group).\n",
            "  -> Processing: AMKBY (A.P. Møller - Mærsk)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for AMKBY (A.P. Møller - Mærsk).\n",
            "  -> Processing: KHNGY (Kuehne + Nagel International)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for KHNGY (Kuehne + Nagel International).\n",
            "  -> Processing: DSDVY (DSV A/S)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for DSDVY (DSV A/S).\n",
            "  -> Processing: UNP (Union Pacific Corp.)\n",
            "    -> Success: Found 128 historic ESG data points for UNP (Union Pacific Corp.).\n",
            "  -> Processing: CNI (Canadian National Railway)\n",
            "    -> Success: Found 128 historic ESG data points for CNI (Canadian National Railway).\n",
            "  -> Processing: CP (Canadian Pacific Kansas City)\n",
            "    -> Success: Found 128 historic ESG data points for CP (Canadian Pacific Kansas City).\n",
            "  -> Processing: CSX (CSX Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for CSX (CSX Corporation).\n",
            "  -> Processing: XPO (XPO, Inc.)\n",
            "    -> Success: Found 6 historic ESG data points for XPO (XPO, Inc.).\n",
            "  -> Processing: ODFL (Old Dominion Freight Line)\n",
            "    -> Success: Found 7 historic ESG data points for ODFL (Old Dominion Freight Line).\n",
            "  -> Processing: JBHT (J.B. Hunt Transport)\n",
            "    -> Success: Found 128 historic ESG data points for JBHT (J.B. Hunt Transport).\n",
            "  -> Processing: ZIM (ZIM Integrated Shipping)\n",
            "    -> Success: Found 6 historic ESG data points for ZIM (ZIM Integrated Shipping).\n",
            "  -> Processing: JYD (Jayud Global Logistics)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for JYD (Jayud Global Logistics).\n",
            "\n",
            "Combining collected historic ESG data...\n",
            "  -> Date column converted to datetime and data sorted.\n",
            "\n",
            "Preview of combined historic ESG data:\n",
            "        Date  Total-Score  E-Score  S-Score  G-Score Ticker\n",
            "0 2014-09-01         66.0     62.0     61.0     79.0    CNI\n",
            "1 2014-10-01         65.0     62.0     61.0     76.0    CNI\n",
            "2 2014-11-01         65.0     62.0     61.0     76.0    CNI\n",
            "3 2014-12-01         64.0     59.0     61.0     76.0    CNI\n",
            "4 2015-01-01         64.0     59.0     61.0     76.0    CNI\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 915 entries, 0 to 914\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   Date         915 non-null    datetime64[ns]\n",
            " 1   Total-Score  617 non-null    float64       \n",
            " 2   E-Score      617 non-null    float64       \n",
            " 3   S-Score      617 non-null    float64       \n",
            " 4   G-Score      617 non-null    float64       \n",
            " 5   Ticker       915 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(1)\n",
            "memory usage: 43.0+ KB\n",
            "\n",
            "Total rows collected: 915\n",
            "\n",
            "Saving historic ESG data to: historic_esg_scores_global_transportation.csv ...\n",
            "Historic ESG data saved successfully.\n",
            "\n",
            "--- Historic ESG Fetching Summary ---\n",
            "Successfully fetched historic ESG data for (10 tickers): ['UPS', 'FDX', 'UNP', 'CNI', 'CP', 'CSX', 'XPO', 'ODFL', 'JBHT', 'ZIM']\n",
            "Failed or no historic ESG data for (5 tickers): ['DPSGY', 'AMKBY', 'KHNGY', 'DSDVY', 'JYD']\n",
            "--- Script Finished ---\n"
          ]
        }
      ],
      "source": [
        "# Required libraries: yesg, pandas\n",
        "# Optional for Google Drive: google.colab\n",
        "# If running locally, you may need to install these:\n",
        "# !pip install yesg pandas\n",
        "\n",
        "import yesg\n",
        "import pandas as pd\n",
        "import time # To add delays between API calls\n",
        "import warnings # To potentially suppress warnings\n",
        "\n",
        "# Attempt to import and use Google Drive specific libraries only if needed\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    google_colab_available = True\n",
        "except ImportError:\n",
        "    google_colab_available = False\n",
        "    print(\"Google Colab environment not detected. Will save CSV locally.\")\n",
        "\n",
        "print(\"--- ESG Data Fetching Script for Global Transportation Firms ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# List of tickers for Global Transportation firms\n",
        "# Note: Data availability may vary significantly by ticker and source.\n",
        "TICKERS_GLOBAL_TRANSPORTATION = [\n",
        "    'UPS',    # United Parcel Service (NYSE)\n",
        "    'FDX',    # FedEx Corporation (NYSE)\n",
        "    'DPSGY',  # Deutsche Post DHL Group (OTC)\n",
        "    'AMKBY',  # A.P. Møller - Mærsk (OTC)\n",
        "    'KHNGY',  # Kuehne + Nagel International (OTC)\n",
        "    'DSDVY',  # DSV A/S (OTC)\n",
        "    'UNP',    # Union Pacific Corp. (NYSE)\n",
        "    'CNI',    # Canadian National Railway (NYSE)\n",
        "    'CP',     # Canadian Pacific Kansas City (NYSE)\n",
        "    'CSX',    # CSX Corporation (NASDAQ)\n",
        "    'XPO',    # XPO, Inc. (NYSE)\n",
        "    'ODFL',   # Old Dominion Freight Line (NASDAQ)\n",
        "    'JBHT',   # J.B. Hunt Transport (NASDAQ)\n",
        "    'ZIM',    # ZIM Integrated Shipping (NYSE)\n",
        "    'JYD',    # Jayud Global Logistics (NASDAQ) - Note: Low Market Cap, data might be sparse\n",
        "]\n",
        "\n",
        "# Optional mapping for clearer output\n",
        "TICKER_NAMES = {\n",
        "    'UPS': 'United Parcel Service', 'FDX': 'FedEx Corporation',\n",
        "    'DPSGY': 'Deutsche Post DHL Group', 'AMKBY': 'A.P. Møller - Mærsk',\n",
        "    'KHNGY': 'Kuehne + Nagel International', 'DSDVY': 'DSV A/S',\n",
        "    'UNP': 'Union Pacific Corp.', 'CNI': 'Canadian National Railway',\n",
        "    'CP': 'Canadian Pacific Kansas City', 'CSX': 'CSX Corporation',\n",
        "    'XPO': 'XPO, Inc.', 'ODFL': 'Old Dominion Freight Line',\n",
        "    'JBHT': 'J.B. Hunt Transport', 'ZIM': 'ZIM Integrated Shipping',\n",
        "    'JYD': 'Jayud Global Logistics'\n",
        "}\n",
        "\n",
        "# Define where to save the output file\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "OUTPUT_FILENAME = 'historic_esg_scores_global_transportation.csv'\n",
        "OUTPUT_PATH_DRIVE = f'{DRIVE_MOUNT_PATH}/My Drive/{OUTPUT_FILENAME}' # Standard Google Drive path\n",
        "OUTPUT_PATH_LOCAL = OUTPUT_FILENAME # Save in current directory if Drive fails\n",
        "\n",
        "# Delay between API calls (in seconds) to avoid potential blocking\n",
        "API_DELAY = 0.8 # Slightly increased delay as a precaution\n",
        "\n",
        "# --- Mount Google Drive (if in Colab) ---\n",
        "drive_mounted = False\n",
        "save_path = OUTPUT_PATH_LOCAL # Default save path\n",
        "\n",
        "if google_colab_available:\n",
        "    try:\n",
        "        print(f\"\\nAttempting to mount Google Drive at {DRIVE_MOUNT_PATH}...\")\n",
        "        # Suppress specific warnings that might appear during mounting\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            drive.mount(DRIVE_MOUNT_PATH, force_remount=True) # force_remount=True can help with issues\n",
        "\n",
        "        drive_mounted = True\n",
        "        save_path = OUTPUT_PATH_DRIVE\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        print(f\"Output CSV will be saved to Google Drive at '{save_path}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to mount Google Drive: {e}\")\n",
        "        print(f\"Output CSV will be saved locally as '{OUTPUT_PATH_LOCAL}'.\")\n",
        "else:\n",
        "    # Not in Colab, saving locally\n",
        "    print(f\"\\nOutput CSV will be saved locally as '{OUTPUT_PATH_LOCAL}'.\")\n",
        "\n",
        "\n",
        "# --- Data Fetching Loop ---\n",
        "print(f\"\\nTickers to fetch ESG data for ({len(TICKERS_GLOBAL_TRANSPORTATION)} total): {TICKERS_GLOBAL_TRANSPORTATION}\")\n",
        "print(\"Starting ESG data download loop...\")\n",
        "print(\"WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\")\n",
        "print(\"Note: ESG data is typically published annually, so expect one data point per year if available.\")\n",
        "\n",
        "\n",
        "# Initialize lists to store results and track progress\n",
        "all_esg_data_list = []\n",
        "successful_tickers = []\n",
        "failed_tickers = []\n",
        "\n",
        "for ticker in TICKERS_GLOBAL_TRANSPORTATION:\n",
        "    ticker_name = TICKER_NAMES.get(ticker, ticker) # Use mapped name if available\n",
        "    print(f\"  -> Processing: {ticker} ({ticker_name})\")\n",
        "    try:\n",
        "        # Add the delay BEFORE the API call to space them out\n",
        "        time.sleep(API_DELAY)\n",
        "        # Fetch all available historic ESG ratings for the ticker\n",
        "        esg_scores_df = yesg.get_historic_esg(ticker)\n",
        "\n",
        "        # Check if the result is a non-empty DataFrame\n",
        "        if isinstance(esg_scores_df, pd.DataFrame) and not esg_scores_df.empty:\n",
        "            # Add a column for the ticker symbol\n",
        "            esg_scores_df['Ticker'] = ticker\n",
        "            # Reset the index to make the date a column before appending\n",
        "            # The date column is typically the index after fetching\n",
        "            esg_scores_df = esg_scores_df.reset_index()\n",
        "            # Rename the date column if needed (common names are 'Date' or 'index')\n",
        "            if 'index' in esg_scores_df.columns:\n",
        "                esg_scores_df = esg_scores_df.rename(columns={'index': 'Date'})\n",
        "\n",
        "            all_esg_data_list.append(esg_scores_df)\n",
        "            successful_tickers.append(ticker)\n",
        "            print(f\"    -> Success: Found {len(esg_scores_df)} historic ESG data points for {ticker} ({ticker_name}).\")\n",
        "        else:\n",
        "            # Handle cases where yesg returns None or an empty DataFrame\n",
        "            print(f\"    -> No valid historic ESG data found/returned for {ticker} ({ticker_name}).\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions during fetching or processing\n",
        "        print(f\"    -> ERROR fetching/processing historic ESG data for {ticker} ({ticker_name}): {e}\")\n",
        "        failed_tickers.append(ticker)\n",
        "\n",
        "# --- Combine and Save Data ---\n",
        "if all_esg_data_list:\n",
        "    print(\"\\nCombining collected historic ESG data...\")\n",
        "    # Concatenate all the collected DataFrames into a single one\n",
        "    final_esg_data = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "\n",
        "    # Ensure the Date column is correctly named and formatted if possible\n",
        "    if 'Date' in final_esg_data.columns:\n",
        "        try:\n",
        "            # Attempt to convert Date column to datetime objects for consistency\n",
        "            final_esg_data['Date'] = pd.to_datetime(final_esg_data['Date'])\n",
        "            # Sort by Ticker and Date for clarity\n",
        "            final_esg_data = final_esg_data.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "            print(\"  -> Date column converted to datetime and data sorted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert 'Date' column to datetime format or sort data: {e}\")\n",
        "            print(\"Please inspect the Date column format manually.\")\n",
        "    else:\n",
        "        print(\"Warning: 'Date' column not found in combined data. Please inspect the output DataFrame structure.\")\n",
        "\n",
        "\n",
        "    # Display first few rows and info of the final DataFrame\n",
        "    print(\"\\nPreview of combined historic ESG data:\")\n",
        "    print(final_esg_data.head())\n",
        "    print(\"\\nData Info:\")\n",
        "    final_esg_data.info()\n",
        "    print(f\"\\nTotal rows collected: {len(final_esg_data)}\")\n",
        "\n",
        "\n",
        "    # Save the combined data to the chosen CSV file path\n",
        "    # Check if the save_path variable was set correctly (especially in Colab failure scenario)\n",
        "    if save_path:\n",
        "        try:\n",
        "            print(f\"\\nSaving historic ESG data to: {save_path} ...\")\n",
        "            final_esg_data.to_csv(save_path, index=False)\n",
        "            print(f\"Historic ESG data saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR saving historic ESG data to CSV at '{save_path}': {e}\")\n",
        "            if google_colab_available and save_path == OUTPUT_PATH_DRIVE:\n",
        "                print(\"Check if your Google Drive is correctly mounted and you have write permissions.\")\n",
        "    else:\n",
        "         print(\"\\nERROR: Save path was not determined. Cannot save CSV.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Message if no data was collected at all\n",
        "    print(\"\\nNo historic ESG data was successfully collected for any ticker. No CSV file created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Historic ESG Fetching Summary ---\")\n",
        "print(f\"Successfully fetched historic ESG data for ({len(successful_tickers)} tickers): {successful_tickers}\")\n",
        "print(f\"Failed or no historic ESG data for ({len(failed_tickers)} tickers): {failed_tickers}\")\n",
        "print(\"--- Script Finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 *Fetch ESG Data for Pharma Companies"
      ],
      "metadata": {
        "id": "oApsB25_yY1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries: yesg, pandas\n",
        "# Optional for Google Drive: google.colab\n",
        "# If running locally, you may need to install these:\n",
        "# !pip install yesg pandas\n",
        "\n",
        "import yesg\n",
        "import pandas as pd\n",
        "import time # To add delays between API calls\n",
        "import warnings # To potentially suppress warnings\n",
        "\n",
        "# Attempt to import and use Google Drive specific libraries only if needed\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    google_colab_available = True\n",
        "except ImportError:\n",
        "    google_colab_available = False\n",
        "    print(\"Google Colab environment not detected. Will save CSV locally.\")\n",
        "\n",
        "print(\"--- ESG Data Fetching Script for Pharmaceutical & Healthcare Firms ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# List of tickers for Pharmaceutical & Healthcare firms\n",
        "# Note: Data availability may vary significantly by ticker and source.\n",
        "TICKERS_PHARMA_HEALTHCARE = [\n",
        "    'LLY',    # Eli Lilly and Company (NYSE)\n",
        "    'JNJ',    # Johnson & Johnson (NYSE)\n",
        "    'MRK',    # Merck & Co. (NYSE)\n",
        "    'NVO',    # Novo Nordisk A/S (NYSE)\n",
        "    'RHHBY',  # Roche Holding AG (OTC)\n",
        "    'PFE',    # Pfizer Inc. (NYSE)\n",
        "    'ABBV',   # AbbVie Inc. (NYSE)\n",
        "    'NVS',    # Novartis AG (NYSE)\n",
        "    'AZN',    # AstraZeneca PLC (NASDAQ/LSE)\n",
        "    'SNY',    # Sanofi (NASDAQ/EPA)\n",
        "    'BMY',    # Bristol Myers Squibb (NYSE)\n",
        "    'GSK',    # GSK plc (NYSE/LSE)\n",
        "    'TAK',    # Takeda Pharmaceutical (NYSE)\n",
        "]\n",
        "\n",
        "# Optional mapping for clearer output\n",
        "TICKER_NAMES = {\n",
        "    'LLY': 'Eli Lilly and Company', 'JNJ': 'Johnson & Johnson',\n",
        "    'MRK': 'Merck & Co.', 'NVO': 'Novo Nordisk A/S',\n",
        "    'RHHBY': 'Roche Holding AG', 'PFE': 'Pfizer Inc.',\n",
        "    'ABBV': 'AbbVie Inc.', 'NVS': 'Novartis AG',\n",
        "    'AZN': 'AstraZeneca PLC', 'SNY': 'Sanofi',\n",
        "    'BMY': 'Bristol Myers Squibb', 'GSK': 'GSK plc',\n",
        "    'TAK': 'Takeda Pharmaceutical',\n",
        "}\n",
        "\n",
        "# Define where to save the output file\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "OUTPUT_FILENAME = 'historic_esg_scores_pharma_healthcare.csv'\n",
        "OUTPUT_PATH_DRIVE = f'{DRIVE_MOUNT_PATH}/My Drive/{OUTPUT_FILENAME}' # Standard Google Drive path\n",
        "OUTPUT_PATH_LOCAL = OUTPUT_FILENAME # Save in current directory if Drive fails\n",
        "\n",
        "# Delay between API calls (in seconds) to avoid potential blocking\n",
        "API_DELAY = 0.8 # Slightly increased delay as a precaution\n",
        "\n",
        "# --- Mount Google Drive (if in Colab) ---\n",
        "drive_mounted = False\n",
        "save_path = OUTPUT_PATH_LOCAL # Default save path\n",
        "\n",
        "if google_colab_available:\n",
        "    try:\n",
        "        print(f\"\\nAttempting to mount Google Drive at {DRIVE_MOUNT_PATH}...\")\n",
        "        # Suppress specific warnings that might appear during mounting\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            drive.mount(DRIVE_MOUNT_PATH, force_remount=True) # force_remount=True can help with issues\n",
        "\n",
        "        drive_mounted = True\n",
        "        save_path = OUTPUT_PATH_DRIVE\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        print(f\"Output CSV will be saved to Google Drive at '{save_path}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to mount Google Drive: {e}\")\n",
        "        print(f\"Output CSV will be saved locally as '{OUTPUT_PATH_LOCAL}'.\")\n",
        "else:\n",
        "    # Not in Colab, saving locally\n",
        "    print(f\"\\nOutput CSV will be saved locally as '{OUTPUT_PATH_LOCAL}'.\")\n",
        "\n",
        "\n",
        "# --- Data Fetching Loop ---\n",
        "print(f\"\\nTickers to fetch ESG data for ({len(TICKERS_PHARMA_HEALTHCARE)} total): {TICKERS_PHARMA_HEALTHCARE}\")\n",
        "print(\"Starting ESG data download loop...\")\n",
        "print(\"WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\")\n",
        "print(\"Note: ESG data is typically published annually, so expect one data point per year if available.\")\n",
        "\n",
        "\n",
        "# Initialize lists to store results and track progress\n",
        "all_esg_data_list = []\n",
        "successful_tickers = []\n",
        "failed_tickers = []\n",
        "\n",
        "for ticker in TICKERS_PHARMA_HEALTHCARE:\n",
        "    ticker_name = TICKER_NAMES.get(ticker, ticker) # Use mapped name if available\n",
        "    print(f\"  -> Processing: {ticker} ({ticker_name})\")\n",
        "    try:\n",
        "        # Add the delay BEFORE the API call to space them out\n",
        "        time.sleep(API_DELAY)\n",
        "        # Fetch all available historic ESG ratings for the ticker\n",
        "        esg_scores_df = yesg.get_historic_esg(ticker)\n",
        "\n",
        "        # Check if the result is a non-empty DataFrame\n",
        "        if isinstance(esg_scores_df, pd.DataFrame) and not esg_scores_df.empty:\n",
        "            # Add a column for the ticker symbol\n",
        "            esg_scores_df['Ticker'] = ticker\n",
        "            # Reset the index to make the date a column before appending\n",
        "            # The date column is typically the index after fetching\n",
        "            esg_scores_df = esg_scores_df.reset_index()\n",
        "            # Rename the date column if needed (common names are 'Date' or 'index')\n",
        "            if 'index' in esg_scores_df.columns:\n",
        "                esg_scores_df = esg_scores_df.rename(columns={'index': 'Date'})\n",
        "\n",
        "            all_esg_data_list.append(esg_scores_df)\n",
        "            successful_tickers.append(ticker)\n",
        "            print(f\"    -> Success: Found {len(esg_scores_df)} historic ESG data points for {ticker} ({ticker_name}).\")\n",
        "        else:\n",
        "            # Handle cases where yesg returns None or an empty DataFrame\n",
        "            print(f\"    -> No valid historic ESG data found/returned for {ticker} ({ticker_name}).\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions during fetching or processing\n",
        "        print(f\"    -> ERROR fetching/processing historic ESG data for {ticker} ({ticker_name}): {e}\")\n",
        "        failed_tickers.append(ticker)\n",
        "\n",
        "# --- Combine and Save Data ---\n",
        "if all_esg_data_list:\n",
        "    print(\"\\nCombining collected historic ESG data...\")\n",
        "    # Concatenate all the collected DataFrames into a single one\n",
        "    final_esg_data = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "\n",
        "    # Ensure the Date column is correctly named and formatted if possible\n",
        "    if 'Date' in final_esg_data.columns:\n",
        "        try:\n",
        "            # Attempt to convert Date column to datetime objects for consistency\n",
        "            final_esg_data['Date'] = pd.to_datetime(final_esg_data['Date'])\n",
        "            # Sort by Ticker and Date for clarity\n",
        "            final_esg_data = final_esg_data.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "            print(\"  -> Date column converted to datetime and data sorted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert 'Date' column to datetime format or sort data: {e}\")\n",
        "            print(\"Please inspect the Date column format manually.\")\n",
        "    else:\n",
        "        print(\"Warning: 'Date' column not found in combined data. Please inspect the output DataFrame structure.\")\n",
        "\n",
        "\n",
        "    # Display first few rows and info of the final DataFrame\n",
        "    print(\"\\nPreview of combined historic ESG data:\")\n",
        "    print(final_esg_data.head())\n",
        "    print(\"\\nData Info:\")\n",
        "    final_esg_data.info()\n",
        "    print(f\"\\nTotal rows collected: {len(final_esg_data)}\")\n",
        "\n",
        "\n",
        "    # Save the combined data to the chosen CSV file path\n",
        "    # Check if the save_path variable was set correctly (especially in Colab failure scenario)\n",
        "    if save_path:\n",
        "        try:\n",
        "            print(f\"\\nSaving historic ESG data to: {save_path} ...\")\n",
        "            final_esg_data.to_csv(save_path, index=False)\n",
        "            print(f\"Historic ESG data saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR saving historic ESG data to CSV at '{save_path}': {e}\")\n",
        "            if google_colab_available and save_path == OUTPUT_PATH_DRIVE:\n",
        "                print(\"Check if your Google Drive is correctly mounted and you have write permissions.\")\n",
        "    else:\n",
        "         print(\"\\nERROR: Save path was not determined. Cannot save CSV.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Message if no data was collected at all\n",
        "    print(\"\\nNo historic ESG data was successfully collected for any ticker. No CSV file created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Historic ESG Fetching Summary ---\")\n",
        "print(f\"Successfully fetched historic ESG data for ({len(successful_tickers)} tickers): {successful_tickers}\")\n",
        "print(f\"Failed or no historic ESG data for ({len(failed_tickers)} tickers): {failed_tickers}\")\n",
        "print(\"--- Script Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnlyUqeMykxn",
        "outputId": "2a96c114-0404-4f02-8c02-c3720b2389c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ESG Data Fetching Script for Pharmaceutical & Healthcare Firms ---\n",
            "\n",
            "Attempting to mount Google Drive at /content/drive...\n",
            "Failed to mount Google Drive: mount failed\n",
            "Output CSV will be saved locally as 'historic_esg_scores_pharma_healthcare.csv'.\n",
            "\n",
            "Tickers to fetch ESG data for (13 total): ['LLY', 'JNJ', 'MRK', 'NVO', 'RHHBY', 'PFE', 'ABBV', 'NVS', 'AZN', 'SNY', 'BMY', 'GSK', 'TAK']\n",
            "Starting ESG data download loop...\n",
            "WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\n",
            "Note: ESG data is typically published annually, so expect one data point per year if available.\n",
            "  -> Processing: LLY (Eli Lilly and Company)\n",
            "    -> Success: Found 128 historic ESG data points for LLY (Eli Lilly and Company).\n",
            "  -> Processing: JNJ (Johnson & Johnson)\n",
            "    -> Success: Found 128 historic ESG data points for JNJ (Johnson & Johnson).\n",
            "  -> Processing: MRK (Merck & Co.)\n",
            "    -> Success: Found 128 historic ESG data points for MRK (Merck & Co.).\n",
            "  -> Processing: NVO (Novo Nordisk A/S)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for NVO (Novo Nordisk A/S).\n",
            "  -> Processing: RHHBY (Roche Holding AG)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for RHHBY (Roche Holding AG).\n",
            "  -> Processing: PFE (Pfizer Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for PFE (Pfizer Inc.).\n",
            "  -> Processing: ABBV (AbbVie Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for ABBV (AbbVie Inc.).\n",
            "  -> Processing: NVS (Novartis AG)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for NVS (Novartis AG).\n",
            "  -> Processing: AZN (AstraZeneca PLC)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for AZN (AstraZeneca PLC).\n",
            "  -> Processing: SNY (Sanofi)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for SNY (Sanofi).\n",
            "  -> Processing: BMY (Bristol Myers Squibb)\n",
            "    -> Success: Found 128 historic ESG data points for BMY (Bristol Myers Squibb).\n",
            "  -> Processing: GSK (GSK plc)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for GSK (GSK plc).\n",
            "  -> Processing: TAK (Takeda Pharmaceutical)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for TAK (Takeda Pharmaceutical).\n",
            "\n",
            "Combining collected historic ESG data...\n",
            "  -> Date column converted to datetime and data sorted.\n",
            "\n",
            "Preview of combined historic ESG data:\n",
            "        Date  Total-Score  E-Score  S-Score  G-Score Ticker\n",
            "0 2014-09-01         64.0     66.0     60.0     69.0   ABBV\n",
            "1 2014-10-01         62.0     67.0     57.0     69.0   ABBV\n",
            "2 2014-11-01         66.0     74.0     62.0     66.0   ABBV\n",
            "3 2014-12-01         66.0     74.0     62.0     66.0   ABBV\n",
            "4 2015-01-01         66.0     74.0     62.0     66.0   ABBV\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   Date         768 non-null    datetime64[ns]\n",
            " 1   Total-Score  510 non-null    float64       \n",
            " 2   E-Score      510 non-null    float64       \n",
            " 3   S-Score      510 non-null    float64       \n",
            " 4   G-Score      510 non-null    float64       \n",
            " 5   Ticker       768 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(1)\n",
            "memory usage: 36.1+ KB\n",
            "\n",
            "Total rows collected: 768\n",
            "\n",
            "Saving historic ESG data to: historic_esg_scores_pharma_healthcare.csv ...\n",
            "Historic ESG data saved successfully.\n",
            "\n",
            "--- Historic ESG Fetching Summary ---\n",
            "Successfully fetched historic ESG data for (6 tickers): ['LLY', 'JNJ', 'MRK', 'PFE', 'ABBV', 'BMY']\n",
            "Failed or no historic ESG data for (7 tickers): ['NVO', 'RHHBY', 'NVS', 'AZN', 'SNY', 'GSK', 'TAK']\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch Construction Firms ESG Data"
      ],
      "metadata": {
        "id": "DWuRw6Dc4ArH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries: yesg, pandas\n",
        "# If running locally, you need to install these:\n",
        "# !pip install yesg pandas\n",
        "\n",
        "import yesg\n",
        "import pandas as pd\n",
        "import time # To add delays between API calls\n",
        "import warnings # To potentially suppress warnings\n",
        "\n",
        "print(\"--- ESG Data Fetching Script for Civil Construction, Engineering & Materials Firms ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# List of tickers for Civil Construction, Engineering & Materials firms\n",
        "# Note: Data availability may vary significantly by ticker and source.\n",
        "TICKERS_CONSTRUCTION = [\n",
        "    'DG.PA',  # VINCI SA (Euronext Paris)\n",
        "    'ACS.MC', # ACS Actividades Const. Ser. (Bolsa de Madrid)\n",
        "    'PWR',    # Quanta Services, Inc. (NYSE)\n",
        "    'HCMLY',  # Holcim Ltd. (OTC)\n",
        "    'VMC',    # Vulcan Materials Company (NYSE)\n",
        "    'MLM',    # Martin Marietta Materials (NYSE)\n",
        "    'CRH',    # CRH plc (NYSE/LSE)\n",
        "    'ACM',    # AECOM (NYSE)\n",
        "    'J',      # Jacobs Solutions Inc. (NYSE)\n",
        "    'FLR',    # Fluor Corporation (NYSE)\n",
        "    'MTZ',    # MasTec, Inc. (NYSE)\n",
        "    'HEI.DE', # Heidelberg Materials AG (Frankfurt)\n",
        "    'LT.NS',  # Larsen & Toubro Ltd. (NSE India)\n",
        "]\n",
        "\n",
        "# Optional mapping for clearer output\n",
        "TICKER_NAMES = {\n",
        "    'DG.PA': 'VINCI SA', 'ACS.MC': 'ACS Actividades Const. Ser.',\n",
        "    'PWR': 'Quanta Services, Inc.', 'HCMLY': 'Holcim Ltd.',\n",
        "    'VMC': 'Vulcan Materials Company', 'MLM': 'Martin Marietta Materials',\n",
        "    'CRH': 'CRH plc', 'ACM': 'AECOM', 'J': 'Jacobs Solutions Inc.',\n",
        "    'FLR': 'Fluor Corporation', 'MTZ': 'MasTec, Inc.',\n",
        "    'HEI.DE': 'Heidelberg Materials AG', 'LT.NS': 'Larsen & Toubro Ltd.',\n",
        "}\n",
        "\n",
        "# Define where to save the output file (locally)\n",
        "OUTPUT_FILENAME = 'historic_esg_scores_construction.csv'\n",
        "OUTPUT_PATH_LOCAL = OUTPUT_FILENAME # Save in current directory\n",
        "\n",
        "# Delay between API calls (in seconds) to avoid potential blocking\n",
        "API_DELAY = 0.8 # Slightly increased delay as a precaution\n",
        "\n",
        "# --- Data Fetching Loop ---\n",
        "print(f\"\\nTickers to fetch ESG data for ({len(TICKERS_CONSTRUCTION)} total): {TICKERS_CONSTRUCTION}\")\n",
        "print(\"Starting ESG data download loop...\")\n",
        "print(\"WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\")\n",
        "print(\"Note: ESG data is typically published annually, so expect one data point per year if available.\")\n",
        "\n",
        "\n",
        "# Initialize lists to store results and track progress\n",
        "all_esg_data_list = []\n",
        "successful_tickers = []\n",
        "failed_tickers = []\n",
        "\n",
        "for ticker in TICKERS_CONSTRUCTION:\n",
        "    ticker_name = TICKER_NAMES.get(ticker, ticker) # Use mapped name if available\n",
        "    print(f\"  -> Processing: {ticker} ({ticker_name})\")\n",
        "    try:\n",
        "        # Add the delay BEFORE the API call to space them out\n",
        "        time.sleep(API_DELAY)\n",
        "        # Fetch all available historic ESG ratings for the ticker\n",
        "        esg_scores_df = yesg.get_historic_esg(ticker)\n",
        "\n",
        "        # Check if the result is a non-empty DataFrame\n",
        "        if isinstance(esg_scores_df, pd.DataFrame) and not esg_scores_df.empty:\n",
        "            # Add a column for the ticker symbol\n",
        "            esg_scores_df['Ticker'] = ticker\n",
        "            # Reset the index to make the date a column before appending\n",
        "            # The date column is typically the index after fetching\n",
        "            esg_scores_df = esg_scores_df.reset_index()\n",
        "            # Rename the date column if needed (common names are 'Date' or 'index')\n",
        "            if 'index' in esg_scores_df.columns:\n",
        "                esg_scores_df = esg_scores_df.rename(columns={'index': 'Date'})\n",
        "\n",
        "            all_esg_data_list.append(esg_scores_df)\n",
        "            successful_tickers.append(ticker)\n",
        "            print(f\"    -> Success: Found {len(esg_scores_df)} historic ESG data points for {ticker} ({ticker_name}).\")\n",
        "        else:\n",
        "            # Handle cases where yesg returns None or an empty DataFrame\n",
        "            print(f\"    -> No valid historic ESG data found/returned for {ticker} ({ticker_name}).\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions during fetching or processing\n",
        "        print(f\"    -> ERROR fetching/processing historic ESG data for {ticker} ({ticker_name}): {e}\")\n",
        "        failed_tickers.append(ticker)\n",
        "\n",
        "# --- Combine and Save Data ---\n",
        "if all_esg_data_list:\n",
        "    print(\"\\nCombining collected historic ESG data...\")\n",
        "    # Concatenate all the collected DataFrames into a single one\n",
        "    final_esg_data = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "\n",
        "    # Ensure the Date column is correctly named and formatted if possible\n",
        "    if 'Date' in final_esg_data.columns:\n",
        "        try:\n",
        "            # Attempt to convert Date column to datetime objects for consistency\n",
        "            final_esg_data['Date'] = pd.to_datetime(final_esg_data['Date'])\n",
        "            # Sort by Ticker and Date for clarity\n",
        "            final_esg_data = final_esg_data.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "            print(\"  -> Date column converted to datetime and data sorted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert 'Date' column to datetime format or sort data: {e}\")\n",
        "            print(\"Please inspect the Date column format manually.\")\n",
        "    else:\n",
        "        print(\"Warning: 'Date' column not found in combined data. Please inspect the output DataFrame structure.\")\n",
        "\n",
        "\n",
        "    # Display first few rows and info of the final DataFrame\n",
        "    print(\"\\nPreview of combined historic ESG data:\")\n",
        "    print(final_esg_data.head())\n",
        "    print(\"\\nData Info:\")\n",
        "    final_esg_data.info()\n",
        "    print(f\"\\nTotal rows collected: {len(final_esg_data)}\")\n",
        "\n",
        "\n",
        "    # Save the combined data to the chosen CSV file path (local)\n",
        "    try:\n",
        "        print(f\"\\nSaving historic ESG data to: {OUTPUT_PATH_LOCAL} ...\")\n",
        "        final_esg_data.to_csv(OUTPUT_PATH_LOCAL, index=False)\n",
        "        print(f\"Historic ESG data saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR saving historic ESG data to CSV at '{OUTPUT_PATH_LOCAL}': {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Message if no data was collected at all\n",
        "    print(\"\\nNo historic ESG data was successfully collected for any ticker. No CSV file created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Historic ESG Fetching Summary ---\")\n",
        "print(f\"Successfully fetched historic ESG data for ({len(successful_tickers)} tickers): {successful_tickers}\")\n",
        "print(f\"Failed or no historic ESG data for ({len(failed_tickers)} tickers): {failed_tickers}\")\n",
        "print(\"--- Script Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Mnj4pA4H-J",
        "outputId": "6902186b-34c0-466d-d7db-db2a739c7c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ESG Data Fetching Script for Civil Construction, Engineering & Materials Firms ---\n",
            "\n",
            "Tickers to fetch ESG data for (13 total): ['DG.PA', 'ACS.MC', 'PWR', 'HCMLY', 'VMC', 'MLM', 'CRH', 'ACM', 'J', 'FLR', 'MTZ', 'HEI.DE', 'LT.NS']\n",
            "Starting ESG data download loop...\n",
            "WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\n",
            "Note: ESG data is typically published annually, so expect one data point per year if available.\n",
            "  -> Processing: DG.PA (VINCI SA)\n",
            "    -> Success: Found 128 historic ESG data points for DG.PA (VINCI SA).\n",
            "  -> Processing: ACS.MC (ACS Actividades Const. Ser.)\n",
            "    -> Success: Found 128 historic ESG data points for ACS.MC (ACS Actividades Const. Ser.).\n",
            "  -> Processing: PWR (Quanta Services, Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for PWR (Quanta Services, Inc.).\n",
            "  -> Processing: HCMLY (Holcim Ltd.)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for HCMLY (Holcim Ltd.).\n",
            "  -> Processing: VMC (Vulcan Materials Company)\n",
            "    -> Success: Found 128 historic ESG data points for VMC (Vulcan Materials Company).\n",
            "  -> Processing: MLM (Martin Marietta Materials)\n",
            "    -> Success: Found 128 historic ESG data points for MLM (Martin Marietta Materials).\n",
            "  -> Processing: CRH (CRH plc)\n",
            "    -> Success: Found 6 historic ESG data points for CRH (CRH plc).\n",
            "  -> Processing: ACM (AECOM)\n",
            "    -> Success: Found 128 historic ESG data points for ACM (AECOM).\n",
            "  -> Processing: J (Jacobs Solutions Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for J (Jacobs Solutions Inc.).\n",
            "  -> Processing: FLR (Fluor Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for FLR (Fluor Corporation).\n",
            "  -> Processing: MTZ (MasTec, Inc.)\n",
            "    -> Success: Found 6 historic ESG data points for MTZ (MasTec, Inc.).\n",
            "  -> Processing: HEI.DE (Heidelberg Materials AG)\n",
            "    -> Success: Found 128 historic ESG data points for HEI.DE (Heidelberg Materials AG).\n",
            "  -> Processing: LT.NS (Larsen & Toubro Ltd.)\n",
            "    -> Success: Found 109 historic ESG data points for LT.NS (Larsen & Toubro Ltd.).\n",
            "\n",
            "Combining collected historic ESG data...\n",
            "  -> Date column converted to datetime and data sorted.\n",
            "\n",
            "Preview of combined historic ESG data:\n",
            "        Date  Total-Score  E-Score  S-Score  G-Score Ticker\n",
            "0 2014-09-01         60.0     56.0     54.0     74.0    ACM\n",
            "1 2014-10-01         60.0     56.0     54.0     74.0    ACM\n",
            "2 2014-11-01         60.0     56.0     54.0     74.0    ACM\n",
            "3 2014-12-01         60.0     56.0     54.0     74.0    ACM\n",
            "4 2015-01-01         59.0     56.0     51.0     73.0    ACM\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1273 entries, 0 to 1272\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   Date         1273 non-null   datetime64[ns]\n",
            " 1   Total-Score  863 non-null    float64       \n",
            " 2   E-Score      863 non-null    float64       \n",
            " 3   S-Score      863 non-null    float64       \n",
            " 4   G-Score      863 non-null    float64       \n",
            " 5   Ticker       1273 non-null   object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(1)\n",
            "memory usage: 59.8+ KB\n",
            "\n",
            "Total rows collected: 1273\n",
            "\n",
            "Saving historic ESG data to: historic_esg_scores_construction.csv ...\n",
            "Historic ESG data saved successfully.\n",
            "\n",
            "--- Historic ESG Fetching Summary ---\n",
            "Successfully fetched historic ESG data for (12 tickers): ['DG.PA', 'ACS.MC', 'PWR', 'VMC', 'MLM', 'CRH', 'ACM', 'J', 'FLR', 'MTZ', 'HEI.DE', 'LT.NS']\n",
            "Failed or no historic ESG data for (1 tickers): ['HCMLY']\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch Crtical Earth Material - Mining ESG Data"
      ],
      "metadata": {
        "id": "cYsz--q045YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries: yesg, pandas\n",
        "# If running locally, you need to install these:\n",
        "# !pip install yesg pandas\n",
        "\n",
        "import yesg\n",
        "import pandas as pd\n",
        "import time # To add delays between API calls\n",
        "import warnings # To potentially suppress warnings\n",
        "\n",
        "print(\"--- ESG Data Fetching Script for Critical Materials, Mining & Rare Earths Firms ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# List of tickers for Critical Materials, Mining & Rare Earths firms\n",
        "# Note: Data availability may vary significantly by ticker and source.\n",
        "TICKERS_MINING_RE = [\n",
        "    'BHP',    # BHP Group Ltd. (NYSE/ASX/LSE) - Using NYSE\n",
        "    'RIO',    # Rio Tinto Group (NYSE/ASX/LSE) - Using NYSE\n",
        "    'VALE',   # Vale S.A. (NYSE)\n",
        "    'GLNCY',  # Glencore plc (OTC) - Using OTC for simplicity\n",
        "    'FCX',    # Freeport-McMoRan Inc. (NYSE)\n",
        "    'ALB',    # Albemarle Corporation (NYSE)\n",
        "    'SQM',    # SQM (Soc. Química Minera) (NYSE)\n",
        "    'MP',     # MP Materials Corp. (NYSE)\n",
        "    'LYSCF',  # Lynas Rare Earths Ltd. (OTC) - Using OTC for simplicity\n",
        "    'LAC',    # Lithium Americas Corp. (NYSE/TSX) - Using NYSE\n",
        "    'ALTM',   # Arcadium Lithium plc (NYSE)\n",
        "    'SGML',   # Sigma Lithium Corp. (NASDAQ/TSXV) - Using NASDAQ\n",
        "    'UUUU',   # Energy Fuels Inc. (NYSEAM/TSX) - Using NYSEAM\n",
        "    'ILKAF',  # Iluka Resources Ltd. (OTC) - Using OTC for simplicity\n",
        "    'PILBF',  # Pilbara Minerals Ltd. (OTC) - Using OTC for simplicity\n",
        "]\n",
        "\n",
        "# Optional mapping for clearer output\n",
        "TICKER_NAMES = {\n",
        "    'BHP': 'BHP Group Ltd.', 'RIO': 'Rio Tinto Group', 'VALE': 'Vale S.A.',\n",
        "    'GLNCY': 'Glencore plc', 'FCX': 'Freeport-McMoRan Inc.', 'ALB': 'Albemarle Corporation',\n",
        "    'SQM': 'SQM (Soc. Química Minera)', 'MP': 'MP Materials Corp.',\n",
        "    'LYSCF': 'Lynas Rare Earths Ltd.', 'LAC': 'Lithium Americas Corp.',\n",
        "    'ALTM': 'Arcadium Lithium plc', 'SGML': 'Sigma Lithium Corp.',\n",
        "    'UUUU': 'Energy Fuels Inc.', 'ILKAF': 'Iluka Resources Ltd.',\n",
        "    'PILBF': 'Pilbara Minerals Ltd.',\n",
        "}\n",
        "\n",
        "# Define where to save the output file (locally)\n",
        "OUTPUT_FILENAME = 'historic_esg_scores_mining_re.csv'\n",
        "OUTPUT_PATH_LOCAL = OUTPUT_FILENAME # Save in current directory\n",
        "\n",
        "# Delay between API calls (in seconds) to avoid potential blocking\n",
        "API_DELAY = 0.8 # Slightly increased delay as a precaution\n",
        "\n",
        "# --- Data Fetching Loop ---\n",
        "print(f\"\\nTickers to fetch ESG data for ({len(TICKERS_MINING_RE)} total): {TICKERS_MINING_RE}\")\n",
        "print(\"Starting ESG data download loop...\")\n",
        "print(\"WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\")\n",
        "print(\"Note: ESG data is typically published annually, so expect one data point per year if available.\")\n",
        "\n",
        "\n",
        "# Initialize lists to store results and track progress\n",
        "all_esg_data_list = []\n",
        "successful_tickers = []\n",
        "failed_tickers = []\n",
        "\n",
        "for ticker in TICKERS_MINING_RE:\n",
        "    ticker_name = TICKER_NAMES.get(ticker, ticker) # Use mapped name if available\n",
        "    print(f\"  -> Processing: {ticker} ({ticker_name})\")\n",
        "    try:\n",
        "        # Add the delay BEFORE the API call to space them out\n",
        "        time.sleep(API_DELAY)\n",
        "        # Fetch all available historic ESG ratings for the ticker\n",
        "        esg_scores_df = yesg.get_historic_esg(ticker)\n",
        "\n",
        "        # Check if the result is a non-empty DataFrame\n",
        "        if isinstance(esg_scores_df, pd.DataFrame) and not esg_scores_df.empty:\n",
        "            # Add a column for the ticker symbol\n",
        "            esg_scores_df['Ticker'] = ticker\n",
        "            # Reset the index to make the date a column before appending\n",
        "            # The date column is typically the index after fetching\n",
        "            esg_scores_df = esg_scores_df.reset_index()\n",
        "            # Rename the date column if needed (common names are 'Date' or 'index')\n",
        "            if 'index' in esg_scores_df.columns:\n",
        "                esg_scores_df = esg_scores_df.rename(columns={'index': 'Date'})\n",
        "\n",
        "            all_esg_data_list.append(esg_scores_df)\n",
        "            successful_tickers.append(ticker)\n",
        "            print(f\"    -> Success: Found {len(esg_scores_df)} historic ESG data points for {ticker} ({ticker_name}).\")\n",
        "        else:\n",
        "            # Handle cases where yesg returns None or an empty DataFrame\n",
        "            print(f\"    -> No valid historic ESG data found/returned for {ticker} ({ticker_name}).\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions during fetching or processing\n",
        "        print(f\"    -> ERROR fetching/processing historic ESG data for {ticker} ({ticker_name}): {e}\")\n",
        "        failed_tickers.append(ticker)\n",
        "\n",
        "# --- Combine and Save Data ---\n",
        "if all_esg_data_list:\n",
        "    print(\"\\nCombining collected historic ESG data...\")\n",
        "    # Concatenate all the collected DataFrames into a single one\n",
        "    final_esg_data = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "\n",
        "    # Ensure the Date column is correctly named and formatted if possible\n",
        "    if 'Date' in final_esg_data.columns:\n",
        "        try:\n",
        "            # Attempt to convert Date column to datetime objects for consistency\n",
        "            final_esg_data['Date'] = pd.to_datetime(final_esg_data['Date'])\n",
        "            # Sort by Ticker and Date for clarity\n",
        "            final_esg_data = final_esg_data.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "            print(\"  -> Date column converted to datetime and data sorted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert 'Date' column to datetime format or sort data: {e}\")\n",
        "            print(\"Please inspect the Date column format manually.\")\n",
        "    else:\n",
        "        print(\"Warning: 'Date' column not found in combined data. Please inspect the output DataFrame structure.\")\n",
        "\n",
        "\n",
        "    # Display first few rows and info of the final DataFrame\n",
        "    print(\"\\nPreview of combined historic ESG data:\")\n",
        "    print(final_esg_data.head())\n",
        "    print(\"\\nData Info:\")\n",
        "    final_esg_data.info()\n",
        "    print(f\"\\nTotal rows collected: {len(final_esg_data)}\")\n",
        "\n",
        "\n",
        "    # Save the combined data to the chosen CSV file path (local)\n",
        "    try:\n",
        "        print(f\"\\nSaving historic ESG data to: {OUTPUT_PATH_LOCAL} ...\")\n",
        "        final_esg_data.to_csv(OUTPUT_PATH_LOCAL, index=False)\n",
        "        print(f\"Historic ESG data saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR saving historic ESG data to CSV at '{OUTPUT_PATH_LOCAL}': {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Message if no data was collected at all\n",
        "    print(\"\\nNo historic ESG data was successfully collected for any ticker. No CSV file created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Historic ESG Fetching Summary ---\")\n",
        "print(f\"Successfully fetched historic ESG data for ({len(successful_tickers)} tickers): {successful_tickers}\")\n",
        "print(f\"Failed or no historic ESG data for ({len(failed_tickers)} tickers): {failed_tickers}\")\n",
        "print(\"--- Script Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUAEungk5Ee-",
        "outputId": "c2f674fb-f873-4bf1-aa7b-33624c1c9a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ESG Data Fetching Script for Critical Materials, Mining & Rare Earths Firms ---\n",
            "\n",
            "Tickers to fetch ESG data for (15 total): ['BHP', 'RIO', 'VALE', 'GLNCY', 'FCX', 'ALB', 'SQM', 'MP', 'LYSCF', 'LAC', 'ALTM', 'SGML', 'UUUU', 'ILKAF', 'PILBF']\n",
            "Starting ESG data download loop...\n",
            "WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\n",
            "Note: ESG data is typically published annually, so expect one data point per year if available.\n",
            "  -> Processing: BHP (BHP Group Ltd.)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for BHP (BHP Group Ltd.).\n",
            "  -> Processing: RIO (Rio Tinto Group)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for RIO (Rio Tinto Group).\n",
            "  -> Processing: VALE (Vale S.A.)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for VALE (Vale S.A.).\n",
            "  -> Processing: GLNCY (Glencore plc)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for GLNCY (Glencore plc).\n",
            "  -> Processing: FCX (Freeport-McMoRan Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for FCX (Freeport-McMoRan Inc.).\n",
            "  -> Processing: ALB (Albemarle Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for ALB (Albemarle Corporation).\n",
            "  -> Processing: SQM (SQM (Soc. Química Minera))\n",
            "    -> Success: Found 128 historic ESG data points for SQM (SQM (Soc. Química Minera)).\n",
            "  -> Processing: MP (MP Materials Corp.)\n",
            "    -> Success: Found 6 historic ESG data points for MP (MP Materials Corp.).\n",
            "  -> Processing: LYSCF (Lynas Rare Earths Ltd.)\n",
            "    -> Success: Found 6 historic ESG data points for LYSCF (Lynas Rare Earths Ltd.).\n",
            "  -> Processing: LAC (Lithium Americas Corp.)\n",
            "    -> Success: Found 6 historic ESG data points for LAC (Lithium Americas Corp.).\n",
            "  -> Processing: ALTM (Arcadium Lithium plc)\n",
            "    -> Success: Found 3 historic ESG data points for ALTM (Arcadium Lithium plc).\n",
            "  -> Processing: SGML (Sigma Lithium Corp.)\n",
            "    -> Success: Found 7 historic ESG data points for SGML (Sigma Lithium Corp.).\n",
            "  -> Processing: UUUU (Energy Fuels Inc.)\n",
            "    -> Success: Found 7 historic ESG data points for UUUU (Energy Fuels Inc.).\n",
            "  -> Processing: ILKAF (Iluka Resources Ltd.)\n",
            "    -> Success: Found 6 historic ESG data points for ILKAF (Iluka Resources Ltd.).\n",
            "  -> Processing: PILBF (Pilbara Minerals Ltd.)\n",
            "    -> Success: Found 6 historic ESG data points for PILBF (Pilbara Minerals Ltd.).\n",
            "\n",
            "Combining collected historic ESG data...\n",
            "  -> Date column converted to datetime and data sorted.\n",
            "\n",
            "Preview of combined historic ESG data:\n",
            "        Date  Total-Score  E-Score  S-Score  G-Score Ticker\n",
            "0 2014-09-01         68.0     57.0     71.0     81.0    ALB\n",
            "1 2014-10-01         67.0     57.0     71.0     81.0    ALB\n",
            "2 2014-11-01         67.0     57.0     71.0     79.0    ALB\n",
            "3 2014-12-01         67.0     57.0     71.0     79.0    ALB\n",
            "4 2015-01-01         65.0     54.0     67.0     80.0    ALB\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 431 entries, 0 to 430\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   Date         431 non-null    datetime64[ns]\n",
            " 1   Total-Score  269 non-null    float64       \n",
            " 2   E-Score      269 non-null    float64       \n",
            " 3   S-Score      269 non-null    float64       \n",
            " 4   G-Score      269 non-null    float64       \n",
            " 5   Ticker       431 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(1)\n",
            "memory usage: 20.3+ KB\n",
            "\n",
            "Total rows collected: 431\n",
            "\n",
            "Saving historic ESG data to: historic_esg_scores_mining_re.csv ...\n",
            "Historic ESG data saved successfully.\n",
            "\n",
            "--- Historic ESG Fetching Summary ---\n",
            "Successfully fetched historic ESG data for (11 tickers): ['FCX', 'ALB', 'SQM', 'MP', 'LYSCF', 'LAC', 'ALTM', 'SGML', 'UUUU', 'ILKAF', 'PILBF']\n",
            "Failed or no historic ESG data for (4 tickers): ['BHP', 'RIO', 'VALE', 'GLNCY']\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regression and Panel Regression Python code"
      ],
      "metadata": {
        "id": "OMV8oabHZARN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3* Fetch Electronics Firm's ESG Data"
      ],
      "metadata": {
        "id": "mzwJUhdH79uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries: yesg, pandas\n",
        "# If running locally, you need to install these:\n",
        "# !pip install yesg pandas\n",
        "\n",
        "import yesg\n",
        "import pandas as pd\n",
        "import time # To add delays between API calls\n",
        "import warnings # To potentially suppress warnings\n",
        "\n",
        "print(\"--- ESG Data Fetching Script for Global Electronic Manufacturers ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# List of tickers for Global Electronic Manufacturers\n",
        "# Prioritizing major US exchanges (NASDAQ, NYSE) or primary local ones if available via Yahoo Finance\n",
        "# Note: Data availability may vary significantly by ticker and source.\n",
        "TICKERS_ELECTRONICS = [\n",
        "    'AAPL',   # Apple Inc. (NASDAQ)\n",
        "    '005930.KS', # Samsung Electronics (KRX) - Yahoo Finance often uses .KS\n",
        "    'SONY',   # Sony Group Corporation (NYSE) - Using NYSE ticker\n",
        "    '6752.T', # Panasonic Holdings (TSE) - Using TSE ticker\n",
        "    '066570.KS',# LG Electronics (KRX) - Yahoo Finance often uses .KS\n",
        "    '2317.TW',# Foxconn (Hon Hai) (TPE) - Yahoo Finance often uses .TW\n",
        "    'PHG',    # Philips (NYSE) - Using NYSE ticker\n",
        "    'SIEGY',  # Siemens AG (OTC) - Using OTC for simplicity\n",
        "    'CAJ',    # Canon Inc. (NYSE) - Using NYSE ticker\n",
        "    '6753.T', # Sharp Corporation (TSE) - Using TSE ticker\n",
        "    'TXN',    # Texas Instruments (NASDAQ)\n",
        "    'INTC',   # Intel Corporation (NASDAQ)\n",
        "    'NXPI',   # NXP Semiconductors (NASDAQ)\n",
        "    'STM',    # STMicroelectronics (NYSE) - Using NYSE ticker\n",
        "    'IFX.DE', # Infineon Technologies (FWB) - Using FWB ticker\n",
        "    'ASML',   # ASML Holding (NASDAQ) - Using NASDAQ ticker\n",
        "    'MU',     # Micron Technology (NASDAQ)\n",
        "    'ADI',    # Analog Devices (NASDAQ)\n",
        "    'AVGO',   # Broadcom Inc. (NASDAQ)\n",
        "    '6502.T', # Toshiba Corporation (TSE) - Using TSE ticker\n",
        "]\n",
        "\n",
        "# Optional mapping for clearer output\n",
        "TICKER_NAMES = {\n",
        "    'AAPL': 'Apple Inc.', '005930.KS': 'Samsung Electronics', 'SONY': 'Sony Group Corporation',\n",
        "    '6752.T': 'Panasonic Holdings', '066570.KS': 'LG Electronics', '2317.TW': 'Foxconn (Hon Hai)',\n",
        "    'PHG': 'Philips', 'SIEGY': 'Siemens AG', 'CAJ': 'Canon Inc.', '6753.T': 'Sharp Corporation',\n",
        "    'TXN': 'Texas Instruments', 'INTC': 'Intel Corporation', 'NXPI': 'NXP Semiconductors',\n",
        "    'STM': 'STMicroelectronics', 'IFX.DE': 'Infineon Technologies', 'ASML': 'ASML Holding',\n",
        "    'MU': 'Micron Technology', 'ADI': 'Analog Devices', 'AVGO': 'Broadcom Inc.',\n",
        "    '6502.T': 'Toshiba Corporation',\n",
        "}\n",
        "\n",
        "# Define where to save the output file (locally)\n",
        "OUTPUT_FILENAME = 'historic_esg_scores_electronics.csv'\n",
        "OUTPUT_PATH_LOCAL = OUTPUT_FILENAME # Save in current directory\n",
        "\n",
        "# Delay between API calls (in seconds) to avoid potential blocking\n",
        "API_DELAY = 0.8 # Slightly increased delay as a precaution\n",
        "\n",
        "# --- Data Fetching Loop ---\n",
        "print(f\"\\nTickers to fetch ESG data for ({len(TICKERS_ELECTRONICS)} total): {TICKERS_ELECTRONICS}\")\n",
        "print(\"Starting ESG data download loop...\")\n",
        "print(\"WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\")\n",
        "print(\"Note: ESG data is typically published annually, so expect one data point per year if available.\")\n",
        "\n",
        "\n",
        "# Initialize lists to store results and track progress\n",
        "all_esg_data_list = []\n",
        "successful_tickers = []\n",
        "failed_tickers = []\n",
        "\n",
        "for ticker in TICKERS_ELECTRONICS:\n",
        "    ticker_name = TICKER_NAMES.get(ticker, ticker) # Use mapped name if available\n",
        "    print(f\"  -> Processing: {ticker} ({ticker_name})\")\n",
        "    try:\n",
        "        # Add the delay BEFORE the API call to space them out\n",
        "        time.sleep(API_DELAY)\n",
        "        # Fetch all available historic ESG ratings for the ticker\n",
        "        esg_scores_df = yesg.get_historic_esg(ticker)\n",
        "\n",
        "        # Check if the result is a non-empty DataFrame\n",
        "        if isinstance(esg_scores_df, pd.DataFrame) and not esg_scores_df.empty:\n",
        "            # Add a column for the ticker symbol\n",
        "            esg_scores_df['Ticker'] = ticker\n",
        "            # Reset the index to make the date a column before appending\n",
        "            # The date column is typically the index after fetching\n",
        "            esg_scores_df = esg_scores_df.reset_index()\n",
        "            # Rename the date column if needed (common names are 'Date' or 'index')\n",
        "            if 'index' in esg_scores_df.columns:\n",
        "                esg_scores_df = esg_scores_df.rename(columns={'index': 'Date'})\n",
        "\n",
        "            all_esg_data_list.append(esg_scores_df)\n",
        "            successful_tickers.append(ticker)\n",
        "            print(f\"    -> Success: Found {len(esg_scores_df)} historic ESG data points for {ticker} ({ticker_name}).\")\n",
        "        else:\n",
        "            # Handle cases where yesg returns None or an empty DataFrame\n",
        "            print(f\"    -> No valid historic ESG data found/returned for {ticker} ({ticker_name}).\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions during fetching or processing\n",
        "        print(f\"    -> ERROR fetching/processing historic ESG data for {ticker} ({ticker_name}): {e}\")\n",
        "        failed_tickers.append(ticker)\n",
        "\n",
        "# --- Combine and Save Data ---\n",
        "if all_esg_data_list:\n",
        "    print(\"\\nCombining collected historic ESG data...\")\n",
        "    # Concatenate all the collected DataFrames into a single one\n",
        "    final_esg_data = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "\n",
        "    # Ensure the Date column is correctly named and formatted if possible\n",
        "    if 'Date' in final_esg_data.columns:\n",
        "        try:\n",
        "            # Attempt to convert Date column to datetime objects for consistency\n",
        "            final_esg_data['Date'] = pd.to_datetime(final_esg_data['Date'])\n",
        "            # Sort by Ticker and Date for clarity\n",
        "            final_esg_data = final_esg_data.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "            print(\"  -> Date column converted to datetime and data sorted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert 'Date' column to datetime format or sort data: {e}\")\n",
        "            print(\"Please inspect the Date column format manually.\")\n",
        "    else:\n",
        "        print(\"Warning: 'Date' column not found in combined data. Please inspect the output DataFrame structure.\")\n",
        "\n",
        "\n",
        "    # Display first few rows and info of the final DataFrame\n",
        "    print(\"\\nPreview of combined historic ESG data:\")\n",
        "    print(final_esg_data.head())\n",
        "    print(\"\\nData Info:\")\n",
        "    final_esg_data.info()\n",
        "    print(f\"\\nTotal rows collected: {len(final_esg_data)}\")\n",
        "\n",
        "\n",
        "    # Save the combined data to the chosen CSV file path (local)\n",
        "    try:\n",
        "        print(f\"\\nSaving historic ESG data to: {OUTPUT_PATH_LOCAL} ...\")\n",
        "        final_esg_data.to_csv(OUTPUT_PATH_LOCAL, index=False)\n",
        "        print(f\"Historic ESG data saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR saving historic ESG data to CSV at '{OUTPUT_PATH_LOCAL}': {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Message if no data was collected at all\n",
        "    print(\"\\nNo historic ESG data was successfully collected for any ticker. No CSV file created.\")\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Historic ESG Fetching Summary ---\")\n",
        "print(f\"Successfully fetched historic ESG data for ({len(successful_tickers)} tickers): {successful_tickers}\")\n",
        "print(f\"Failed or no historic ESG data for ({len(failed_tickers)} tickers): {failed_tickers}\")\n",
        "print(\"--- Script Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3Bkqn9I8HQP",
        "outputId": "1dd7cc96-e64b-446e-cd12-094a2bbcd95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ESG Data Fetching Script for Global Electronic Manufacturers ---\n",
            "\n",
            "Tickers to fetch ESG data for (20 total): ['AAPL', '005930.KS', 'SONY', '6752.T', '066570.KS', '2317.TW', 'PHG', 'SIEGY', 'CAJ', '6753.T', 'TXN', 'INTC', 'NXPI', 'STM', 'IFX.DE', 'ASML', 'MU', 'ADI', 'AVGO', '6502.T']\n",
            "Starting ESG data download loop...\n",
            "WARNING: 'yesg' library relies on Yahoo Finance and may be outdated or have limited data coverage.\n",
            "Note: ESG data is typically published annually, so expect one data point per year if available.\n",
            "  -> Processing: AAPL (Apple Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for AAPL (Apple Inc.).\n",
            "  -> Processing: 005930.KS (Samsung Electronics)\n",
            "    -> Success: Found 109 historic ESG data points for 005930.KS (Samsung Electronics).\n",
            "  -> Processing: SONY (Sony Group Corporation)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for SONY (Sony Group Corporation).\n",
            "  -> Processing: 6752.T (Panasonic Holdings)\n",
            "    -> Success: Found 128 historic ESG data points for 6752.T (Panasonic Holdings).\n",
            "  -> Processing: 066570.KS (LG Electronics)\n",
            "    -> Success: Found 109 historic ESG data points for 066570.KS (LG Electronics).\n",
            "  -> Processing: 2317.TW (Foxconn (Hon Hai))\n",
            "    -> Success: Found 109 historic ESG data points for 2317.TW (Foxconn (Hon Hai)).\n",
            "  -> Processing: PHG (Philips)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for PHG (Philips).\n",
            "  -> Processing: SIEGY (Siemens AG)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for SIEGY (Siemens AG).\n",
            "  -> Processing: CAJ (Canon Inc.)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for CAJ (Canon Inc.).\n",
            "  -> Processing: 6753.T (Sharp Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for 6753.T (Sharp Corporation).\n",
            "  -> Processing: TXN (Texas Instruments)\n",
            "    -> Success: Found 128 historic ESG data points for TXN (Texas Instruments).\n",
            "  -> Processing: INTC (Intel Corporation)\n",
            "    -> Success: Found 128 historic ESG data points for INTC (Intel Corporation).\n",
            "  -> Processing: NXPI (NXP Semiconductors)\n",
            "    -> Success: Found 128 historic ESG data points for NXPI (NXP Semiconductors).\n",
            "  -> Processing: STM (STMicroelectronics)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for STM (STMicroelectronics).\n",
            "  -> Processing: IFX.DE (Infineon Technologies)\n",
            "    -> Success: Found 128 historic ESG data points for IFX.DE (Infineon Technologies).\n",
            "  -> Processing: ASML (ASML Holding)\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "    -> No valid historic ESG data found/returned for ASML (ASML Holding).\n",
            "  -> Processing: MU (Micron Technology)\n",
            "    -> Success: Found 128 historic ESG data points for MU (Micron Technology).\n",
            "  -> Processing: ADI (Analog Devices)\n",
            "    -> Success: Found 128 historic ESG data points for ADI (Analog Devices).\n",
            "  -> Processing: AVGO (Broadcom Inc.)\n",
            "    -> Success: Found 128 historic ESG data points for AVGO (Broadcom Inc.).\n",
            "  -> Processing: 6502.T (Toshiba Corporation)\n",
            "    -> Success: Found 109 historic ESG data points for 6502.T (Toshiba Corporation).\n",
            "\n",
            "Combining collected historic ESG data...\n",
            "  -> Date column converted to datetime and data sorted.\n",
            "\n",
            "Preview of combined historic ESG data:\n",
            "        Date  Total-Score  E-Score  S-Score  G-Score     Ticker\n",
            "0 2014-09-01         71.0     90.0     58.0     58.0  005930.KS\n",
            "1 2014-10-01         69.0     90.0     58.0     58.0  005930.KS\n",
            "2 2014-11-01         71.0     90.0     58.0     58.0  005930.KS\n",
            "3 2014-12-01         71.0     90.0     58.0     58.0  005930.KS\n",
            "4 2015-01-01         71.0     90.0     58.0     58.0  005930.KS\n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1716 entries, 0 to 1715\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   Date         1716 non-null   datetime64[ns]\n",
            " 1   Total-Score  1175 non-null   float64       \n",
            " 2   E-Score      1175 non-null   float64       \n",
            " 3   S-Score      1175 non-null   float64       \n",
            " 4   G-Score      1175 non-null   float64       \n",
            " 5   Ticker       1716 non-null   object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(1)\n",
            "memory usage: 80.6+ KB\n",
            "\n",
            "Total rows collected: 1716\n",
            "\n",
            "Saving historic ESG data to: historic_esg_scores_electronics.csv ...\n",
            "Historic ESG data saved successfully.\n",
            "\n",
            "--- Historic ESG Fetching Summary ---\n",
            "Successfully fetched historic ESG data for (14 tickers): ['AAPL', '005930.KS', '6752.T', '066570.KS', '2317.TW', '6753.T', 'TXN', 'INTC', 'NXPI', 'IFX.DE', 'MU', 'ADI', 'AVGO', '6502.T']\n",
            "Failed or no historic ESG data for (6 tickers): ['SONY', 'PHG', 'SIEGY', 'CAJ', 'STM', 'ASML']\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transportation Regression and Panel Regression"
      ],
      "metadata": {
        "id": "P6UmEHDIa6tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install famafrench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wszAFPbkb5vm",
        "outputId": "a08814a7-dd53-4336-b457-660dcd0a9e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: famafrench in /usr/local/lib/python3.11/dist-packages (0.1.4)\n",
            "Requirement already satisfied: IPython>=7.12.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (7.34.0)\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.11/dist-packages (from famafrench) (2.0.2)\n",
            "Requirement already satisfied: numpydoc>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from famafrench) (1.8.0)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (0.60.0)\n",
            "Requirement already satisfied: methodtools>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (0.4.7)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from famafrench) (2.2.2)\n",
            "Requirement already satisfied: pandas-datareader>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (0.10.0)\n",
            "Requirement already satisfied: pandas-market-calendars>=1.1 in /usr/local/lib/python3.11/dist-packages (from famafrench) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from famafrench) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (1.1.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.13 in /usr/local/lib/python3.11/dist-packages (from famafrench) (2.0.40)\n",
            "Requirement already satisfied: sphinx>=2.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (8.2.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from famafrench) (3.0.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from famafrench) (4.67.1)\n",
            "Requirement already satisfied: wrds>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from famafrench) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython>=7.12.0->famafrench) (4.9.0)\n",
            "Requirement already satisfied: wirerope>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from methodtools>=0.1.0->famafrench) (1.0.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.48.0->famafrench) (0.43.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from numpydoc>=0.9.2->famafrench) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->famafrench) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->famafrench) (2025.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pandas-datareader>=0.7.0->famafrench) (5.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pandas-datareader>=0.7.0->famafrench) (2.32.3)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /usr/local/lib/python3.11/dist-packages (from pandas-market-calendars>=1.1->famafrench) (4.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->famafrench) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (3.1.6)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=2.0->famafrench) (24.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.3.13->famafrench) (3.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.3.13->famafrench) (4.13.2)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/dist-packages (from wrds>=3.0.8->famafrench) (2.9.10)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas-market-calendars>=1.1->famafrench) (2.2.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas-market-calendars>=1.1->famafrench) (0.12.1)\n",
            "Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas-market-calendars>=1.1->famafrench) (0.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython>=7.12.0->famafrench) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=2.0->famafrench) (3.0.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython>=7.12.0->famafrench) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython>=7.12.0->famafrench) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader>=0.7.0->famafrench) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader>=0.7.0->famafrench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader>=0.7.0->famafrench) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader>=0.7.0->famafrench) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yfinance statsmodels pandas numpy linearmodels pandas-datareader requests beautifulsoup4 lxml\n",
        "\n",
        "# --- Core Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from linearmodels.panel import PanelOLS, RandomEffects\n",
        "from linearmodels.panel import compare as model_compare\n",
        "from linearmodels.panel.results import PanelEffectsResults, RandomEffectsResults\n",
        "import pandas_datareader.data as web # For Fama-French data\n",
        "import requests                          # For downloading FF files (fallback, keep for now)\n",
        "from io import BytesIO                   # *** Use BytesIO instead of StringIO ***\n",
        "from zipfile import ZipFile              # For handling zipped FF files (fallback, keep for now)\n",
        "import warnings\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import traceback # For detailed error logging if needed\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# --- MICE Imputation Libraries (Kept) ---\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import BayesianRidge # Needed for IterativeImputer estimator\n",
        "\n",
        "# --- Removed Plotting Libraries ---\n",
        "\n",
        "# --- Settings and Configuration ---\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "from statsmodels.tools.sm_exceptions import (ValueWarning, ConvergenceWarning,\n",
        "                                             HessianInversionWarning, PerfectSeparationWarning,\n",
        "                                             CollinearityWarning, PerfectSeparationError)\n",
        "warnings.simplefilter('ignore', ValueWarning)\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "warnings.simplefilter('ignore', HessianInversionWarning)\n",
        "warnings.simplefilter('ignore', PerfectSeparationWarning)\n",
        "warnings.simplefilter('ignore', CollinearityWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"Variables are collinear\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered in scalar divide\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in scalar divide\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in divide\")\n",
        "from linearmodels.panel.utility import AbsorbingEffectWarning\n",
        "warnings.filterwarnings(\"ignore\", category=AbsorbingEffectWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n",
        "pd.set_option('display.width', 140)\n",
        "pd.set_option('display.max_columns', 18)\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "# --- Define Tickers for Global Transportation Firms ---\n",
        "TICKERS_TRANSPORT = [\n",
        "    'UPS', 'FDX', 'XPO', 'JYD',\n",
        "    #'DPSGY', # Often fails download\n",
        "    'AMKBY',\n",
        "    'CNI', 'CP', 'UNP', 'CSX',\n",
        "    'ODFL', 'JBHT',\n",
        "    'ZIM',\n",
        "]\n",
        "print(f\"Attempting analysis for tickers: {TICKERS_TRANSPORT}\")\n",
        "\n",
        "\n",
        "TICKER_NAMES = { # Optional mapping\n",
        "    'UPS': 'United Parcel Service', 'FDX': 'FedEx', 'XPO': 'XPO Inc.', 'JYD': 'Jayud Global Logistics',\n",
        "    #'DPSGY': 'Deutsche Post DHL',\n",
        "    'AMKBY': 'A.P. Møller - Mærsk',\n",
        "    'CNI': 'Canadian National Railway', 'CP': 'Canadian Pacific Kansas City', 'UNP': 'Union Pacific', 'CSX': 'CSX Corp.',\n",
        "    'ODFL': 'Old Dominion Freight Line', 'JBHT': 'J.B. Hunt Transport',\n",
        "    'ZIM': 'ZIM Integrated Shipping',\n",
        "}\n",
        "\n",
        "# --- Define ESG Risk Categories (Time-Invariant) based on user info ---\n",
        "esg_risk_categories = {\n",
        "    # Low Risk\n",
        "    #'DPSGY': 'Low',\n",
        "    'AMKBY': 'Low', 'CNI': 'Low', 'CP': 'Low',\n",
        "    # Medium Risk\n",
        "    'UPS': 'Medium', 'FDX': 'Medium', 'UNP': 'Medium', 'CSX': 'Medium',\n",
        "    'XPO': 'Medium', 'ODFL': 'Medium', 'JBHT': 'Medium', 'ZIM': 'Medium',\n",
        "    # High Risk\n",
        "    'JYD': 'High',\n",
        "}\n",
        "\n",
        "# --- Define Date Range ---\n",
        "START_DATE_PRICES = \"2014-01-01\" # Start earlier for prices/lags\n",
        "END_DATE_PRICES = \"2024-12-31\"\n",
        "START_DATE_ANALYSIS = \"2015-01-01\" # Analysis start date\n",
        "END_DATE_ANALYSIS = \"2023-12-31\" # Analysis end date\n",
        "\n",
        "# --- File Paths ---\n",
        "ESG_DATA_PATH = \"/content/historic_esg_scores_global_transportation.csv\" # *** USE THE CORRECT FILENAME ***\n",
        "\n",
        "# --- Parameters ---\n",
        "ESG_LAG_MONTHS = 1\n",
        "VIF_THRESHOLD = 10\n",
        "IMPUTE_DATA = True\n",
        "RUN_WITHOUT_IMPUTATION_SENSITIVITY = True\n",
        "\n",
        "# --- Version Control & Script Info ---\n",
        "SCRIPT_VERSION = \"Panel Only v14.1 - Transportation & FF DataReader\" # Updated version\n",
        "print(f\"--- Global Transportation ESG Impact Analysis Script Started ({SCRIPT_VERSION}) ---\")\n",
        "# print(f\"Tickers: {TICKERS_TRANSPORT}\") # Already printed above\n",
        "print(f\"Analysis Period: {START_DATE_ANALYSIS} to {END_DATE_ANALYSIS}\")\n",
        "print(f\"ESG Lag: {ESG_LAG_MONTHS} months\")\n",
        "print(f\"Imputation Enabled (Main Run - MICE): {IMPUTE_DATA}\")\n",
        "print(f\"Run Sensitivity without Imputation: {RUN_WITHOUT_IMPUTATION_SENSITIVITY}\")\n",
        "print(f\"Factors Path: Data fetched from K. French Library\") # Modified print\n",
        "print(f\"ESG Data Path: {ESG_DATA_PATH} (Source/Quality Not Verified by Script)\")\n",
        "\n",
        "\n",
        "# --- Advanced Imputation Function ---\n",
        "def advanced_imputation(df_input):\n",
        "    \"\"\"\n",
        "    Performs Iterative Imputation (MICE-like) on numeric columns of a DataFrame.\n",
        "    Uses BayesianRidge as the estimator by default. Handles all-NaN columns.\n",
        "    \"\"\"\n",
        "    df = df_input.copy()\n",
        "    original_index = df.index; original_cols = df.columns\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    non_numeric_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    if not numeric_cols: print(\"  -> Imputation: No numeric columns found.\"); return df_input\n",
        "    df_numeric = df[numeric_cols].copy(); df_non_numeric = df[non_numeric_cols].copy()\n",
        "    if df_numeric.isnull().sum().sum() == 0: print(\"  -> Imputation: No missing values detected.\"); return df_input\n",
        "\n",
        "    print(f\"  -> Imputation: Attempting Iterative Imputation (MICE) for {len(numeric_cols)} numeric columns.\")\n",
        "    n_features = len(numeric_cols); n_neighbors = min(5, n_features - 1) if n_features > 1 else 1\n",
        "    all_nan_cols = df_numeric.columns[df_numeric.isnull().all()].tolist()\n",
        "    if all_nan_cols:\n",
        "        print(f\"    -> Warning: All-NaN columns cannot be imputed: {all_nan_cols}\")\n",
        "        df_numeric_imputable = df_numeric.drop(columns=all_nan_cols); numeric_cols_imputable = df_numeric_imputable.columns.tolist()\n",
        "        if not numeric_cols_imputable: print(\"    -> Error: No imputable numeric columns remain.\"); return df_input\n",
        "        n_features = len(numeric_cols_imputable); n_neighbors = min(5, n_features - 1) if n_features > 1 else 1\n",
        "    else: df_numeric_imputable = df_numeric; numeric_cols_imputable = numeric_cols\n",
        "    if df_numeric_imputable.empty:\n",
        "         print(\"    -> Error: No numeric columns available for imputation.\")\n",
        "         if all_nan_cols: df_all_nan = df_numeric[all_nan_cols]; df_out = pd.concat([df_all_nan, df_non_numeric], axis=1); return df_out[original_cols]\n",
        "         else: return df_input\n",
        "\n",
        "    imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42, tol=1e-3, n_nearest_features=n_neighbors, verbose=0, imputation_order='ascending')\n",
        "    try:\n",
        "        imputed_values = imputer.fit_transform(df_numeric_imputable)\n",
        "        df_imputed_numeric = pd.DataFrame(imputed_values, columns=numeric_cols_imputable, index=df_numeric_imputable.index)\n",
        "        if all_nan_cols:\n",
        "             for col in all_nan_cols: df_imputed_numeric[col] = np.nan\n",
        "        df_out = pd.concat([df_imputed_numeric, df_non_numeric], axis=1); df_out = df_out[original_cols]\n",
        "        for col in non_numeric_cols:\n",
        "             if col in df_out.columns:\n",
        "                 try: df_out[col] = df_out[col].astype(df_input[col].dtype)\n",
        "                 except Exception as type_err: print(f\"    -> Warning: Restore dtype failed '{col}': {type_err}\")\n",
        "        print(\"  -> Imputation: MICE imputation completed.\")\n",
        "        remaining_nan_count = df_out[numeric_cols].isnull().sum().sum()\n",
        "        if remaining_nan_count > 0: print(f\"  -> !!! WARNING: {remaining_nan_count} NaNs remain post-imputation. !!!\")\n",
        "        return df_out\n",
        "    except ValueError as ve: print(f\"  -> Imputation ERROR (ValueError): {ve}. Check sparse data.\"); return df_input\n",
        "    except Exception as e: print(f\"  -> Imputation ERROR (General): {e}.\"); traceback.print_exc(limit=2); return df_input\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 1: Download Stock Returns ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 1. Downloading and Preparing Stock Returns ---\")\n",
        "stock_monthly_returns = pd.DataFrame(); tickers_available_yf = []\n",
        "try:\n",
        "    tickers_to_download = TICKERS_TRANSPORT # Use the correct list\n",
        "    all_stock_data = yf.download( tickers_to_download, start=START_DATE_PRICES, end=END_DATE_PRICES, progress=False, auto_adjust=False, actions=False, ignore_tz=True, group_by='ticker')\n",
        "    if all_stock_data.empty: raise ValueError(\"No stock price data downloaded.\")\n",
        "    price_data_list = []; available_tickers_in_download = []\n",
        "    if len(tickers_to_download) == 1: # Handle single ticker case\n",
        "        ticker = tickers_to_download[0]\n",
        "        if not all_stock_data.empty:\n",
        "            df_ticker = all_stock_data[['Adj Close']].copy()\n",
        "            if df_ticker.empty or df_ticker['Adj Close'].isnull().all(): df_ticker = all_stock_data[['Close']].copy();\n",
        "            if not (df_ticker.empty or df_ticker['Close'].isnull().all()): print(f\"  -> Warning: Using 'Close' for {ticker}.\")\n",
        "            else: print(f\"  -> Warning: No valid price for {ticker}. Skipping.\")\n",
        "            if not df_ticker.empty and not df_ticker.isnull().all().all(): df_ticker.columns = [ticker]; price_data_list.append(df_ticker); available_tickers_in_download.append(ticker)\n",
        "    else: # Handle multiple tickers\n",
        "        if isinstance(all_stock_data.columns, pd.MultiIndex):\n",
        "             valid_tickers = all_stock_data.columns.get_level_values(0).unique().tolist()\n",
        "             for ticker in tickers_to_download:\n",
        "                  if ticker in valid_tickers:\n",
        "                    try:\n",
        "                        df_ticker = all_stock_data[ticker][['Adj Close']].copy()\n",
        "                        if df_ticker.empty or df_ticker['Adj Close'].isnull().all():\n",
        "                            df_ticker = all_stock_data[ticker][['Close']].copy()\n",
        "                            if not (df_ticker.empty or df_ticker['Close'].isnull().all()): print(f\"  -> Warning: Using 'Close' for {ticker}.\")\n",
        "                            else: print(f\"  -> Warning: No valid price for {ticker}. Skipping.\"); continue\n",
        "                        if not df_ticker.empty and not df_ticker.isnull().all().all(): df_ticker.columns = [ticker]; price_data_list.append(df_ticker); available_tickers_in_download.append(ticker)\n",
        "                    except KeyError: print(f\"  -> Warning: Data for {ticker} not in MultiIndex.\")\n",
        "                  else: print(f\"  -> Warning: Ticker {ticker} requested but not in yfinance result for this run.\") # Modified message\n",
        "        else: raise TypeError(f\"Unexpected yfinance structure: {type(all_stock_data)}\") # Should be MultiIndex for multiple tickers\n",
        "\n",
        "    if not price_data_list: raise ValueError(\"No valid price data collected.\")\n",
        "    price_data = pd.concat(price_data_list, axis=1); price_data = price_data.ffill().bfill().dropna(axis=1, how='all')\n",
        "    if price_data.empty: raise ValueError(\"Price data empty after cleaning.\")\n",
        "    tickers_available_yf = sorted(list(price_data.columns)); print(f\"  -> Stock price data processed for {len(tickers_available_yf)} tickers: {tickers_available_yf}\")\n",
        "    price_data.index = pd.to_datetime(price_data.index); monthly_prices = price_data.resample('ME').last()\n",
        "    stock_monthly_returns = monthly_prices.pct_change()\n",
        "    buffer_start_date = (pd.to_datetime(START_DATE_ANALYSIS) - pd.DateOffset(months=ESG_LAG_MONTHS + 2))\n",
        "    stock_monthly_returns = stock_monthly_returns.loc[buffer_start_date:END_DATE_PRICES] # Filter includes buffer\n",
        "    if stock_monthly_returns.empty or stock_monthly_returns.isnull().all().all(): raise ValueError(\"Monthly returns empty/all NaN after date filtering.\")\n",
        "    print(f\"  -> Stock monthly returns prepared: {stock_monthly_returns.index.min().date()} to {stock_monthly_returns.index.max().date()}\")\n",
        "except Exception as e: print(f\" FATAL ERROR processing stock returns: {e}\"); traceback.print_exc(); sys.exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 2: Load and Prepare Fama-French Factors ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 2. Loading and Preparing Fama-French Factors ---\")\n",
        "ff_factors_monthly = pd.DataFrame(); rf_col = None; available_factors_list = []\n",
        "\n",
        "try:\n",
        "    print(f\"  -> Downloading Fama-French factors using pandas-datareader...\")\n",
        "    ff_start = START_DATE_PRICES\n",
        "    ff_end = END_DATE_PRICES\n",
        "\n",
        "    try:\n",
        "        # Fetch FF5 factors (monthly is index 0)\n",
        "        ff_data_5f = web.DataReader('Developed_5_Factors', 'famafrench', start=ff_start, end=ff_end)\n",
        "        ff_monthly_5f = ff_data_5f[0].copy()\n",
        "\n",
        "        # Fetch Momentum factor (monthly is index 0)\n",
        "        ff_mom_data = web.DataReader('Developed_Mom_Factor', 'famafrench', start=ff_start, end=ff_end)\n",
        "        ff_monthly_mom = ff_mom_data[0].copy()\n",
        "\n",
        "        # Merge\n",
        "        ff_factors_raw = pd.merge(ff_monthly_5f, ff_monthly_mom, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "        # Rename columns\n",
        "        ff_factors_raw = ff_factors_raw.rename(columns={\n",
        "            'Mkt-RF': 'mkt_rf', 'SMB': 'smb', 'HML': 'hml',\n",
        "            'RMW': 'rmw', 'CMA': 'cma', 'RF': 'rf', 'Mom': 'mom'\n",
        "        })\n",
        "\n",
        "        # Data from pandas-datareader is already in decimals\n",
        "        print(\"  -> Factors downloaded and merged successfully.\")\n",
        "        print(f\"  -> Raw factors shape: {ff_factors_raw.shape}\")\n",
        "\n",
        "    except Exception as ff_err:\n",
        "        print(f\"    -> ERROR downloading/processing Fama-French factors via pandas-datareader: {ff_err}\")\n",
        "        print(\"    -> Check dataset names ('Developed_5_Factors', 'Developed_Mom_Factor') and internet connection.\")\n",
        "        traceback.print_exc(limit=1)\n",
        "        raise ValueError(\"Failed to obtain Fama-French factors.\") from ff_err\n",
        "\n",
        "    # Convert PeriodIndex to DatetimeIndex and set to month end\n",
        "    ff_factors_raw.index = ff_factors_raw.index.to_timestamp() + pd.offsets.MonthEnd(0)\n",
        "\n",
        "    # Filter by date range needed for analysis + buffer\n",
        "    buffer_start_date = (pd.to_datetime(START_DATE_ANALYSIS) - pd.DateOffset(months=ESG_LAG_MONTHS + 2))\n",
        "    ff_factors_monthly_filtered = ff_factors_raw.loc[buffer_start_date:END_DATE_PRICES].copy()\n",
        "\n",
        "    if ff_factors_monthly_filtered.empty:\n",
        "        raise ValueError(f\"No factor data found within the required date range ({buffer_start_date.date()} to {END_DATE_PRICES}).\")\n",
        "\n",
        "    # --- Imputation for Factors ---\n",
        "    if IMPUTE_DATA and ff_factors_monthly_filtered.isnull().any().any():\n",
        "        print(\"  -> Imputing missing values in factors data (if any)...\")\n",
        "        factor_numeric_cols = ff_factors_monthly_filtered.select_dtypes(include=np.number).columns\n",
        "        if not factor_numeric_cols.empty:\n",
        "             ff_factors_monthly = advanced_imputation(ff_factors_monthly_filtered)\n",
        "             if ff_factors_monthly is ff_factors_monthly_filtered: print(\"    -> Warning: Factor imputation skipped or failed.\"); ff_factors_monthly = ff_factors_monthly_filtered.copy()\n",
        "             elif ff_factors_monthly[factor_numeric_cols].isnull().any().any(): print(\"    -> Warning: NaNs may remain post-imputation.\")\n",
        "        else: print(\"    -> No numeric factors found for imputation.\"); ff_factors_monthly = ff_factors_monthly_filtered.copy()\n",
        "    else:\n",
        "        ff_factors_monthly = ff_factors_monthly_filtered.copy()\n",
        "        if not IMPUTE_DATA: print(\"  -> Imputation disabled for factors.\")\n",
        "        elif not ff_factors_monthly_filtered.isnull().any().any(): print(\"  -> No missing values detected in fetched factors.\")\n",
        "\n",
        "    # --- Identify Risk-Free Rate and Available Factors ---\n",
        "    rf_col_options = ['rf', 'risk_free_rate']\n",
        "    rf_col = next((col for col in rf_col_options if col in ff_factors_monthly.columns), None)\n",
        "    if rf_col: print(f\"  -> Using '{rf_col}' as RF.\");\n",
        "    if rf_col is None: raise ValueError(f\"Critical Error: RF column ({rf_col_options}) not found in downloaded factors.\")\n",
        "    elif ff_factors_monthly[rf_col].isnull().any(): print(f\"  -> !!! WARNING: RF column ('{rf_col}') contains NaNs after processing!!!\")\n",
        "\n",
        "    factor_cols_check_for_factors = [\"mkt_rf\", \"smb\", \"hml\", \"rmw\", \"cma\", \"mom\"]\n",
        "    available_factors_list = sorted([f for f in factor_cols_check_for_factors if f in ff_factors_monthly.columns and pd.api.types.is_numeric_dtype(ff_factors_monthly[f])])\n",
        "    if not available_factors_list: print(\"  -> !!! WARNING: No standard factors found. !!!\")\n",
        "    else: print(f\"  -> Available factors identified: {available_factors_list}\")\n",
        "\n",
        "except ValueError as ve: print(f\" FATAL ERROR processing factors: {ve}\"); sys.exit()\n",
        "except Exception as e: print(f\" FATAL ERROR processing factors: {e}\"); traceback.print_exc(); sys.exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 3: Load and Prepare ESG Data from CSV ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 3. Loading and Preparing ESG Data from CSV ---\")\n",
        "esg_panel_raw = pd.DataFrame()\n",
        "try:\n",
        "    try: esg_data_loaded = pd.read_csv(ESG_DATA_PATH)\n",
        "    except FileNotFoundError: raise ValueError(f\"ESG file not found: '{ESG_DATA_PATH}'.\")\n",
        "    except Exception as e: raise ValueError(f\"Could not read ESG file '{ESG_DATA_PATH}': {e}\")\n",
        "    print(f\"  -> Raw ESG data loaded. Shape: {esg_data_loaded.shape}\")\n",
        "    original_cols = list(esg_data_loaded.columns)\n",
        "    # *** ADJUST standardization based on actual CSV columns ***\n",
        "    standardized_names = {\n",
        "        'total-score': 'total_score',\n",
        "        'e-score': 'e_score',\n",
        "        's-score': 's_score',\n",
        "        'g-score': 'g_score'\n",
        "    }\n",
        "    esg_data_loaded.columns = [standardized_names.get(re.sub(r'\\s+', '_', col).replace('.', '').lower(),\n",
        "                                                    re.sub(r'\\s+', '_', col).replace('.', '').lower())\n",
        "                               for col in esg_data_loaded.columns]\n",
        "    standardized_cols = list(esg_data_loaded.columns); print(f\"  -> Standardized ESG columns: {standardized_cols}\")\n",
        "    required_esg_cols = ['date', 'ticker', 'total_score', 'e_score', 's_score', 'g_score'] # Use standardized names\n",
        "    missing_cols = [col for col in required_esg_cols if col not in esg_data_loaded.columns];\n",
        "    if missing_cols: raise ValueError(f\"ESG CSV missing required columns: {missing_cols}.\")\n",
        "    esg_data_loaded['date'] = pd.to_datetime(esg_data_loaded['date'], errors='coerce')\n",
        "    initial_rows = len(esg_data_loaded); esg_data_loaded = esg_data_loaded.dropna(subset=['date'])\n",
        "    if len(esg_data_loaded) < initial_rows: print(f\"  -> Warning: Dropped {initial_rows - len(esg_data_loaded)} rows due to invalid ESG dates.\")\n",
        "    if esg_data_loaded.empty: raise ValueError(\"ESG data empty after removing invalid dates.\")\n",
        "    score_cols_std = ['total_score', 'e_score', 's_score', 'g_score']; # Use standardized names\n",
        "    print(f\"  -> Converting score columns to numeric: {score_cols_std}\")\n",
        "    non_numeric_issues = False\n",
        "    for col in score_cols_std:\n",
        "        initial_nan_count = esg_data_loaded[col].isnull().sum(); esg_data_loaded[col] = pd.to_numeric(esg_data_loaded[col], errors='coerce'); final_nan_count = esg_data_loaded[col].isnull().sum()\n",
        "        if final_nan_count > initial_nan_count: num_coerced = final_nan_count - initial_nan_count; print(f\"    -> CRITICAL WARNING: Column '{col}' had {num_coerced} non-numeric values converted to NaN.\"); non_numeric_issues = True\n",
        "    if non_numeric_issues and not IMPUTE_DATA: raise ValueError(f\"Non-numeric ESG scores found and imputation disabled. Clean source CSV.\")\n",
        "    elif non_numeric_issues: print(\"    -> Imputation will attempt to handle NaNs from non-numeric scores.\")\n",
        "    esg_data_loaded['ticker'] = esg_data_loaded['ticker'].astype(str).str.upper().str.strip()\n",
        "    stock_tickers_upper = [t.upper().strip() for t in tickers_available_yf]; esg_tickers = esg_data_loaded['ticker'].unique()\n",
        "    common_tickers = sorted(list(set(stock_tickers_upper) & set(esg_tickers)));\n",
        "    if not common_tickers: raise ValueError(\"No common tickers found between stock and ESG data.\")\n",
        "    print(f\"  -> Common tickers identified: {common_tickers} ({len(common_tickers)} firms)\")\n",
        "    esg_only = sorted(list(set(esg_tickers) - set(stock_tickers_upper))); stock_only = sorted(list(set(stock_tickers_upper) - set(esg_tickers)))\n",
        "    if esg_only: print(f\"    -> Tickers in ESG only: {esg_only}\")\n",
        "    if stock_only: print(f\"    -> Tickers in Stock only: {stock_only}\")\n",
        "    esg_data_filtered = esg_data_loaded[esg_data_loaded['ticker'].isin(common_tickers)].copy()\n",
        "    buffer_start_date = (pd.to_datetime(START_DATE_ANALYSIS) - pd.DateOffset(months=ESG_LAG_MONTHS + 2))\n",
        "    esg_filter_end_date = pd.to_datetime(END_DATE_ANALYSIS) + pd.offsets.MonthEnd(0)\n",
        "    esg_data_filtered = esg_data_filtered[(esg_data_filtered['date'] >= buffer_start_date) & (esg_data_filtered['date'] <= esg_filter_end_date)]\n",
        "    if esg_data_filtered.empty: raise ValueError(\"No ESG data remains after filtering.\")\n",
        "    esg_data_filtered['date'] = esg_data_filtered['date'] + pd.offsets.MonthEnd(0); esg_data_filtered = esg_data_filtered.sort_values(by=['ticker', 'date']).drop_duplicates(subset=['ticker', 'date'], keep='last')\n",
        "    panel_start_date = esg_data_filtered['date'].min(); panel_end_date = esg_data_filtered['date'].max(); print(f\"  -> Creating ESG panel from {panel_start_date.date()} to {panel_end_date.date()}\")\n",
        "    full_date_range = pd.date_range(start=panel_start_date, end=panel_end_date, freq='ME'); multi_index = pd.MultiIndex.from_product([common_tickers, full_date_range], names=['Ticker', 'Date'])\n",
        "    esg_panel_raw = esg_data_filtered.set_index(['ticker', 'date'])[score_cols_std].reindex(multi_index); print(f\"  -> Forward-filling ESG scores...\")\n",
        "    esg_panel_raw[score_cols_std] = esg_panel_raw.groupby(level='Ticker')[score_cols_std].ffill()\n",
        "    if esg_panel_raw[score_cols_std].isnull().values.any(): nan_counts = esg_panel_raw[score_cols_std].isnull().sum(); print(f\"  -> !!! WARNING: NaNs remain after ffill. Imputation will attempt. Counts:\\n{nan_counts[nan_counts > 0]}\")\n",
        "    else: print(\"  -> No NaNs detected post-ffill.\")\n",
        "    esg_panel_raw = esg_panel_raw.reset_index(); print(f\"  -> ESG panel structure created. Shape: {esg_panel_raw.shape}\")\n",
        "except Exception as e: print(f\" FATAL ERROR processing ESG data: {e}\"); traceback.print_exc(); sys.exit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 5: Check for Multicollinearity (VIF) ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 5. Checking for Multicollinearity (VIF) ---\")\n",
        "vif_results_total = None; vif_results_components = None\n",
        "def calculate_vif(data, predictors, model_name=\"VIF Check\"):\n",
        "    print(f\"\\n  Calculating VIF for: {model_name}\")\n",
        "    if not predictors: print(\"    -> No predictors.\"); return None, []\n",
        "    predictors_in_data = [p for p in predictors if p in data.columns]; missing = [p for p in predictors if p not in data.columns]\n",
        "    if missing: print(f\"    -> Warning: Predictors missing for VIF: {missing}\")\n",
        "    if not predictors_in_data: print(\"    -> No valid predictors.\"); return None, []\n",
        "    X = data[predictors_in_data].copy(); initial_rows = len(X); X = X.dropna(); dropped_rows = initial_rows - len(X)\n",
        "    if dropped_rows > 0: print(f\"    -> Dropped {dropped_rows} rows with NaNs for VIF.\")\n",
        "    if X.empty or len(X) < 2 or X.shape[1] < 1: print(f\"    -> Not enough data for VIF.\"); return None, predictors_in_data\n",
        "    constant_cols = X.columns[X.nunique() <= 1].tolist()\n",
        "    if constant_cols: print(f\"    -> Warning: Constant columns removed for VIF: {constant_cols}\"); X = X.drop(columns=constant_cols); predictors_in_data = X.columns.tolist();\n",
        "    if X.empty or X.shape[1] < 1: print(f\"    -> No variables left after removing constant cols.\"); return None, predictors_in_data\n",
        "    try: X_vif = sm.add_constant(X, prepend=True, has_constant='skip')\n",
        "    except Exception as e: print(f\"    -> Error adding constant: {e}.\"); return None, predictors_in_data\n",
        "    if not np.all(np.isfinite(X_vif.values)): print(\"    -> Warning: Non-finite values detected. Attempting removal...\"); X_vif = X_vif.replace([np.inf, -np.inf], np.nan).dropna();\n",
        "    if X_vif.empty: print(\"    -> Error: Data empty after removing non-finite.\"); return None, predictors_in_data\n",
        "    try:\n",
        "        vif_data = pd.DataFrame(); vif_data[\"Variable\"] = [col for col in X_vif.columns if col.lower() != 'const']\n",
        "        vif_values = [variance_inflation_factor(X_vif.values.astype(float), i) for i, col in enumerate(X_vif.columns) if col.lower() != 'const']\n",
        "        vif_data[\"VIF\"] = vif_values; print(vif_data.sort_values('VIF', ascending=False).to_string(index=False))\n",
        "        high_vif_vars = vif_data[vif_data[\"VIF\"] > VIF_THRESHOLD][\"Variable\"].tolist()\n",
        "        if high_vif_vars: print(f\"    -> !!! WARNING: High VIF (> {VIF_THRESHOLD}): {high_vif_vars}. !!!\")\n",
        "        else: print(f\"    -> VIF check passed (threshold={VIF_THRESHOLD}).\")\n",
        "        return vif_data, predictors_in_data\n",
        "    except (np.linalg.LinAlgError, ValueError) as vif_calc_err: print(f\"    -> VIF calc failed: {vif_calc_err} (Perfect multicollinearity likely).\"); return None, predictors_in_data\n",
        "    except Exception as e: print(f\"    -> VIF error: {e}\"); traceback.print_exc(); return None, predictors_in_data\n",
        "\n",
        "# *** Use standardized variable name from Step 4 ***\n",
        "primary_esg_var = f'total_score_lag{ESG_LAG_MONTHS}' if f'total_score_lag{ESG_LAG_MONTHS}' in final_panel_data.columns else None\n",
        "component_esg_vars = [col for col in [f'e_score_lag{ESG_LAG_MONTHS}', f's_score_lag{ESG_LAG_MONTHS}', f'g_score_lag{ESG_LAG_MONTHS}'] if col in final_panel_data.columns]\n",
        "if primary_esg_var:\n",
        "    predictors_total_vif = available_factors + [primary_esg_var]; predictors_total_vif = [p for p in predictors_total_vif if p in final_panel_data.columns]\n",
        "    if predictors_total_vif: vif_results_total, _ = calculate_vif(final_panel_data.reset_index(), predictors_total_vif, \"Factors + Total ESG\")\n",
        "    else: print(\" -> Skipping VIF (Total ESG): No valid predictors.\")\n",
        "else: print(\"  -> Skipping VIF (Total ESG): Primary ESG var missing.\")\n",
        "if component_esg_vars:\n",
        "    predictors_components_vif = available_factors + component_esg_vars; predictors_components_vif = [p for p in predictors_components_vif if p in final_panel_data.columns]\n",
        "    if predictors_components_vif: vif_results_components, _ = calculate_vif(final_panel_data.reset_index(), predictors_components_vif, \"Factors + ESG Components\")\n",
        "    else: print(\" -> Skipping VIF (Components): No valid predictors.\")\n",
        "else: print(\"  -> Skipping VIF (Components): ESG component vars missing.\")\n",
        "if primary_esg_var: print(f\"\\n  -> Primary ESG var for models: '{primary_esg_var}'\")\n",
        "if component_esg_vars: print(f\"  -> Component ESG vars: {component_esg_vars}\")\n",
        "if not primary_esg_var and not component_esg_vars: print(\"\\n!!! WARNING: No lagged ESG vars available. !!!\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 6: Panel Regression Analysis ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 6. Panel Regression Analysis (Main Run - Imputed Data if Enabled) ---\")\n",
        "regression_results = {}; model_summaries = {}; sensitivity_regression_results = {}; model_formulas_used = {}\n",
        "sensitivity_summaries = {}; sensitivity_formulas_used = {} # Separate dicts for sensitivity\n",
        "\n",
        "formula_pooled_interaction = None; formula_fe_re_simple = None\n",
        "if primary_esg_var and available_factors:\n",
        "    base_factors_str = ' + '.join(available_factors); esg_term = primary_esg_var\n",
        "    # *** Use standardized variable name ***\n",
        "    formula_fe_re_simple = f\"ExcessReturn ~ 1 + {base_factors_str} + {esg_term}\"; print(f\"\\n  -> Formula for FE & RE models: {formula_fe_re_simple}\")\n",
        "    if lagged_category_col and lagged_category_col in final_panel_data.columns and final_panel_data[lagged_category_col].nunique() > 1:\n",
        "        preferred_reference_category = 'Medium'; print(f\"  -> Attempting to set '{preferred_reference_category}' as reference category for Pooled OLS.\")\n",
        "        available_cats = [c for c in final_panel_data[lagged_category_col].unique() if isinstance(c, str)]; final_reference_category = None\n",
        "        if preferred_reference_category in available_cats: final_reference_category = preferred_reference_category; print(f\"     -> Found exact match: Using '{final_reference_category}'.\")\n",
        "        else:\n",
        "            ref_cat_lower = preferred_reference_category.lower(); matching_cats = [c for c in available_cats if c.lower() == ref_cat_lower]\n",
        "            if matching_cats: final_reference_category = matching_cats[0]; print(f\"     -> Found case-insensitive match: Using '{final_reference_category}'.\")\n",
        "            elif available_cats:\n",
        "                 most_frequent_cat = final_panel_data[lagged_category_col].mode()\n",
        "                 if not most_frequent_cat.empty: final_reference_category = most_frequent_cat[0]; print(f\"     -> '{preferred_reference_category}' not found. Using most frequent category '{final_reference_category}' as fallback reference.\")\n",
        "                 else: print(f\"     -> Warning: Could not determine most frequent category. Cannot create Pooled OLS interaction formula.\")\n",
        "            else: print(f\"     -> Warning: No valid string categories found. Cannot create Pooled OLS interaction formula.\")\n",
        "        if final_reference_category: interaction_term = f\"{esg_term} * C({lagged_category_col}, Treatment(reference='{final_reference_category}'))\"; formula_pooled_interaction = f\"ExcessReturn ~ 1 + {base_factors_str} + {interaction_term}\"; print(f\"  -> Formula for Pooled OLS (Interaction): {formula_pooled_interaction}\")\n",
        "        else: print(f\"  -> Warn: Could not determine reference category. Using simple formula for Pooled OLS.\"); formula_pooled_interaction = formula_fe_re_simple\n",
        "    else:\n",
        "        if not lagged_category_col or lagged_category_col not in final_panel_data.columns: reason = \"missing\"\n",
        "        else: reason = \"has <= 1 unique value\"\n",
        "        print(f\"  -> Warn: Category column ('{lagged_category_col}') {reason}. Using simple formula for Pooled OLS.\"); formula_pooled_interaction = formula_fe_re_simple\n",
        "else: missing_info = [];\n",
        "if not primary_esg_var: missing_info.append(\"primary ESG variable\")\n",
        "if not available_factors: missing_info.append(\"factor variables\"); print(f\"\\n!!! CRITICAL WARNING: Cannot construct formulas (missing {', '.join(missing_info)}). Regression cannot proceed. !!!\")\n",
        "\n",
        "def run_panel_model(formula, model_type, model_key, data,\n",
        "                    cov_config={'cov_type':'clustered', 'cluster_entity':True, 'cluster_time': False},\n",
        "                    results_dict=None, summary_dict=None, formula_dict=None):\n",
        "    \"\"\"Fits panel model, handles errors, stores results/summaries.\"\"\"\n",
        "    print(f\"\\n  --- Fitting {model_key} ({model_type}) ---\")\n",
        "    if results_dict is None: results_dict = {}\n",
        "    if summary_dict is None: summary_dict = {}\n",
        "    if formula_dict is None: formula_dict = {}\n",
        "    results = None; error_msg = None; formula_status = \"Attempted\"\n",
        "    if not formula: error_msg = \"Skipped: No formula.\"; formula_status = \"Skipped - No Formula\"\n",
        "    elif data is None or data.empty: error_msg = \"Skipped: Empty data.\"; formula_status = \"Skipped - Empty Data\"\n",
        "    if error_msg: print(f\"    -> {error_msg}\"); summary_dict[model_key] = error_msg; results_dict[model_key] = None; formula_dict[model_key] = formula_status; return\n",
        "    try:\n",
        "        print(f\"    Using Formula: {formula}\")\n",
        "        dep, exog_formula = formula.split('~', 1); dep = dep.strip(); exog_formula = exog_formula.strip()\n",
        "        final_cov_config = cov_config.copy(); model = None; summary_obj = None\n",
        "        if model_type == 'Pooled':\n",
        "            pooled_cov_config = {'cov_type': 'robust'}; print(\"     (Note: Using robust covariance for Pooled OLS)\")\n",
        "            model = PanelOLS.from_formula(formula, data=data); final_cov_config = pooled_cov_config; formula_status = \"Pooled Spec (Robust SE)\"\n",
        "        elif model_type == 'RE':\n",
        "            model = RandomEffects.from_formula(formula, data=data)\n",
        "            if 'cluster' not in cov_config.get('cov_type','robust').lower(): final_cov_config = {'cov_type': 'robust'}\n",
        "            else: print(\"     (Note: Applying requested clustering to RE model)\")\n",
        "            formula_status = \"RE Spec\"\n",
        "        elif model_type == 'FE_Entity':\n",
        "            model = PanelOLS.from_formula(f\"{dep} ~ {exog_formula} + EntityEffects\", data=data, drop_absorbed=True); final_cov_config['cluster_time'] = False; formula_status = \"FE Entity Spec\"\n",
        "        elif model_type == 'FE_TwoWay':\n",
        "            model = PanelOLS.from_formula(f\"{dep} ~ {exog_formula} + EntityEffects + TimeEffects\", data=data, drop_absorbed=True); final_cov_config['cluster_time'] = True; formula_status = \"FE TwoWay Spec\"\n",
        "        else: raise ValueError(f\"Invalid model_type: {model_type}\")\n",
        "        results = model.fit(**final_cov_config); print(f\"    -> Fit OK.\")\n",
        "        try:\n",
        "             summary_obj = results.summary; print(f\"    -> Summary generation OK.\")\n",
        "             summary_dict[model_key] = summary_obj\n",
        "             if model_type.startswith('FE'):\n",
        "                 summary_str = str(summary_obj); # Define summary_str here\n",
        "                 if 'Absorbed' in summary_str or 'dropped' in summary_str.lower(): print(\"    -> Warning: Summary indicates potential variable absorption/dropping.\")\n",
        "        except (np.linalg.LinAlgError, ValueError) as summary_err: error_msg = f\"Error: Summary failed - {type(summary_err).__name__}: {summary_err} (Singular matrix likely).\"; print(f\"    -> {error_msg}\"); summary_dict[model_key] = error_msg; results_dict[model_key] = results; formula_dict[model_key] = formula_status + \" (Summary Failed)\"; return\n",
        "        except Exception as gen_summary_err: error_msg = f\"Error: Unexpected summary error - {type(gen_summary_err).__name__}: {gen_summary_err}\"; print(f\"    -> {error_msg}\"); summary_dict[model_key] = error_msg; results_dict[model_key] = results; formula_dict[model_key] = formula_status + \" (Summary Failed - Unknown)\"; return\n",
        "    except (ValueError, np.linalg.LinAlgError, PerfectSeparationError, ZeroDivisionError) as fit_err: error_msg = f\"Error: Fit failed - {type(fit_err).__name__}: {fit_err}\"; print(f\"    -> {error_msg}\")\n",
        "    except Exception as e: error_msg = f\"Error: Unexpected fit error - {type(e).__name__}: {e}\"; print(f\"    -> {error_msg}\"); traceback.print_exc(limit=1)\n",
        "    if results is not None and error_msg is None: results_dict[model_key] = results; formula_dict[model_key] = formula_status + \" (Success)\"\n",
        "    elif results is not None and error_msg is not None: pass\n",
        "    else: results_dict[model_key] = None; summary_dict[model_key] = error_msg; formula_dict[model_key] = formula_status + \" (Fit Failed)\"\n",
        "\n",
        "# --- Run Main Models ---\n",
        "if formula_pooled_interaction and formula_fe_re_simple:\n",
        "    run_panel_model(formula_pooled_interaction, 'Pooled', 'Pooled_Interaction', final_panel_data, results_dict=regression_results, summary_dict=model_summaries, formula_dict=model_formulas_used)\n",
        "    run_panel_model(formula_fe_re_simple, 'RE', 'RE_Simple', final_panel_data, results_dict=regression_results, summary_dict=model_summaries, formula_dict=model_formulas_used)\n",
        "    run_panel_model(formula_fe_re_simple, 'FE_Entity', 'FE_Entity_Simple', final_panel_data, results_dict=regression_results, summary_dict=model_summaries, formula_dict=model_formulas_used)\n",
        "    run_panel_model(formula_fe_re_simple, 'FE_TwoWay', 'FE_TwoWay_Simple', final_panel_data, results_dict=regression_results, summary_dict=model_summaries, formula_dict=model_formulas_used)\n",
        "else: print(\"\\n!!! Skipping main panel estimations: Missing required formulas. !!!\")\n",
        "\n",
        "# --- Run Sensitivity Analysis (No Imputation) ---\n",
        "if RUN_WITHOUT_IMPUTATION_SENSITIVITY:\n",
        "    print(\"\\n--- 6b. Panel Regression Analysis (Sensitivity Run - NO IMPUTATION) ---\")\n",
        "    if panel_data_no_imputation is None or panel_data_no_imputation.empty:\n",
        "        print(\"    -> Skipping Sensitivity: Non-imputed dataset empty.\")\n",
        "        sensitivity_regression_results['Pooled_Interaction_Sens'] = None; sensitivity_summaries['Pooled_Interaction_Sens'] = \"Skipped - Empty Data\"; sensitivity_formulas_used['Pooled_Interaction_Sens'] = \"Skipped - Empty Data\"\n",
        "        sensitivity_regression_results['FE_Entity_Simple_Sens'] = None; sensitivity_summaries['FE_Entity_Simple_Sens'] = \"Skipped - Empty Data\"; sensitivity_formulas_used['FE_Entity_Simple_Sens'] = \"Skipped - Empty Data\"\n",
        "    elif formula_pooled_interaction and formula_fe_re_simple:\n",
        "        panel_data_no_imputation_indexed = None\n",
        "        if isinstance(panel_data_no_imputation.index, pd.MultiIndex): panel_data_no_imputation_indexed = panel_data_no_imputation.copy()\n",
        "        elif {'Ticker', 'Date'}.issubset(panel_data_no_imputation.columns):\n",
        "             panel_data_no_imputation_idx_temp = panel_data_no_imputation.copy(); panel_data_no_imputation_idx_temp['Date'] = pd.to_datetime(panel_data_no_imputation_idx_temp['Date'])\n",
        "             panel_data_no_imputation_indexed = panel_data_no_imputation_idx_temp.set_index(['Ticker', 'Date']).sort_index()\n",
        "        if panel_data_no_imputation_indexed is not None:\n",
        "            run_panel_model(formula_pooled_interaction, 'Pooled', 'Pooled_Interaction_Sens', panel_data_no_imputation_indexed, results_dict=sensitivity_regression_results, summary_dict=sensitivity_summaries, formula_dict=sensitivity_formulas_used)\n",
        "            run_panel_model(formula_fe_re_simple, 'FE_Entity', 'FE_Entity_Simple_Sens', panel_data_no_imputation_indexed, results_dict=sensitivity_regression_results, summary_dict=sensitivity_summaries, formula_dict=sensitivity_formulas_used)\n",
        "        else: print(\"    -> Error: Cannot set index for sensitivity data. Skipping.\"); sensitivity_summaries['Pooled_Interaction_Sens'] = \"Skipped - Index Error\"; sensitivity_summaries['FE_Entity_Simple_Sens'] = \"Skipped - Index Error\"; sensitivity_formulas_used['Pooled_Interaction_Sens'] = \"Skipped - Index Error\"; sensitivity_formulas_used['FE_Entity_Simple_Sens'] = \"Skipped - Index Error\"\n",
        "    else: print(\"    -> Skipping Sensitivity: Missing required formulas.\"); sensitivity_summaries['Pooled_Interaction_Sens'] = \"Skipped - No Formula\"; sensitivity_summaries['FE_Entity_Simple_Sens'] = \"Skipped - No Formula\"; sensitivity_formulas_used['Pooled_Interaction_Sens'] = \"Skipped - No Formula\"; sensitivity_formulas_used['FE_Entity_Simple_Sens'] = \"Skipped - No Formula\"\n",
        "else: print(\"\\n--- Sensitivity Analysis Skipped (Configured Off). ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 7: Specification Tests & Interpretation ---\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 7. Specification Tests & Interpretation (using Main Run results) ---\")\n",
        "spec_test_results_list = []; preferred_model_key = None\n",
        "fe_model = regression_results.get('FE_Entity_Simple'); re_model = regression_results.get('RE_Simple')\n",
        "pooled_model = regression_results.get('Pooled_Interaction'); fe_tw_model = regression_results.get('FE_TwoWay_Simple')\n",
        "is_fe_valid = isinstance(fe_model, PanelEffectsResults); is_re_valid = isinstance(re_model, RandomEffectsResults)\n",
        "is_pooled_valid = isinstance(pooled_model, PanelEffectsResults); is_fe_tw_valid = isinstance(fe_tw_model, PanelEffectsResults)\n",
        "\n",
        "try:\n",
        "    print(\"\\n    Comparing FE (Simple) vs RE (Simple) - Hausman Test:\")\n",
        "    if is_fe_valid and is_re_valid:\n",
        "        try:\n",
        "            common_params = list(set(fe_model.params.index) & set(re_model.params.index))\n",
        "            if not common_params: print(\"      -> Skipping Hausman: No common parameters.\"); spec_test_results_list.append({'Test': 'Hausman (FE vs RE - Simple)', 'Details': 'No common parameters', 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "            else:\n",
        "                print(\"      -> Performing Hausman test via model comparison...\"); comparison_fe_re = model_compare({\"FE_Simple\": fe_model, \"RE_Simple\": re_model})\n",
        "                print(comparison_fe_re); hausman_pval_str = \"See Table\"; pval_num = np.nan\n",
        "                try:\n",
        "                    summary_str = str(comparison_fe_re); match = re.search(r\"Hausman\\s+([\\d\\.]+)\", summary_str);\n",
        "                    if match: pval_num = float(match.group(1)); hausman_pval_str = f\"{pval_num:.4f}\"\n",
        "                except Exception as parse_err: print(f\"       -> Warning: Could not parse Hausman p-value: {parse_err}\")\n",
        "                conclusion_hausman_test = 'Check Table';\n",
        "                if not pd.isna(pval_num): conclusion_hausman_test = 'Prefer FE if Hausman p < 0.05'\n",
        "                spec_test_results_list.append({'Test': 'Hausman (FE vs RE - Simple)', 'Details': 'Comparison table printed', 'P-value': hausman_pval_str, 'Conclusion': conclusion_hausman_test})\n",
        "        except Exception as comp_e: print(f\"      -> Error running Hausman comparison: {comp_e}\"); spec_test_results_list.append({'Test': 'Hausman (FE vs RE - Simple)', 'Details': f\"Error: {comp_e}\", 'P-value': '-', 'Conclusion': 'Error'})\n",
        "    else: details = \"RE invalid\" if is_fe_valid else \"FE invalid\" if is_re_valid else \"Both invalid\"; print(f\"\\n    Skipping Hausman test: {details}.\"); spec_test_results_list.append({'Test': 'Hausman (FE vs RE - Simple)', 'Details': details, 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "\n",
        "    print(\"\\n    F-test for Poolability (Entity Effects):\")\n",
        "    if is_fe_valid:\n",
        "        try:\n",
        "            if hasattr(fe_model, 'f_pooled'):\n",
        "                f_pool = fe_model.f_pooled; stat_val = f_pool.stat; pval_val = f_pool.pval; df_num = getattr(f_pool, 'df_num', '?'); df_denom = getattr(f_pool, 'df_denom', '?')\n",
        "                print(f\"      F={stat_val:.4f}, P-value={pval_val:.4f} (df_num={df_num}, df_denom={df_denom})\"); conclusion = 'Reject Pooling (Use FE)' if pval_val < 0.05 else 'Cannot Reject Pooling (Pooled OK)'\n",
        "                spec_test_results_list.append({'Test': 'F-test (Poolability - Entity)', 'Details': f'F({df_num},{df_denom})={stat_val:.4f}', 'P-value': f'{pval_val:.4f}', 'Conclusion': conclusion})\n",
        "            else: print(\"      -> Poolability F-stat (f_pooled) not available.\"); spec_test_results_list.append({'Test': 'F-test (Poolability - Entity)', 'Details': 'f_pooled unavailable', 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "        except Exception as ftest_e: print(f\"      -> Error accessing Poolability F-test: {ftest_e}\"); spec_test_results_list.append({'Test': 'F-test (Poolability - Entity)', 'Details': f\"Error: {ftest_e}\", 'P-value': '-', 'Conclusion': 'Error'})\n",
        "    else: print(\"      -> Skipping Poolability F-test: FE (Simple) model invalid.\"); spec_test_results_list.append({'Test': 'F-test (Poolability - Entity)', 'Details': 'FE Invalid', 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "\n",
        "    print(\"\\n    F-test for Time Effects:\");\n",
        "    if is_fe_tw_valid:\n",
        "         try:\n",
        "            if hasattr(fe_tw_model, 'f_test_time'):\n",
        "                f_time = fe_tw_model.f_test_time; stat_val = f_time.stat; pval_val = f_time.pval; df_num = getattr(f_time, 'df_num', '?'); df_denom = getattr(f_time, 'df_denom', '?')\n",
        "                print(f\"      F={stat_val:.4f}, P-value={pval_val:.4f} (df_num={df_num}, df_denom={df_denom})\"); conclusion = 'Time Effects Significant (Use Two-Way FE)' if pval_val < 0.05 else 'Time Effects Not Significant (Entity FE OK)'\n",
        "                spec_test_results_list.append({'Test': 'F-test (Time Effects)', 'Details': f'F({df_num},{df_denom})={stat_val:.4f}', 'P-value': f'{pval_val:.4f}', 'Conclusion': conclusion})\n",
        "            else: print(\"      -> Time Effects F-stat (f_test_time) not available.\"); spec_test_results_list.append({'Test': 'F-test (Time Effects)', 'Details': 'f_test_time unavailable', 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "         except Exception as ftest_e: print(f\"      -> Error accessing Time Effects F-test: {ftest_e}\"); spec_test_results_list.append({'Test': 'F-test (Time Effects)', 'Details': f\"Error: {ftest_e}\", 'P-value': '-', 'Conclusion': 'Error'})\n",
        "    else: print(\"      -> Skipping Time Effects F-test: Two-Way FE (Simple) model invalid.\"); spec_test_results_list.append({'Test': 'F-test (Time Effects)', 'Details': 'Two-Way FE Invalid', 'P-value': '-', 'Conclusion': 'Cannot Run'})\n",
        "    spec_test_df = pd.DataFrame(spec_test_results_list)\n",
        "except Exception as e: print(f\"\\nError during spec tests: {e}\"); traceback.print_exc(); spec_test_df = pd.DataFrame()\n",
        "\n",
        "print(\"\\n--- Preferred Model Selection Logic (Main Run) ---\")\n",
        "conclusion_pool = 'Cannot Run'; conclusion_time = 'Cannot Run'; conclusion_hausman = 'Cannot Run'\n",
        "if not spec_test_df.empty:\n",
        "    pool_row = spec_test_df[spec_test_df['Test'] == 'F-test (Poolability - Entity)']; conclusion_pool = pool_row['Conclusion'].iloc[0] if not pool_row.empty else 'Cannot Run'\n",
        "    time_row = spec_test_df[spec_test_df['Test'] == 'F-test (Time Effects)']; conclusion_time = time_row['Conclusion'].iloc[0] if not time_row.empty else 'Cannot Run'\n",
        "    hausman_row = spec_test_df[spec_test_df['Test'] == 'Hausman (FE vs RE - Simple)']; conclusion_hausman = hausman_row['Conclusion'].iloc[0] if not hausman_row.empty else 'Cannot Run'\n",
        "print(f\"  - Poolability Test Result: '{conclusion_pool}'\"); print(f\"  - Hausman Test (Simple Models) Result: '{conclusion_hausman}'\"); print(f\"  - Time Effects Test Result: '{conclusion_time}'\")\n",
        "\n",
        "preferred_model_key = None # Reset\n",
        "if 'Time Effects Significant' in conclusion_time and is_fe_tw_valid:\n",
        "    print(\"  - Logic: Time effects significant & Two-Way FE valid.\"); preferred_model_key = 'FE_TwoWay_Simple'; print(\"  -> Tentative Preference: FE Two-Way (Simple).\")\n",
        "elif 'Reject Pooling' in conclusion_pool and is_fe_valid:\n",
        "    print(\"  - Logic: Pooling rejected & Entity FE valid.\")\n",
        "    if 'Prefer FE' in conclusion_hausman: print(\"  - Logic: Hausman also prefers FE.\"); preferred_model_key = 'FE_Entity_Simple'; print(\"  -> Tentative Preference: FE Entity (Simple).\")\n",
        "    else: print(f\"  - Logic: Hausman ({conclusion_hausman}), but Poolability rejects Pooled. Prioritizing FE Entity.\"); preferred_model_key = 'FE_Entity_Simple'; print(\"  -> Tentative Preference: FE Entity (Simple).\")\n",
        "elif is_re_valid and 'Prefer FE' not in conclusion_hausman:\n",
        "     print(\"  - Logic: FE not selected/valid, RE valid, Hausman doesn't strongly prefer FE.\"); preferred_model_key = 'RE_Simple'; print(\"  -> Tentative Preference: RE (Simple).\")\n",
        "elif 'Cannot Reject Pooling' in conclusion_pool and is_pooled_valid:\n",
        "     print(\"  - Logic: FE/RE models not selected/valid, Pooling allowed, Pooled OLS valid.\"); preferred_model_key = 'Pooled_Interaction'; print(\"  -> Tentative Preference: Pooled OLS (Interaction).\")\n",
        "if preferred_model_key is None:\n",
        "    print(\"\\n  - No model selected by primary logic. Applying fallback...\")\n",
        "    fallback_order = ['FE_TwoWay_Simple', 'FE_Entity_Simple', 'RE_Simple', 'Pooled_Interaction']\n",
        "    for model_key_fb in fallback_order:\n",
        "        if model_key_fb in regression_results and isinstance(regression_results.get(model_key_fb), (PanelEffectsResults, RandomEffectsResults)):\n",
        "            summary_val = model_summaries.get(model_key_fb)\n",
        "            if not isinstance(summary_val, str): preferred_model_key = model_key_fb; print(f\"  -> Fallback Selection: '{preferred_model_key}'.\"); break\n",
        "            else: print(f\"      -> Skipping fallback {model_key_fb} (summary failed).\")\n",
        "if preferred_model_key and (preferred_model_key not in regression_results or not isinstance(regression_results.get(preferred_model_key), (PanelEffectsResults, RandomEffectsResults))): print(f\"  -> ERROR: Preferred model '{preferred_model_key}' not valid. Resetting.\"); preferred_model_key = None\n",
        "if preferred_model_key and preferred_model_key in model_summaries and isinstance(model_summaries[preferred_model_key], str): print(f\"  -> ERROR: Preferred model '{preferred_model_key}' failed summary. Resetting.\"); preferred_model_key = None\n",
        "if preferred_model_key is None: print(\"\\n  -> !!! CRITICAL: No valid model results available after fallback. !!!\")\n",
        "print(f\"\\n---> Final Preferred Model Selected (Main Run): {preferred_model_key if preferred_model_key else 'None (Review Logs)'} <---\")\n",
        "\n",
        "print(f\"\\n--- Interpretation (Based on '{preferred_model_key if preferred_model_key else 'Available Models'}') ---\")\n",
        "interpretation_provided = False\n",
        "def get_coeff_info(results, var_name):\n",
        "    if results and hasattr(results, 'params') and var_name in results.params.index:\n",
        "        coeff = results.params.get(var_name); pval = results.pvalues.get(var_name); stderr = results.std_errors.get(var_name)\n",
        "        sig_marker = '***' if pval < 0.001 else '**' if pval < 0.01 else '*' if pval < 0.05 else '.' if pval < 0.1 else ''\n",
        "        return f\"Coeff={coeff:.4f}, SE={stderr:.4f}, Pval={pval:.4f} {sig_marker}\"\n",
        "    return f\"Variable '{var_name}' not found or results invalid.\"\n",
        "\n",
        "if preferred_model_key and preferred_model_key in regression_results and isinstance(regression_results[preferred_model_key], (PanelEffectsResults, RandomEffectsResults)):\n",
        "    preferred_model_results = regression_results[preferred_model_key]; formula_used_key = model_formulas_used.get(preferred_model_key, \"Unknown\")\n",
        "    print(f\"    Interpreting Preferred Model: '{preferred_model_key}' (Formula Type: {formula_used_key})\")\n",
        "    if primary_esg_var:\n",
        "        print(f\"\\n    Interpretation for '{primary_esg_var}':\")\n",
        "        if preferred_model_key == 'Pooled_Interaction' and any(':' in p for p in preferred_model_results.params.index):\n",
        "             ref_cat_used = 'Unknown'; formula_string_to_parse = formula_pooled_interaction\n",
        "             if formula_string_to_parse:\n",
        "                  try: match = re.search(r\"Treatment\\(reference='([^']+)'\\)\", formula_string_to_parse); ref_cat_used = match.group(1) if match else 'Unknown'\n",
        "                  except Exception: pass\n",
        "             print(f\"      Model includes interactions (Ref Cat: '{ref_cat_used}')\"); print(f\"      - Baseline Effect ({ref_cat_used}): {get_coeff_info(preferred_model_results, primary_esg_var)}\"); print(\"      - (Interactions: See Pooled summary)\")\n",
        "        elif preferred_model_key in ['FE_Entity_Simple', 'FE_TwoWay_Simple', 'RE_Simple']:\n",
        "             effects_description = \"Unknown\"\n",
        "             if 'FE_Entity' in preferred_model_key: effects_description = \"Entity Fixed\"\n",
        "             elif 'FE_TwoWay' in preferred_model_key: effects_description = \"Entity & Time Fixed\"\n",
        "             elif 'RE' in preferred_model_key: effects_description = \"Random Entity\"\n",
        "             print(f\"      Model estimates avg effect controlling for {effects_description} effects.\"); print(f\"      - Average Effect: {get_coeff_info(preferred_model_results, primary_esg_var)}\")\n",
        "        else: print(f\"      - Unknown structure. Basic Effect: {get_coeff_info(preferred_model_results, primary_esg_var)}\")\n",
        "    else: print(\"    -> Primary ESG variable not specified/found.\")\n",
        "    print(\"\\n    Interpretation Notes:\"); print(f\"      - Results based on '{preferred_model_key}'. Check Pval.\");\n",
        "    if IMPUTE_DATA and initial_missing_stats: print(\"      - !!! CAVEAT: Uses imputed data. High initial missingness. Compare w/ Sensitivity. !!!\")\n",
        "    print(\"      - Consider economic significance.\"); interpretation_provided = True\n",
        "elif RUN_WITHOUT_IMPUTATION_SENSITIVITY:\n",
        "    print(\"\\n    -> Main run failed/not selected. Checking Sensitivity Run...\")\n",
        "    sens_key_to_interpret = None; sens_results = None; sens_pref_order = ['FE_Entity_Simple_Sens', 'Pooled_Interaction_Sens']\n",
        "    for key in sens_pref_order:\n",
        "        result_obj = sensitivity_regression_results.get(key) # Check result object\n",
        "        summary_obj_or_err = sensitivity_summaries.get(key) # Check summary object/error\n",
        "        if isinstance(result_obj, (PanelEffectsResults, RandomEffectsResults)) and not isinstance(summary_obj_or_err, str):\n",
        "            sens_key_to_interpret = key; sens_results = result_obj; break\n",
        "    if sens_key_to_interpret and sens_results:\n",
        "        print(f\"    -> Interpreting Sensitivity Model: '{sens_key_to_interpret}' (NO IMPUTATION) as fallback.\"); print(\"       !!! CAVEATS: Smaller subset of data. !!!\")\n",
        "        if primary_esg_var: print(f\"       - ESG Effect ('{primary_esg_var}'): {get_coeff_info(sens_results, primary_esg_var)}\")\n",
        "        else: print(\"       - Primary ESG variable not found.\"); interpretation_provided = True\n",
        "    else: print(\"    -> Sensitivity models also failed or were skipped.\")\n",
        "if not interpretation_provided: print(\"\\n    -> No valid model results found to provide interpretation.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Step 9: Print Consolidated Results to Console ---\n",
        "# ==============================================================================\n",
        "print(\"\\n\\n==============================================================================\")\n",
        "print(f\"--- 9. Consolidated Analysis Results ({SCRIPT_VERSION}) ---\")\n",
        "print(\"==============================================================================\")\n",
        "\n",
        "print(f\"\\n--- Panel Model Summaries (Main Run - Imputed: {IMPUTE_DATA}) ---\")\n",
        "if not model_summaries: print(\"No models run/attempted.\")\n",
        "else:\n",
        "    model_order = ['Pooled_Interaction', 'RE_Simple', 'FE_Entity_Simple', 'FE_TwoWay_Simple']\n",
        "    for name in model_order:\n",
        "        if name in model_summaries:\n",
        "            result_or_error_summary = model_summaries[name]\n",
        "            formula_used_key = model_formulas_used.get(name, \"Unknown\")\n",
        "            print(f\"\\n--- Model: {name} ---\"); print(f\"    Formula Type Used: {formula_used_key}\")\n",
        "            if isinstance(result_or_error_summary, str): print(f\"    -> Model Failed/Summary Error: {result_or_error_summary}\")\n",
        "            elif hasattr(result_or_error_summary, 'tables'):\n",
        "                try:\n",
        "                    if hasattr(result_or_error_summary, 'tables') and isinstance(result_or_error_summary.tables, list):\n",
        "                         for table in result_or_error_summary.tables: print(table.as_text())\n",
        "                    else: print(str(result_or_error_summary))\n",
        "                except Exception as e: print(f\"    -> Error formatting/printing summary tables for '{name}': {e}\");\n",
        "                try: print(str(result_or_error_summary))\n",
        "                except: print(\"    -> Could not even convert summary object to string.\")\n",
        "            else: print(f\"    -> Unknown result status stored (Type: {type(result_or_error_summary)}): {result_or_error_summary}\")\n",
        "\n",
        "if RUN_WITHOUT_IMPUTATION_SENSITIVITY:\n",
        "    print(\"\\n\\n--- Panel Model Summaries (Sensitivity Run - NO IMPUTATION) ---\")\n",
        "    if not sensitivity_summaries: print(\"No sensitivity models run/results available.\")\n",
        "    else:\n",
        "        sens_model_order = ['Pooled_Interaction_Sens', 'FE_Entity_Simple_Sens']\n",
        "        for name in sens_model_order:\n",
        "             if name in sensitivity_summaries:\n",
        "                result_or_error_summary = sensitivity_summaries[name]\n",
        "                formula_used_key = sensitivity_formulas_used.get(name, \"Unknown\")\n",
        "                print(f\"\\n--- Model: {name} ---\"); print(f\"    Formula Type Used: {formula_used_key}\")\n",
        "                if isinstance(result_or_error_summary, str): print(f\"    -> Model Failed/Summary Error: {result_or_error_summary}\")\n",
        "                elif hasattr(result_or_error_summary, 'tables'):\n",
        "                     try:\n",
        "                         if hasattr(result_or_error_summary, 'tables') and isinstance(result_or_error_summary.tables, list):\n",
        "                              for table in result_or_error_summary.tables: print(table.as_text())\n",
        "                         else: print(str(result_or_error_summary))\n",
        "                     except Exception as e: print(f\"    -> Error formatting/printing summary tables for '{name}': {e}\");\n",
        "                     try: print(str(result_or_error_summary))\n",
        "                     except: print(\"    -> Could not even convert summary object to string.\")\n",
        "                else: print(f\"    -> Unknown result status stored for {name} (Type: {type(result_or_error_summary)}): {result_or_error_summary}\")\n",
        "\n",
        "print(f\"\\n\\n--- Preferred Model Selection & Comparison (Main Run) ---\")\n",
        "print(f\"  -> Preferred model selected: {preferred_model_key if preferred_model_key else 'None (Review Logs)'}\"); print(f\"  -> Review specification tests and theory.\")\n",
        "if preferred_model_key and RUN_WITHOUT_IMPUTATION_SENSITIVITY:\n",
        "    sens_key_map = {'Pooled_Interaction': 'Pooled_Interaction_Sens', 'FE_Entity_Simple': 'FE_Entity_Simple_Sens', 'FE_TwoWay_Simple': None, 'RE_Simple': None}\n",
        "    sens_key = sens_key_map.get(preferred_model_key)\n",
        "    if sens_key and sens_key in sensitivity_summaries:\n",
        "        sens_result_or_error = sensitivity_summaries[sens_key]\n",
        "        if not isinstance(sens_result_or_error, str): print(f\"  -> COMPARISON: Sensitivity '{sens_key}' ran successfully. Compare results.\")\n",
        "        else: print(f\"  -> COMPARISON NOTE: Sensitivity counterpart '{sens_key}' failed/skipped ({sens_result_or_error}).\")\n",
        "    elif sens_key: print(f\"  -> COMPARISON NOTE: Sensitivity counterpart '{sens_key}' not found in sensitivity results.\")\n",
        "    else: print(f\"  -> COMPARISON NOTE: No sensitivity counterpart defined for '{preferred_model_key}'.\")\n",
        "\n",
        "print(\"\\n--- Interpretation Summary (Refer to Step 7) ---\")\n",
        "if interpretation_provided: print(\"  -> Interpretation provided in Step 7.\")\n",
        "else: print(\"  -> No interpretation provided (model failures).\")\n",
        "\n",
        "print(\"\\n--- Specification Tests Summary (Main Run) ---\")\n",
        "if 'spec_test_df' in locals() and isinstance(spec_test_df, pd.DataFrame) and not spec_test_df.empty:\n",
        "    try: print(spec_test_df.to_string(index=False, justify='left', max_colwidth=60))\n",
        "    except Exception as print_err: print(f\"  -> Print error: {print_err}\"); print(spec_test_df)\n",
        "else: print(\"  -> Specification tests unavailable.\")\n",
        "\n",
        "print(\"\\n--- VIF Results (Main Run Data) ---\"); print(\"  (VIF > 10 indicates potential issues)\")\n",
        "if vif_results_total is not None and isinstance(vif_results_total, pd.DataFrame): print(\"\\n  VIF (Factors + Total ESG):\"); print(vif_results_total.sort_values('VIF', ascending=False).to_string(index=False))\n",
        "else: print(\"\\n  VIF check (Total ESG) N/A or failed.\")\n",
        "if vif_results_components is not None and isinstance(vif_results_components, pd.DataFrame): print(\"\\n  VIF (Factors + ESG Components):\"); print(vif_results_components.sort_values('VIF', ascending=False).to_string(index=False))\n",
        "else: print(\"\\n  VIF check (Components) N/A or failed.\")\n",
        "\n",
        "print(\"\\n\\n--- Overall Reliability Assessment & Disclaimers ---\")\n",
        "print(\"  - Data Quality: Verify sources (Steps 2 & 3 logs).\")\n",
        "print(f\"  - Imputation ({'MICE' if IMPUTE_DATA else 'Disabled'}): Main run {'used' if IMPUTE_DATA else 'did not use'} imputation.\")\n",
        "if IMPUTE_DATA and initial_missing_stats: high_missing_cols = [col for col, pct in initial_missing_stats.items() if pct > 25];\n",
        "if IMPUTE_DATA and initial_missing_stats and high_missing_cols: print(f\"    -> !!! CONCERN: High initial missingness (>25%): {high_missing_cols}.\")\n",
        "if IMPUTE_DATA: print(\"    -> Compare Main Run vs. Sensitivity Run (if successful).\")\n",
        "print(\"  - Model Specification & Validity:\")\n",
        "main_models_failed_count = sum(1 for res in model_summaries.values() if isinstance(res, str))\n",
        "if main_models_failed_count == len(model_summaries): print(\"    -> !!! CRITICAL: All main run models failed. Review Step 6 logs. !!!\")\n",
        "elif main_models_failed_count > 0: print(f\"    -> Warning: {main_models_failed_count} main run model(s) failed/summary error. Review logs.\")\n",
        "# Check specific model failures\n",
        "if 'Pooled_Interaction' in model_summaries and isinstance(model_summaries['Pooled_Interaction'], str): print(\"    -> Warning: Pooled OLS model failed or had summary error.\")\n",
        "elif 'Pooled_Interaction' in regression_results and isinstance(regression_results['Pooled_Interaction'], PanelEffectsResults):\n",
        "    try:\n",
        "        pooled_f_robust = regression_results['Pooled_Interaction'].f_statistic_robust.stat\n",
        "        if not np.isfinite(pooled_f_robust) or pooled_f_robust < 0 :\n",
        "             print(\"    -> Warning: Pooled OLS robust F-stat appears invalid. Check SE calculation.\")\n",
        "    except Exception:\n",
        "        print(\"    -> Warning: Could not access Pooled OLS robust F-statistic.\")\n",
        "        pass\n",
        "if 'RE_Simple' in model_summaries and isinstance(model_summaries['RE_Simple'], str): print(\"    -> Warning: RE model failed. Hausman test invalid.\")\n",
        "fe_models_to_check = ['FE_Entity_Simple', 'FE_TwoWay_Simple']\n",
        "for fe_key in fe_models_to_check:\n",
        "     if fe_key in model_summaries:\n",
        "          summary_val = model_summaries[fe_key]\n",
        "          if isinstance(summary_val, str) and ('Singular' in summary_val or 'Error' in summary_val): print(f\"    -> Warning: {fe_key} failed summary ({summary_val[:60]}...).\")\n",
        "          elif not isinstance(summary_val, str) and hasattr(summary_val, 'summary'):\n",
        "               summary_str = str(summary_val);\n",
        "               if 'Absorbed' in summary_str or 'dropped' in summary_str.lower(): print(f\"    -> Warning: {fe_key} summary indicates absorbed/dropped vars.\")\n",
        "\n",
        "print(\"    -> Note: FE/RE models used simplified spec (no category interactions). Pooled OLS used interactions.\")\n",
        "print(\"    -> Review model selection (Step 7) and theoretical fit.\")\n",
        "print(\"  - Data Characteristics: Check VIF results and Step 4 category warnings.\")\n",
        "print(\"\\n  --- Conclusion ---\"); print(\"  -> Treat findings with caution. Prioritize successful Sensitivity Run if Main had issues/imputation.\")\n",
        "\n",
        "print(\"\\n==============================================================================\")\n",
        "print(\"--- End of Consolidated Results ---\")\n",
        "print(\"==============================================================================\")\n",
        "\n",
        "print(f\"\\n--- Script Finished ({SCRIPT_VERSION}) ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URoY4AceZOQV",
        "outputId": "b7d4e6e1-4715-4160-eca2-8850a46aeda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting analysis for tickers: ['UPS', 'FDX', 'XPO', 'JYD', 'AMKBY', 'CNI', 'CP', 'UNP', 'CSX', 'ODFL', 'JBHT', 'ZIM']\n",
            "--- Global Transportation ESG Impact Analysis Script Started (Panel Only v14.1 - Transportation & FF DataReader) ---\n",
            "Analysis Period: 2015-01-01 to 2023-12-31\n",
            "ESG Lag: 1 months\n",
            "Imputation Enabled (Main Run - MICE): True\n",
            "Run Sensitivity without Imputation: True\n",
            "Factors Path: Data fetched from K. French Library\n",
            "ESG Data Path: /content/historic_esg_scores_global_transportation.csv (Source/Quality Not Verified by Script)\n",
            "\n",
            "--- 1. Downloading and Preparing Stock Returns ---\n",
            "  -> Stock price data processed for 12 tickers: ['AMKBY', 'CNI', 'CP', 'CSX', 'FDX', 'JBHT', 'JYD', 'ODFL', 'UNP', 'UPS', 'XPO', 'ZIM']\n",
            "  -> Stock monthly returns prepared: 2014-10-31 to 2024-12-31\n",
            "\n",
            "--- 2. Loading and Preparing Fama-French Factors ---\n",
            "  -> Downloading Fama-French factors using pandas-datareader...\n",
            "  -> Factors downloaded and merged successfully.\n",
            "  -> Raw factors shape: (132, 7)\n",
            "  -> No missing values detected in fetched factors.\n",
            "  -> Using 'rf' as RF.\n",
            "  -> Available factors identified: ['cma', 'hml', 'mkt_rf', 'rmw', 'smb']\n",
            "\n",
            "--- 3. Loading and Preparing ESG Data from CSV ---\n",
            "  -> Raw ESG data loaded. Shape: (915, 6)\n",
            "  -> Standardized ESG columns: ['date', 'total_score', 'e_score', 's_score', 'g_score', 'ticker']\n",
            "  -> Converting score columns to numeric: ['total_score', 'e_score', 's_score', 'g_score']\n",
            "  -> Common tickers identified: ['CNI', 'CP', 'CSX', 'FDX', 'JBHT', 'ODFL', 'UNP', 'UPS', 'XPO', 'ZIM'] (10 firms)\n",
            "    -> Tickers in Stock only: ['AMKBY', 'JYD']\n",
            "  -> Creating ESG panel from 2014-10-31 to 2023-12-31\n",
            "  -> Forward-filling ESG scores...\n",
            "  -> !!! WARNING: NaNs remain after ffill. Imputation will attempt. Counts:\n",
            "total_score    333\n",
            "e_score        333\n",
            "s_score        333\n",
            "g_score        333\n",
            "dtype: int64\n",
            "  -> ESG panel structure created. Shape: (1110, 6)\n",
            "\n",
            "--- 5. Checking for Multicollinearity (VIF) ---\n",
            "\n",
            "  Calculating VIF for: Factors + Total ESG\n",
            "        Variable    VIF\n",
            "             hml 4.8649\n",
            "             cma 4.0008\n",
            "             rmw 1.9476\n",
            "          mkt_rf 1.3215\n",
            "             smb 1.2618\n",
            "total_score_lag1 1.0157\n",
            "    -> VIF check passed (threshold=10).\n",
            "\n",
            "  Calculating VIF for: Factors + ESG Components\n",
            "    Variable     VIF\n",
            "s_score_lag1 21.0431\n",
            "g_score_lag1 15.7938\n",
            "e_score_lag1 14.3221\n",
            "         hml  4.8656\n",
            "         cma  4.0037\n",
            "         rmw  1.9501\n",
            "      mkt_rf  1.3216\n",
            "         smb  1.2650\n",
            "    -> !!! WARNING: High VIF (> 10): ['e_score_lag1', 's_score_lag1', 'g_score_lag1']. !!!\n",
            "\n",
            "  -> Primary ESG var for models: 'total_score_lag1'\n",
            "  -> Component ESG vars: ['e_score_lag1', 's_score_lag1', 'g_score_lag1']\n",
            "\n",
            "--- 6. Panel Regression Analysis (Main Run - Imputed Data if Enabled) ---\n",
            "\n",
            "  -> Formula for FE & RE models: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1\n",
            "  -> Attempting to set 'Medium' as reference category for Pooled OLS.\n",
            "     -> Found exact match: Using 'Medium'.\n",
            "  -> Formula for Pooled OLS (Interaction): ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1 * C(ESG_Category_lag1, Treatment(reference='Medium'))\n",
            "\n",
            "  --- Fitting Pooled_Interaction (Pooled) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1 * C(ESG_Category_lag1, Treatment(reference='Medium'))\n",
            "     (Note: Using robust covariance for Pooled OLS)\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "  --- Fitting RE_Simple (RE) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1\n",
            "     (Note: Applying requested clustering to RE model)\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "  --- Fitting FE_Entity_Simple (FE_Entity) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "  --- Fitting FE_TwoWay_Simple (FE_TwoWay) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "--- 6b. Panel Regression Analysis (Sensitivity Run - NO IMPUTATION) ---\n",
            "\n",
            "  --- Fitting Pooled_Interaction_Sens (Pooled) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1 * C(ESG_Category_lag1, Treatment(reference='Medium'))\n",
            "     (Note: Using robust covariance for Pooled OLS)\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "  --- Fitting FE_Entity_Simple_Sens (FE_Entity) ---\n",
            "    Using Formula: ExcessReturn ~ 1 + cma + hml + mkt_rf + rmw + smb + total_score_lag1\n",
            "    -> Fit OK.\n",
            "    -> Summary generation OK.\n",
            "\n",
            "--- 7. Specification Tests & Interpretation (using Main Run results) ---\n",
            "\n",
            "    Comparing FE (Simple) vs RE (Simple) - Hausman Test:\n",
            "      -> Performing Hausman test via model comparison...\n",
            "                     Model Comparison                     \n",
            "==========================================================\n",
            "                               FE_Simple         RE_Simple\n",
            "----------------------------------------------------------\n",
            "Dep. Variable               ExcessReturn      ExcessReturn\n",
            "Estimator                       PanelOLS     RandomEffects\n",
            "No. Observations                    1080              1080\n",
            "Cov. Est.                      Clustered         Clustered\n",
            "R-squared                         0.1880            0.1867\n",
            "R-Squared (Within)                0.1880            0.1880\n",
            "R-Squared (Between)              -0.7763           -0.7255\n",
            "R-Squared (Overall)               0.1866            0.1867\n",
            "F-statistic                       41.061            41.047\n",
            "P-value (F-stat)                  0.0000            0.0000\n",
            "=====================     ==============   ===============\n",
            "Intercept                        -0.1717           -0.1693\n",
            "                               (-30.621)         (-50.257)\n",
            "cma                               0.0091            0.0091\n",
            "                                (2.8368)          (2.8345)\n",
            "hml                               0.0005            0.0005\n",
            "                                (0.2235)          (0.2121)\n",
            "mkt_rf                            0.0129            0.0128\n",
            "                                (12.189)          (12.180)\n",
            "rmw                               0.0126            0.0125\n",
            "                                (9.1122)          (9.0944)\n",
            "smb                               0.0186            0.0187\n",
            "                                (17.536)          (17.565)\n",
            "total_score_lag1                  0.0016            0.0016\n",
            "                                (11.672)          (12.415)\n",
            "======================= ================ =================\n",
            "Effects                           Entity                  \n",
            "----------------------------------------------------------\n",
            "\n",
            "T-stats reported in parentheses\n",
            "\n",
            "    F-test for Poolability (Entity Effects):\n",
            "      F=0.3624, P-value=0.9527 (df_num=?, df_denom=1064)\n",
            "\n",
            "    F-test for Time Effects:\n",
            "      -> Time Effects F-stat (f_test_time) not available.\n",
            "\n",
            "--- Preferred Model Selection Logic (Main Run) ---\n",
            "  - Poolability Test Result: 'Cannot Reject Pooling (Pooled OK)'\n",
            "  - Hausman Test (Simple Models) Result: 'Check Table'\n",
            "  - Time Effects Test Result: 'Cannot Run'\n",
            "  - Logic: Pooling rejected & Entity FE valid.\n",
            "  - Logic: Hausman (Check Table), but Poolability rejects Pooled. Prioritizing FE Entity.\n",
            "  -> Tentative Preference: FE Entity (Simple).\n",
            "\n",
            "---> Final Preferred Model Selected (Main Run): FE_Entity_Simple <---\n",
            "\n",
            "--- Interpretation (Based on 'FE_Entity_Simple') ---\n",
            "    Interpreting Preferred Model: 'FE_Entity_Simple' (Formula Type: FE Entity Spec (Success))\n",
            "\n",
            "    Interpretation for 'total_score_lag1':\n",
            "      Model estimates avg effect controlling for Entity Fixed effects.\n",
            "      - Average Effect: Coeff=0.0016, SE=0.0001, Pval=0.0000 ***\n",
            "\n",
            "    Interpretation Notes:\n",
            "      - Results based on 'FE_Entity_Simple'. Check Pval.\n",
            "      - !!! CAVEAT: Uses imputed data. High initial missingness. Compare w/ Sensitivity. !!!\n",
            "      - Consider economic significance.\n",
            "\n",
            "\n",
            "==============================================================================\n",
            "--- 9. Consolidated Analysis Results (Panel Only v14.1 - Transportation & FF DataReader) ---\n",
            "==============================================================================\n",
            "\n",
            "--- Panel Model Summaries (Main Run - Imputed: True) ---\n",
            "\n",
            "--- Model: Pooled_Interaction ---\n",
            "    Formula Type Used: Pooled Spec (Robust SE) (Success)\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1868\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.7074\n",
            "No. Observations:                1080   R-squared (Within):               0.1881\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1868\n",
            "Time:                        17:59:24   Log-likelihood                    555.72\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      30.754\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(8,1071)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             28.764\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(8,1071)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                                              Parameter Estimates                                                              \n",
            "===============================================================================================================================================\n",
            "                                                                             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Intercept                                                                      -0.1705     0.0161    -10.587     0.0000     -0.2021     -0.1389\n",
            "cma                                                                             0.0091     0.0052     1.7467     0.0810     -0.0011      0.0193\n",
            "hml                                                                             0.0005     0.0038     0.1240     0.9013     -0.0070      0.0079\n",
            "mkt_rf                                                                          0.0128     0.0011     11.317     0.0000      0.0106      0.0151\n",
            "rmw                                                                             0.0125     0.0042     2.9606     0.0031      0.0042      0.0208\n",
            "smb                                                                             0.0187     0.0036     5.1142     0.0000      0.0115      0.0258\n",
            "total_score_lag1                                                                0.0016     0.0003     5.1849     0.0000      0.0010      0.0022\n",
            "C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]                      0.0029     0.0310     0.0947     0.9245     -0.0578      0.0637\n",
            "total_score_lag1:C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]    -0.0002     0.0006    -0.2721     0.7856     -0.0013      0.0010\n",
            "===============================================================================================================================================\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1868\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.7074\n",
            "No. Observations:                1080   R-squared (Within):               0.1881\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1868\n",
            "Time:                        17:59:24   Log-likelihood                    555.72\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      30.754\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(8,1071)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             28.764\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(8,1071)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "                                                              Parameter Estimates                                                              \n",
            "===============================================================================================================================================\n",
            "                                                                             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Intercept                                                                      -0.1705     0.0161    -10.587     0.0000     -0.2021     -0.1389\n",
            "cma                                                                             0.0091     0.0052     1.7467     0.0810     -0.0011      0.0193\n",
            "hml                                                                             0.0005     0.0038     0.1240     0.9013     -0.0070      0.0079\n",
            "mkt_rf                                                                          0.0128     0.0011     11.317     0.0000      0.0106      0.0151\n",
            "rmw                                                                             0.0125     0.0042     2.9606     0.0031      0.0042      0.0208\n",
            "smb                                                                             0.0187     0.0036     5.1142     0.0000      0.0115      0.0258\n",
            "total_score_lag1                                                                0.0016     0.0003     5.1849     0.0000      0.0010      0.0022\n",
            "C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]                      0.0029     0.0310     0.0947     0.9245     -0.0578      0.0637\n",
            "total_score_lag1:C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]    -0.0002     0.0006    -0.2721     0.7856     -0.0013      0.0010\n",
            "===============================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "--- Model: RE_Simple ---\n",
            "    Formula Type Used: RE Spec (Success)\n",
            "                        RandomEffects Estimation Summary                        \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1867\n",
            "Estimator:              RandomEffects   R-squared (Between):             -0.7255\n",
            "No. Observations:                1080   R-squared (Within):               0.1880\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1867\n",
            "Time:                        17:59:25   Log-likelihood                    555.63\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      41.047\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(6,1073)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             255.70\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(6,1073)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1693     0.0034    -50.257     0.0000     -0.1759     -0.1627\n",
            "cma                  0.0091     0.0032     2.8345     0.0047      0.0028      0.0154\n",
            "hml                  0.0005     0.0022     0.2121     0.8321     -0.0039      0.0049\n",
            "mkt_rf               0.0128     0.0011     12.180     0.0000      0.0108      0.0149\n",
            "rmw                  0.0125     0.0014     9.0944     0.0000      0.0098      0.0152\n",
            "smb                  0.0187     0.0011     17.565     0.0000      0.0166      0.0208\n",
            "total_score_lag1     0.0016     0.0001     12.415     0.0000      0.0013      0.0018\n",
            "====================================================================================\n",
            "                        RandomEffects Estimation Summary                        \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1867\n",
            "Estimator:              RandomEffects   R-squared (Between):             -0.7255\n",
            "No. Observations:                1080   R-squared (Within):               0.1880\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1867\n",
            "Time:                        17:59:25   Log-likelihood                    555.63\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      41.047\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(6,1073)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             255.70\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(6,1073)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1693     0.0034    -50.257     0.0000     -0.1759     -0.1627\n",
            "cma                  0.0091     0.0032     2.8345     0.0047      0.0028      0.0154\n",
            "hml                  0.0005     0.0022     0.2121     0.8321     -0.0039      0.0049\n",
            "mkt_rf               0.0128     0.0011     12.180     0.0000      0.0108      0.0149\n",
            "rmw                  0.0125     0.0014     9.0944     0.0000      0.0098      0.0152\n",
            "smb                  0.0187     0.0011     17.565     0.0000      0.0166      0.0208\n",
            "total_score_lag1     0.0016     0.0001     12.415     0.0000      0.0013      0.0018\n",
            "====================================================================================\n",
            "\n",
            "--- Model: FE_Entity_Simple ---\n",
            "    Formula Type Used: FE Entity Spec (Success)\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1880\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.7763\n",
            "No. Observations:                1080   R-squared (Within):               0.1880\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1866\n",
            "Time:                        17:59:25   Log-likelihood                    557.29\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      41.061\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(6,1064)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             227.47\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(6,1064)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1717     0.0056    -30.621     0.0000     -0.1827     -0.1607\n",
            "cma                  0.0091     0.0032     2.8368     0.0046      0.0028      0.0154\n",
            "hml                  0.0005     0.0022     0.2235     0.8232     -0.0039      0.0049\n",
            "mkt_rf               0.0129     0.0011     12.189     0.0000      0.0108      0.0149\n",
            "rmw                  0.0126     0.0014     9.1122     0.0000      0.0099      0.0153\n",
            "smb                  0.0186     0.0011     17.536     0.0000      0.0165      0.0207\n",
            "total_score_lag1     0.0016     0.0001     11.672     0.0000      0.0014      0.0019\n",
            "====================================================================================\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.1880\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.7763\n",
            "No. Observations:                1080   R-squared (Within):               0.1880\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.1866\n",
            "Time:                        17:59:25   Log-likelihood                    557.29\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      41.061\n",
            "Entities:                          10   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                  F(6,1064)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             227.47\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                  F(6,1064)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1717     0.0056    -30.621     0.0000     -0.1827     -0.1607\n",
            "cma                  0.0091     0.0032     2.8368     0.0046      0.0028      0.0154\n",
            "hml                  0.0005     0.0022     0.2235     0.8232     -0.0039      0.0049\n",
            "mkt_rf               0.0129     0.0011     12.189     0.0000      0.0108      0.0149\n",
            "rmw                  0.0126     0.0014     9.1122     0.0000      0.0099      0.0153\n",
            "smb                  0.0186     0.0011     17.536     0.0000      0.0165      0.0207\n",
            "total_score_lag1     0.0016     0.0001     11.672     0.0000      0.0014      0.0019\n",
            "====================================================================================\n",
            "\n",
            "F-test for Poolability: 0.3624\n",
            "P-value: 0.9527\n",
            "Distribution: F(9,1064)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FE_TwoWay_Simple ---\n",
            "    Formula Type Used: FE TwoWay Spec (Success)\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.0038\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.0686\n",
            "No. Observations:                1080   R-squared (Within):               0.0130\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.0129\n",
            "Time:                        17:59:25   Log-likelihood                    1438.9\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      3.6760\n",
            "Entities:                          10   P-value                           0.0555\n",
            "Avg Obs:                       108.00   Distribution:                   F(1,962)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             4.0990\n",
            "                                        P-value                           0.0432\n",
            "Time periods:                     108   Distribution:                   F(1,962)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1113     0.0093    -11.951     0.0000     -0.1295     -0.0930\n",
            "total_score_lag1     0.0004     0.0002     2.0246     0.0432   1.294e-05      0.0008\n",
            "====================================================================================\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.0038\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -0.0686\n",
            "No. Observations:                1080   R-squared (Within):               0.0130\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.0129\n",
            "Time:                        17:59:25   Log-likelihood                    1438.9\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      3.6760\n",
            "Entities:                          10   P-value                           0.0555\n",
            "Avg Obs:                       108.00   Distribution:                   F(1,962)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             4.0990\n",
            "                                        P-value                           0.0432\n",
            "Time periods:                     108   Distribution:                   F(1,962)\n",
            "Avg Obs:                      10.0000                                           \n",
            "Min Obs:                      10.0000                                           \n",
            "Max Obs:                      10.0000                                           \n",
            "                                                                                \n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1113     0.0093    -11.951     0.0000     -0.1295     -0.0930\n",
            "total_score_lag1     0.0004     0.0002     2.0246     0.0432   1.294e-05      0.0008\n",
            "====================================================================================\n",
            "\n",
            "F-test for Poolability: 42.580\n",
            "P-value: 0.0000\n",
            "Distribution: F(116,962)\n",
            "\n",
            "Included effects: Entity, Time\n",
            "\n",
            "\n",
            "--- Panel Model Summaries (Sensitivity Run - NO IMPUTATION) ---\n",
            "\n",
            "--- Model: Pooled_Interaction_Sens ---\n",
            "    Formula Type Used: Pooled Spec (Robust SE) (Success)\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.2009\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -10.014\n",
            "No. Observations:                 756   R-squared (Within):               0.2024\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.2009\n",
            "Time:                        17:59:25   Log-likelihood                    440.95\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      23.468\n",
            "Entities:                           7   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                   F(8,747)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             23.658\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                   F(8,747)\n",
            "Avg Obs:                       7.0000                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       7.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                                              Parameter Estimates                                                              \n",
            "===============================================================================================================================================\n",
            "                                                                             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Intercept                                                                      -0.1700     0.0175    -9.7323     0.0000     -0.2042     -0.1357\n",
            "cma                                                                             0.0153     0.0055     2.7699     0.0057      0.0045      0.0262\n",
            "hml                                                                            -0.0033     0.0040    -0.8307     0.4064     -0.0111      0.0045\n",
            "mkt_rf                                                                          0.0128     0.0013     10.009     0.0000      0.0103      0.0153\n",
            "rmw                                                                             0.0110     0.0046     2.3809     0.0175      0.0019      0.0201\n",
            "smb                                                                             0.0174     0.0041     4.2148     0.0000      0.0093      0.0255\n",
            "total_score_lag1                                                                0.0015     0.0003     4.7898     0.0000      0.0009      0.0021\n",
            "C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]                      0.0018     0.0317     0.0583     0.9535     -0.0603      0.0640\n",
            "total_score_lag1:C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low] -2.706e-05     0.0006    -0.0471     0.9624     -0.0012      0.0011\n",
            "===============================================================================================================================================\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.2009\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -10.014\n",
            "No. Observations:                 756   R-squared (Within):               0.2024\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.2009\n",
            "Time:                        17:59:25   Log-likelihood                    440.95\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      23.468\n",
            "Entities:                           7   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                   F(8,747)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             23.658\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                   F(8,747)\n",
            "Avg Obs:                       7.0000                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       7.0000                                           \n",
            "                                                                                \n",
            "                                                              Parameter Estimates                                                              \n",
            "===============================================================================================================================================\n",
            "                                                                             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Intercept                                                                      -0.1700     0.0175    -9.7323     0.0000     -0.2042     -0.1357\n",
            "cma                                                                             0.0153     0.0055     2.7699     0.0057      0.0045      0.0262\n",
            "hml                                                                            -0.0033     0.0040    -0.8307     0.4064     -0.0111      0.0045\n",
            "mkt_rf                                                                          0.0128     0.0013     10.009     0.0000      0.0103      0.0153\n",
            "rmw                                                                             0.0110     0.0046     2.3809     0.0175      0.0019      0.0201\n",
            "smb                                                                             0.0174     0.0041     4.2148     0.0000      0.0093      0.0255\n",
            "total_score_lag1                                                                0.0015     0.0003     4.7898     0.0000      0.0009      0.0021\n",
            "C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low]                      0.0018     0.0317     0.0583     0.9535     -0.0603      0.0640\n",
            "total_score_lag1:C(ESG_Category_lag1, Treatment(reference='Medium'))[T.Low] -2.706e-05     0.0006    -0.0471     0.9624     -0.0012      0.0011\n",
            "===============================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "--- Model: FE_Entity_Simple_Sens ---\n",
            "    Formula Type Used: FE Entity Spec (Success)\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.2024\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -10.660\n",
            "No. Observations:                 756   R-squared (Within):               0.2024\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.2008\n",
            "Time:                        17:59:25   Log-likelihood                    441.76\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      31.432\n",
            "Entities:                           7   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                   F(6,743)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             647.18\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                   F(6,743)\n",
            "Avg Obs:                       7.0000                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       7.0000                                           \n",
            "                                                                                \n",
            "================================================================================\n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1717     0.0046    -37.020     0.0000     -0.1808     -0.1626\n",
            "cma                  0.0153     0.0015     9.9667     0.0000      0.0123      0.0184\n",
            "hml                 -0.0033     0.0014    -2.3022     0.0216     -0.0060     -0.0005\n",
            "mkt_rf               0.0128     0.0004     29.411     0.0000      0.0120      0.0137\n",
            "rmw                  0.0111     0.0016     6.8649     0.0000      0.0079      0.0142\n",
            "smb                  0.0174     0.0012     14.374     0.0000      0.0150      0.0197\n",
            "total_score_lag1     0.0016     0.0001     13.197     0.0000      0.0013      0.0018\n",
            "====================================================================================\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:           ExcessReturn   R-squared:                        0.2024\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -10.660\n",
            "No. Observations:                 756   R-squared (Within):               0.2024\n",
            "Date:                Mon, Apr 28 2025   R-squared (Overall):              0.2008\n",
            "Time:                        17:59:25   Log-likelihood                    441.76\n",
            "Cov. Estimator:             Clustered                                           \n",
            "                                        F-statistic:                      31.432\n",
            "Entities:                           7   P-value                           0.0000\n",
            "Avg Obs:                       108.00   Distribution:                   F(6,743)\n",
            "Min Obs:                       108.00                                           \n",
            "Max Obs:                       108.00   F-statistic (robust):             647.18\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     108   Distribution:                   F(6,743)\n",
            "Avg Obs:                       7.0000                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       7.0000                                           \n",
            "                                                                                \n",
            "                                Parameter Estimates                                 \n",
            "====================================================================================\n",
            "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept           -0.1717     0.0046    -37.020     0.0000     -0.1808     -0.1626\n",
            "cma                  0.0153     0.0015     9.9667     0.0000      0.0123      0.0184\n",
            "hml                 -0.0033     0.0014    -2.3022     0.0216     -0.0060     -0.0005\n",
            "mkt_rf               0.0128     0.0004     29.411     0.0000      0.0120      0.0137\n",
            "rmw                  0.0111     0.0016     6.8649     0.0000      0.0079      0.0142\n",
            "smb                  0.0174     0.0012     14.374     0.0000      0.0150      0.0197\n",
            "total_score_lag1     0.0016     0.0001     13.197     0.0000      0.0013      0.0018\n",
            "====================================================================================\n",
            "\n",
            "F-test for Poolability: 0.2666\n",
            "P-value: 0.9524\n",
            "Distribution: F(6,743)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "\n",
            "--- Preferred Model Selection & Comparison (Main Run) ---\n",
            "  -> Preferred model selected: FE_Entity_Simple\n",
            "  -> Review specification tests and theory.\n",
            "  -> COMPARISON: Sensitivity 'FE_Entity_Simple_Sens' ran successfully. Compare results.\n",
            "\n",
            "--- Interpretation Summary (Refer to Step 7) ---\n",
            "  -> Interpretation provided in Step 7.\n",
            "\n",
            "--- Specification Tests Summary (Main Run) ---\n",
            "Test                          Details                  P-value   Conclusion                       \n",
            "  Hausman (FE vs RE - Simple) Comparison table printed See Table                       Check Table\n",
            "F-test (Poolability - Entity)         F(?,1064)=0.3624    0.9527 Cannot Reject Pooling (Pooled OK)\n",
            "        F-test (Time Effects)  f_test_time unavailable         -                        Cannot Run\n",
            "\n",
            "--- VIF Results (Main Run Data) ---\n",
            "  (VIF > 10 indicates potential issues)\n",
            "\n",
            "  VIF (Factors + Total ESG):\n",
            "        Variable    VIF\n",
            "             hml 4.8649\n",
            "             cma 4.0008\n",
            "             rmw 1.9476\n",
            "          mkt_rf 1.3215\n",
            "             smb 1.2618\n",
            "total_score_lag1 1.0157\n",
            "\n",
            "  VIF (Factors + ESG Components):\n",
            "    Variable     VIF\n",
            "s_score_lag1 21.0431\n",
            "g_score_lag1 15.7938\n",
            "e_score_lag1 14.3221\n",
            "         hml  4.8656\n",
            "         cma  4.0037\n",
            "         rmw  1.9501\n",
            "      mkt_rf  1.3216\n",
            "         smb  1.2650\n",
            "\n",
            "\n",
            "--- Overall Reliability Assessment & Disclaimers ---\n",
            "  - Data Quality: Verify sources (Steps 2 & 3 logs).\n",
            "  - Imputation (MICE): Main run used imputation.\n",
            "    -> !!! CONCERN: High initial missingness (>25%): ['e_score_lag1', 'g_score_lag1', 's_score_lag1', 'total_score_lag1'].\n",
            "    -> Compare Main Run vs. Sensitivity Run (if successful).\n",
            "  - Model Specification & Validity:\n",
            "    -> Note: FE/RE models used simplified spec (no category interactions). Pooled OLS used interactions.\n",
            "    -> Review model selection (Step 7) and theoretical fit.\n",
            "  - Data Characteristics: Check VIF results and Step 4 category warnings.\n",
            "\n",
            "  --- Conclusion ---\n",
            "  -> Treat findings with caution. Prioritize successful Sensitivity Run if Main had issues/imputation.\n",
            "\n",
            "==============================================================================\n",
            "--- End of Consolidated Results ---\n",
            "==============================================================================\n",
            "\n",
            "--- Script Finished (Panel Only v14.1 - Transportation & FF DataReader) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Good Transport firms Panel Regression"
      ],
      "metadata": {
        "id": "5DJyZjviFGcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas yfinance statsmodels fancyimpute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHzCDsA0Slf8",
        "outputId": "0b56b0db-22af-4166-a557-b66abee6ff1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.56)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Collecting fancyimpute\n",
            "  Downloading fancyimpute-0.7.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Collecting knnimpute>=0.1.0 (from fancyimpute)\n",
            "  Downloading knnimpute-0.1.0.tar.gz (8.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (1.6.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (1.6.5)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (8.3.5)\n",
            "Collecting nose (from fancyimpute)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (3.6.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (3.2.7.post2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->fancyimpute) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->fancyimpute) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->fancyimpute) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->fancyimpute) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->osqp>=0.6.2->cvxpy->fancyimpute) (3.0.2)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fancyimpute, knnimpute\n",
            "  Building wheel for fancyimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fancyimpute: filename=fancyimpute-0.7.0-py3-none-any.whl size=29879 sha256=c408b69adef19bccd4367f520c0db36bcbfca67918696d2ec24a930f7f472dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f3/a1/f7f10b5ae2c2459398762a3fcf4ac18c325311c7e3163d5a15\n",
            "  Building wheel for knnimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for knnimpute: filename=knnimpute-0.1.0-py3-none-any.whl size=11331 sha256=fe292638285209604799af734c7d19c51c3fb512ac008a30d0b1c9e2b8f2f79d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e8/e0/79872972161e54486517ae507f94b2c7cea27fb7ef793bd415\n",
            "Successfully built fancyimpute knnimpute\n",
            "Installing collected packages: nose, knnimpute, fancyimpute\n",
            "Successfully installed fancyimpute-0.7.0 knnimpute-0.1.0 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.regression.linear_model import OLS # Keep OLS for single entity fallback\n",
        "from fancyimpute import IterativeImputer # MICE imputation\n",
        "import io\n",
        "import yesg # Import yesg\n",
        "\n",
        "# Import PanelOLS correctly from linearmodels\n",
        "try:\n",
        "    from linearmodels.panel import PanelOLS\n",
        "    panelols_available = True\n",
        "    print(\"linearmodels PanelOLS imported successfully.\")\n",
        "except ImportError:\n",
        "    panelols_available = False\n",
        "    print(\"Warning: linearmodels not found. PanelOLS will not be available. Only Pooled OLS with robust errors will be used.\")\n",
        "    print(\"Please install linearmodels: pip install linearmodels\")\n",
        "\n",
        "\n",
        "# --- 1. Define Inputs ---\n",
        "firms = {\n",
        "    \"Low ESG Risk\": [\"DPSGY\", \"AMKBY\", \"KHNGY\", \"DSDVY\", \"CNI\", \"CP\"],\n",
        "    \"Medium ESG Risk\": [\"UPS\", \"FDX\", \"UNP\", \"CSX\", \"XPO\", \"ODFL\", \"JBHT\", \"ZIM\"],\n",
        "    \"High ESG Risk\": [\"JYD\"]\n",
        "}\n",
        "all_tickers_initial = [ticker for group in firms.values() for ticker in group]\n",
        "\n",
        "# Define Date Ranges\n",
        "# Full period for FF6 Only model\n",
        "start_date_full = \"2014-09-01\"\n",
        "end_date_full = \"2025-03-01\"\n",
        "\n",
        "# Period post ESG methodology change (Nov 2019 change, use data from Dec 2019 onwards)\n",
        "start_date_post_change = \"2019-12-01\"\n",
        "end_date_post_change = \"2025-03-01\" # Same end date as full period\n",
        "\n",
        "# Date range needed for downloading initial stock prices (starts one month before the first return needed)\n",
        "start_date_prices_download = \"2014-08-01\"\n",
        "# End date for prices to calculate returns up to end_date_full\n",
        "end_date_prices_download = \"2025-03-31\"\n",
        "\n",
        "# Define ESG score columns to use\n",
        "esg_cols_to_use = [\"Total-Score\", \"E-Score\", \"S-Score\", \"G-Score\"]\n",
        "ff_factors = ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'WML']\n",
        "\n",
        "# --- 2. Load Fama-French Data ---\n",
        "# Using the provided string data\n",
        "ff_data = \"\"\"Date\tMkt_RF\tSMB\tHML\tRMW\tCMA\tRF\tWML\n",
        "9/1/14\t-3.09\t-2.77\t-0.83\t0.18\t-0.23\t0\t1.08\n",
        "10/1/14\t0.33\t-0.58\t-2.81\t0.84\t-0.23\t0\t-0.14\n",
        "11/1/14\t1.65\t-2.44\t-2\t0.76\t-0.27\t0\t0.98\n",
        "12/1/14\t-1.44\t1.78\t0.04\t-0.31\t0.13\t0\t1.12\n",
        "1/1/15\t-1.74\t-0.05\t-2.49\t2.09\t-0.69\t0\t3.9\n",
        "2/1/15\t5.91\t-0.51\t-0.3\t-0.99\t-0.88\t0\t-2.43\n",
        "3/1/15\t-1.17\t1.44\t-0.8\t0.05\t-0.4\t0\t1.75\n",
        "4/1/15\t2.71\t0.9\t2.27\t-0.7\t-0.48\t0\t-3.3\n",
        "5/1/15\t0.5\t1.05\t-1.12\t-1.06\t-0.41\t0\t4.29\n",
        "6/1/15\t-2.09\t2.06\t-0.66\t0\t-0.27\t0\t1.99\n",
        "7/1/15\t1.13\t-3.29\t-3.25\t1.76\t-0.72\t0\t3.53\n",
        "8/1/15\t-6.18\t1.37\t1.27\t0.51\t0.84\t0\t-0.68\n",
        "9/1/15\t-3.91\t-0.1\t-1.02\t1.86\t0.53\t0\t3.48\n",
        "10/1/15\t7.32\t-2.36\t0.34\t0.82\t-0.49\t0\t-2.61\n",
        "11/1/15\t-0.3\t1.46\t-1.99\t0.55\t-1.39\t0\t2.07\n",
        "12/1/15\t-1.73\t0.84\t-1.7\t0.76\t-0.43\t0.01\t3.39\n",
        "1/1/16\t-6.32\t-1.75\t0.76\t2.73\t2.75\t0.01\t0.58\n",
        "2/1/16\t-0.5\t0.93\t-0.21\t1.17\t1.83\t0.02\t-2.71\n",
        "3/1/16\t6.92\t1.34\t0.42\t0.66\t-0.67\t0.02\t-2.74\n",
        "4/1/16\t1.91\t1.25\t2.98\t-2.77\t0.77\t0.01\t-3.26\n",
        "5/1/16\t0.45\t-0.23\t-2.11\t0.64\t-1.25\t0.01\t2.93\n",
        "6/1/16\t-1.67\t-0.41\t-0.73\t1.59\t2.11\t0.02\t6.09\n",
        "7/1/16\t4.48\t1.38\t0.46\t0.02\t-1.4\t0.02\t-1.71\n",
        "8/1/16\t0.29\t0.56\t2.19\t-1.38\t-0.44\t0.02\t-3.85\n",
        "9/1/16\t0.9\t2.1\t-1.23\t-0.52\t0.13\t0.02\t1.63\n",
        "10/1/16\t-1.88\t-1.09\t4.09\t-1.07\t1.27\t0.02\t-1.14\n",
        "11/1/16\t1.39\t0.61\t4.39\t-2.02\t1.77\t0.01\t-4.14\n",
        "12/1/16\t2.25\t-0.14\t2.76\t-0.11\t0.79\t0.03\t-1.49\n",
        "1/1/17\t2.72\t0.4\t-0.53\t-0.16\t-0.77\t0.04\t1.76\n",
        "2/1/17\t2.28\t-0.71\t-1.74\t1.14\t-0.99\t0.04\t-1.88\n",
        "3/1/17\t1.48\t0.18\t-1.81\t1.01\t-0.54\t0.03\t0.12\n",
        "4/1/17\t1.86\t0.33\t-1.39\t1.03\t-1.01\t0.05\t0.06\n",
        "5/1/17\t2.2\t-0.61\t-2.59\t2.09\t-1.03\t0.06\t1.11\n",
        "6/1/17\t0.6\t1.64\t1.74\t-1.9\t0.05\t0.06\t0.5\n",
        "7/1/17\t2.51\t0.02\t1.02\t-0.76\t0.34\t0.07\t1.77\n",
        "8/1/17\t0.13\t-0.15\t-1.34\t1.21\t-1.3\t0.09\t1.95\n",
        "9/1/17\t2.3\t1.35\t1.64\t-1.17\t1.09\t0.09\t0.01\n",
        "10/1/17\t1.8\t-0.85\t-0.9\t0.99\t-1.92\t0.09\t3.44\n",
        "11/1/17\t1.93\t-0.67\t-0.13\t1.43\t-0.21\t0.08\t0.46\n",
        "12/1/17\t1.38\t0.83\t0.11\t-0.19\t0.81\t0.09\t-1.07\n",
        "1/1/18\t5.15\t-1.21\t-1.43\t-0.17\t-0.54\t0.11\t3.27\n",
        "2/1/18\t-4\t0.77\t-1.79\t1.1\t-1.77\t0.11\t2.43\n",
        "3/1/18\t-1.76\t1.2\t-0.42\t0.4\t-0.55\t0.12\t-1.11\n",
        "4/1/18\t0.93\t-0.4\t1.76\t-1.53\t1.02\t0.14\t-0.49\n",
        "5/1/18\t0.49\t1.51\t-4.52\t1.21\t-2.58\t0.14\t3.11\n",
        "6/1/18\t-0.49\t-0.8\t-1.39\t0.45\t-0.08\t0.14\t-1.38\n",
        "7/1/18\t2.53\t-2.48\t1.19\t-0.1\t0.25\t0.16\t-1.63\n",
        "8/1/18\t0.87\t-0.71\t-4.19\t1.2\t-2.61\t0.16\t3.46\n",
        "9/1/18\t0.32\t-1.43\t0.3\t0.16\t0.57\t0.15\t0.59\n",
        "10/1/18\t-8.19\t-2.28\t2.44\t0.28\t2.31\t0.19\t-2.46\n",
        "11/1/18\t0.88\t-0.62\t-1.22\t0.46\t0.08\t0.18\t-1.58\n",
        "12/1/18\t-7.67\t-0.96\t0.56\t-0.22\t0.13\t0.19\t1.46\n",
        "1/1/19\t7.47\t-0.14\t-0.71\t0.15\t-1.19\t0.21\t-3.85\n",
        "2/1/19\t2.94\t0.1\t-2.09\t0.21\t-0.93\t0.18\t0.36\n",
        "3/1/19\t0.76\t-2.08\t-3.28\t1.32\t-1.49\t0.19\t2.89\n",
        "4/1/19\t3.44\t-1.18\t-0.26\t0.8\t-1.09\t0.21\t-2.81\n",
        "5/1/19\t-5.99\t0.54\t-1.7\t0.45\t0.8\t0.21\t6.68\n",
        "6/1/19\t6.13\t-1.99\t-0.17\t0.43\t-0.02\t0.18\t-1.07\n",
        "7/1/19\t-0.07\t-1.13\t-0.63\t0.64\t-0.33\t0.19\t1.01\n",
        "8/1/19\t-2.55\t-1.64\t-2.7\t1.19\t-0.18\t0.16\t4.32\n",
        "9/1/19\t1.9\t0.19\t4.15\t0.41\t2.17\t0.18\t-4.43\n",
        "10/1/19\t2.56\t0.81\t-0.53\t0.96\t-0.72\t0.15\t-1.23\n",
        "11/1/19\t2.74\t0.04\t-2.22\t-0.09\t-1.2\t0.12\t-1.02\n",
        "12/1/19\t3.05\t1.34\t0.62\t-0.32\t0.48\t0.14\t0.27\n",
        "1/1/20\t-1.2\t-2.77\t-4.33\t0.07\t-1.97\t0.13\t4.15\n",
        "2/1/20\t-8.54\t-1.51\t-1.07\t-1.13\t-1.73\t0.12\t0.19\n",
        "3/1/20\t-13.77\t-4.44\t-9.3\t1.47\t-0.88\t0.12\t6.23\n",
        "4/1/20\t10.68\t1.76\t-4.62\t2.66\t-3\t0\t0.3\n",
        "5/1/20\t5.3\t2.02\t-5.1\t1.92\t-3.18\t0.01\t1.57\n",
        "6/1/20\t2.76\t-0.03\t-1.73\t-0.09\t-0.67\t0.01\t2.21\n",
        "7/1/20\t4.34\t-1.76\t-3.64\t1.33\t-1.21\t0.01\t6.45\n",
        "8/1/20\t6.84\t0.98\t-3.88\t2.27\t-1.52\t0.01\t0.71\n",
        "9/1/20\t-2.99\t1.98\t-2.46\t0.74\t-1.19\t0.01\t2.4\n",
        "10/1/20\t-2.6\t1.43\t2.04\t-1.04\t-0.54\t0.01\t-1.69\n",
        "11/1/20\t13.34\t1.29\t4.34\t-2.35\t0.79\t0.01\t-10.92\n",
        "12/1/20\t4.82\t3.16\t-1.2\t-0.64\t0.14\t0.01\t-0.09\n",
        "1/1/21\t-0.54\t2.93\t1.01\t-2.43\t1.31\t0\t2.64\n",
        "2/1/21\t2.88\t1.55\t7.46\t-1.92\t0.61\t0\t-6.52\n",
        "3/1/21\t3.12\t-1.32\t5.58\t1.98\t3.12\t0\t-4.83\n",
        "4/1/21\t4.49\t-1.22\t-1.97\t1.39\t-2.3\t0\t2.05\n",
        "5/1/21\t1.65\t0.24\t4.14\t0.2\t2.29\t0\t0.69\n",
        "6/1/21\t1.01\t-1.01\t-5.23\t0.7\t-2.28\t0\t0.17\n",
        "7/1/21\t1.04\t-2.16\t-1.67\t4.2\t-0.22\t0\t-0.84\n",
        "8/1/21\t2.42\t-0.13\t-1.71\t0.08\t-2.07\t0\t1.3\n",
        "9/1/21\t-3.81\t1.05\t3.86\t-2.21\t1.88\t0\t0.79\n",
        "10/1/21\t5.04\t-3.31\t-1.48\t1.51\t-1.32\t0\t3.85\n",
        "11/1/21\t-2.64\t-2.02\t-2.21\t4.59\t-0.36\t0\t0.83\n",
        "12/1/21\t3.63\t-0.91\t3.9\t2.18\t4.98\t0.01\t-0.73\n",
        "1/1/22\t-5.58\t-2.57\t11.96\t-2.91\t8.09\t0\t-3.38\n",
        "2/1/22\t-2.25\t2.11\t2.89\t-1.35\t2.27\t0\t0.6\n",
        "3/1/22\t2.16\t-1.89\t-1.1\t0.36\t0.79\t0\t3.59\n",
        "4/1/22\t-8.17\t0.48\t4.95\t0.94\t5.97\t0\t2.32\n",
        "5/1/22\t0.28\t-0.5\t6.14\t-1.04\t4.04\t0.03\t0.12\n",
        "6/1/22\t-8.69\t-0.02\t-1.41\t1.75\t-0.64\t0.06\t1.27\n",
        "7/1/22\t7.79\t-0.41\t-5.48\t2.37\t-5.36\t0.08\t-3.59\n",
        "8/1/22\t-4.22\t0.55\t2.49\t-2.57\t1.34\t0.19\t3.33\n",
        "9/1/22\t-9.46\t-1.49\t1.89\t-1.21\t1.17\t0.19\t3.36\n",
        "10/1/22\t6.79\t-1.35\t4.41\t0.36\t3.27\t0.23\t2.63\n",
        "11/1/22\t7.25\t-0.05\t-0.28\t2.18\t0.98\t0.29\t-1.86\n",
        "12/1/22\t-4.33\t2.42\t2.51\t-0.47\t3.25\t0.33\t3.8\n",
        "1/1/23\t6.92\t0.68\t-1.89\t-1.02\t-3.11\t0.35\t-9\n",
        "2/1/23\t-2.67\t0.05\t0.69\t-0.1\t0.48\t0.34\t0.5\n",
        "3/1/23\t2.27\t-3.68\t-6.92\t4.27\t-1.12\t0.36\t-1.79\n",
        "4/1/23\t1.22\t-1.52\t1\t0.56\t1.87\t0.35\t0.8\n",
        "5/1/23\t-1.61\t-1.31\t-4.76\t0.27\t-3.99\t0.36\t0.32\n",
        "6/1/23\t5.6\t-1.49\t1.04\t0.35\t-0.52\t0.4\t-0.14\n",
        "7/1/23\t2.96\t1.2\t3.43\t-1.05\t0.08\t0.45\t-2.46\n",
        "8/1/23\t-3.06\t-1.63\t-0.36\t2.49\t0.55\t0.45\t2.45\n",
        "9/1/23\t-4.79\t-0.76\t3.3\t0.04\t1.08\t0.43\t-0.55\n",
        "10/1/23\t-3.76\t-2.62\t0.36\t1.41\t1.52\t0.47\t1.28\n",
        "11/1/23\t8.63\t-0.88\t-1.68\t-0.77\t-2.72\t0.44\t2.04\n",
        "12/1/23\t4.97\t3.59\t1.11\t-1.4\t-1.34\t0.43\t-3.67\n",
        "1/1/24\t0.09\t-3.49\t-0.03\t1.2\t0.02\t0.47\t4.05\n",
        "2/1/24\t3.75\t-2.59\t-1.35\t0.14\t-2.56\t0.42\t5.02\n",
        "3/1/24\t2.87\t-0.43\t4.06\t-0.16\t1.17\t0.43\t0.34\n",
        "4/1/24\t-4.19\t-0.6\t1.33\t-0.08\t0.31\t0.47\t-1.63\n",
        "5/1/24\t4.05\t0.16\t-0.95\t1.03\t-1.63\t0.44\t1.89\n",
        "6/1/24\t0.83\t-3.27\t-4.46\t1.12\t-1.33\t0.41\t1.55\n",
        "7/1/24\t1.81\t4.21\t3.36\t-2.55\t2.02\t0.45\t-2.83\n",
        "8/1/24\t1.83\t-2.23\t-1.93\t1.25\t-0.09\t0.48\t1.35\n",
        "9/1/24\t1.51\t-0.07\t-0.72\t-0.46\t0.32\t0.4\t-0.64\n",
        "10/1/24\t-2.47\t-2.01\t1.29\t-1.44\t0.91\t0.39\t2.82\n",
        "11/1/24\t4.08\t-0.94\t0.61\t-2.47\t-0.54\t0.4\t2.4\n",
        "12/1/24\t-3.15\t-1.21\t-2.2\t2.07\t0.05\t0.37\t-0.56\n",
        "1/1/25\t3.22\t-1.4\t0.82\t-1.52\t-2.7\t0.37\t0.66\n",
        "2/1/25\t-1.22\t-1.38\t3.75\t0.79\t2.6\t0.33\t-1.59\n",
        "3/1/25\t-4.35\t2.43\t4.4\t0.39\t0.35\t0.34\t-1.57\"\"\"\n",
        "\n",
        "ff_df = pd.read_csv(io.StringIO(ff_data), sep='\\t')\n",
        "ff_df['Date'] = pd.to_datetime(ff_df['Date'], format='%m/%d/%y') + pd.offsets.MonthEnd(0)\n",
        "# Convert percentages to decimals\n",
        "ff_df[ff_factors] = ff_df[ff_factors] / 100\n",
        "ff_df['RF'] = ff_df['RF'] / 100 # Risk-Free Rate\n",
        "\n",
        "# Filter FF data to the full desired range\n",
        "ff_df_full = ff_df[(ff_df['Date'] >= pd.to_datetime(start_date_full) + pd.offsets.MonthEnd(0)) &\n",
        "                   (ff_df['Date'] <= pd.to_datetime(end_date_full) + pd.offsets.MonthEnd(0))].copy()\n",
        "\n",
        "\n",
        "# --- 3. Load ESG Data using yesg ---\n",
        "print(\"\\nDownloading historic ESG data using yesg...\")\n",
        "all_esg_data_list = []\n",
        "failed_esg_tickers = []\n",
        "\n",
        "for ticker in all_tickers_initial:\n",
        "    print(f\"Attempting to download ESG data for {ticker}...\")\n",
        "    try:\n",
        "        esg_ticker_df = yesg.get_historic_esg(ticker)\n",
        "        if esg_ticker_df is not None and not esg_ticker_df.empty:\n",
        "            esg_ticker_df = esg_ticker_df.reset_index()\n",
        "            esg_ticker_df.columns = ['Date', 'Total-Score', 'E-Score', 'S-Score', 'G-Score']\n",
        "            esg_ticker_df['Ticker'] = ticker\n",
        "            # Ensure Date is end of month for merging\n",
        "            esg_ticker_df['Date'] = pd.to_datetime(esg_ticker_df['Date']) + pd.offsets.MonthEnd(0)\n",
        "            all_esg_data_list.append(esg_ticker_df)\n",
        "            print(f\"Successfully downloaded ESG data for {ticker}.\")\n",
        "        else:\n",
        "            print(f\"No historic ESG data found for {ticker} using yesg.\")\n",
        "            failed_esg_tickers.append(ticker)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading ESG data for {ticker} using yesg: {e}\")\n",
        "        failed_esg_tickers.append(ticker)\n",
        "\n",
        "if all_esg_data_list:\n",
        "    esg_df_raw = pd.concat(all_esg_data_list, ignore_index=True)\n",
        "    # Filter ESG data to the relevant period *for potential merging* (full period initially)\n",
        "    esg_df_raw = esg_df_raw[(esg_df_raw['Date'] >= pd.to_datetime(start_date_full) + pd.offsets.MonthEnd(0)) &\n",
        "                            (esg_df_raw['Date'] <= pd.to_datetime(end_date_full) + pd.offsets.MonthEnd(0))].copy()\n",
        "else:\n",
        "    esg_df_raw = pd.DataFrame(columns=['Date', 'Total-Score', 'E-Score', 'S-Score', 'G-Score', 'Ticker'])\n",
        "    print(\"No ESG data was successfully downloaded using yesg for any ticker in the specified period.\")\n",
        "\n",
        "\n",
        "# --- 4. Download Stock Data ---\n",
        "print(\"\\nDownloading stock data...\")\n",
        "stock_data = yf.download(all_tickers_initial, start=start_date_prices_download, end=end_date_prices_download, auto_adjust=False, progress=False)\n",
        "\n",
        "# Keep only Adjusted Close and resample to monthly (end of month)\n",
        "stock_adj_close = stock_data['Adj Close'].resample('M').last()\n",
        "\n",
        "# Check for tickers that failed to download stock data\n",
        "downloaded_tickers = stock_adj_close.columns.tolist()\n",
        "failed_stock_tickers = [t for t in all_tickers_initial if t not in downloaded_tickers]\n",
        "if failed_stock_tickers:\n",
        "    print(f\"Warning: Failed to download stock data for tickers: {failed_stock_tickers}\")\n",
        "    # Remove failed stock tickers from analysis groups and ESG data\n",
        "    for group, tickers in firms.items():\n",
        "        firms[group] = [t for t in tickers if t not in failed_stock_tickers]\n",
        "    all_tickers_analysis = [ticker for group in firms.values() for ticker in group]\n",
        "    stock_adj_close = stock_adj_close[all_tickers_analysis] # Filter stock data\n",
        "    if not esg_df_raw.empty:\n",
        "         esg_df_raw = esg_df_raw[esg_df_raw['Ticker'].isin(all_tickers_analysis)].copy()\n",
        "    print(f\"Analysis proceeding with tickers: {all_tickers_analysis}\")\n",
        "\n",
        "\n",
        "# --- 5. Calculate Returns (using the full period) ---\n",
        "# Calculate monthly simple returns\n",
        "stock_returns_full = stock_adj_close.pct_change()\n",
        "\n",
        "# Reshape stock returns to long format (panel data)\n",
        "stock_returns_long_full = stock_returns_full.stack().reset_index()\n",
        "stock_returns_long_full.columns = ['Date', 'Ticker', 'Return']\n",
        "\n",
        "# Ensure returns and FF dates align precisely for the full period\n",
        "stock_returns_long_full = stock_returns_long_full[stock_returns_long_full['Date'].isin(ff_df_full['Date'])].copy()\n",
        "ff_df_aligned = ff_df_full[ff_df_full['Date'].isin(stock_returns_long_full['Date'])].copy() # Ensure FF matches return dates\n",
        "\n",
        "# Merge returns with Fama-French data for the full period\n",
        "full_df = pd.merge(stock_returns_long_full, ff_df_aligned, on='Date', how='left')\n",
        "\n",
        "# Calculate Excess Return (Return - RF)\n",
        "full_df['Excess_Return'] = full_df['Return'] - full_df['RF']\n",
        "\n",
        "# Drop the individual 'Return' and 'RF' columns\n",
        "full_df = full_df.drop(columns=['Return', 'RF'], errors='ignore')\n",
        "\n",
        "# Merge the combined FF+Returns data with ESG data (full period initially)\n",
        "# Use left merge to keep all stock return/FF observations, adding ESG where available\n",
        "full_panel_df_raw_esg = pd.merge(full_df, esg_df_raw, on=['Date', 'Ticker'], how='left')\n",
        "\n",
        "# Set MultiIndex for panel data\n",
        "full_panel_df_raw_esg = full_panel_df_raw_esg.set_index(['Ticker', 'Date'])\n",
        "\n",
        "# Ensure relevant columns are numeric\n",
        "# Only include ESG columns if they were actually downloaded/exist in esg_df_raw\n",
        "available_esg_cols = [col for col in esg_cols_to_use if col in full_panel_df_raw_esg.columns]\n",
        "numeric_cols = ['Excess_Return'] + ff_factors + available_esg_cols\n",
        "for col in numeric_cols:\n",
        "    full_panel_df_raw_esg[col] = pd.to_numeric(full_panel_df_raw_esg[col], errors='coerce')\n",
        "\n",
        "\n",
        "# --- 6. Handle Missing ESG Data (MICE) ---\n",
        "# Note: MICE should be applied BEFORE splitting by date, to use all available data for imputation.\n",
        "# Identify columns to impute (ESG scores). Only impute if the column exists and has NaNs.\n",
        "cols_to_impute = [col for col in available_esg_cols if full_panel_df_raw_esg[col].isnull().any()]\n",
        "\n",
        "if cols_to_impute:\n",
        "    print(f\"\\nApplying MICE imputation to: {cols_to_impute}\")\n",
        "    df_for_imputation = full_panel_df_raw_esg.copy()\n",
        "    # Use all currently existing numeric columns for imputation predictors\n",
        "    numerical_cols_for_mice = df_for_imputation.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "    # Check if there's enough non-missing data across relevant columns to run MICE\n",
        "    # Need at least 2 non-NaN rows across the columns being used for imputation predictors\n",
        "    if df_for_imputation[numerical_cols_for_mice].dropna(how='all').shape[0] < 2:\n",
        "         print(\"Warning: Not enough non-missing data across numerical columns to perform MICE. Skipping imputation.\")\n",
        "         full_panel_df_imputed = full_panel_df_raw_esg # Keep original with NaNs\n",
        "    else:\n",
        "        try:\n",
        "            imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "            imputed_data = imputer.fit_transform(df_for_imputation[numerical_cols_for_mice])\n",
        "            imputed_df = pd.DataFrame(imputed_data, columns=numerical_cols_for_mice, index=full_panel_df_raw_esg.index)\n",
        "\n",
        "            # Copy imputed columns back to the main dataframe structure\n",
        "            full_panel_df_imputed = full_panel_df_raw_esg.copy()\n",
        "            for col in numerical_cols_for_mice:\n",
        "                 full_panel_df_imputed[col] = imputed_df[col]\n",
        "\n",
        "            print(\"MICE imputation complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during MICE imputation: {e}\")\n",
        "            print(\"Skipping MICE imputation. Analysis will use data with NaNs (which will be dropped per model).\")\n",
        "            full_panel_df_imputed = full_panel_df_raw_esg # Keep original with NaNs\n",
        "\n",
        "else:\n",
        "     print(\"\\nNo ESG columns found with missing values to impute after data retrieval.\")\n",
        "     full_panel_df_imputed = full_panel_df_raw_esg # No imputation needed\n",
        "\n",
        "\n",
        "# --- 7. Prepare dataframes for different analysis periods ---\n",
        "# Add ESG Group to the DataFrame (before splitting)\n",
        "esg_group_map = {}\n",
        "for group, tickers in firms.items():\n",
        "    for ticker in tickers:\n",
        "        esg_group_map[ticker] = group\n",
        "\n",
        "full_panel_df_imputed['ESG_Group'] = full_panel_df_imputed.index.get_level_values('Ticker').map(esg_group_map)\n",
        "\n",
        "# Drop rows where Excess_Return is NaN (cannot perform regression)\n",
        "initial_rows = len(full_panel_df_imputed)\n",
        "full_panel_df_imputed = full_panel_df_imputed.dropna(subset=['Excess_Return'])\n",
        "if len(full_panel_df_imputed) < initial_rows:\n",
        "    print(f\"Dropped {initial_rows - len(full_panel_df_imputed)} rows due to missing Excess_Return.\")\n",
        "\n",
        "# --- FIX: Sort the index before slicing ---\n",
        "full_panel_df_imputed = full_panel_df_imputed.sort_index()\n",
        "# --- End FIX ---\n",
        "\n",
        "# Filter data for the post-change period for ESG analysis\n",
        "post_change_panel_df = full_panel_df_imputed.loc[(slice(None), slice(pd.to_datetime(start_date_post_change) + pd.offsets.MonthEnd(0), pd.to_datetime(end_date_post_change) + pd.offsets.MonthEnd(0))), :].copy()\n",
        "\n",
        "print(f\"\\nAnalysis period for FF6 Only: {start_date_full} to {end_date_full}\")\n",
        "print(f\"Analysis period for FF6 + ESG: {start_date_post_change} to {end_date_post_change}\")\n",
        "\n",
        "\n",
        "# --- 8. Analysis per Group and per Model Specification ---\n",
        "\n",
        "# Define the model specifications to test\n",
        "model_specs = {\n",
        "    'FF6 Only': {'esg_predictors': [], 'period_df': full_panel_df_imputed},\n",
        "    'FF6 + Total-Score': {'esg_predictors': ['Total-Score'], 'period_df': post_change_panel_df},\n",
        "    'FF6 + E-Score': {'esg_predictors': ['E-Score'], 'period_df': post_change_panel_df},\n",
        "    'FF6 + S-Score': {'esg_predictors': ['S-Score'], 'period_df': post_change_panel_df},\n",
        "    'FF6 + G-Score': {'esg_predictors': ['G-Score'], 'period_df': post_change_panel_df}\n",
        "}\n",
        "\n",
        "# Identify which ESG columns are actually present in the imputed data\n",
        "available_esg_cols_imputed = [col for col in esg_cols_to_use if col in full_panel_df_imputed.columns]\n",
        "\n",
        "for group_name, group_tickers in firms.items():\n",
        "    print(f\"\\n--- Analyzing: {group_name} ---\")\n",
        "\n",
        "    # Determine which models to run for this group based on data availability\n",
        "    model_specs_group_filtered = model_specs.copy()\n",
        "\n",
        "    # Check for ESG data availability for the ESG models in this group *in the post-change period*\n",
        "    for model_name in list(model_specs_group_filtered.keys()): # Iterate over a copy\n",
        "        if model_name != 'FF6 Only':\n",
        "            esg_col = model_specs_group_filtered[model_name]['esg_predictors'][0]\n",
        "\n",
        "            # Check if the ESG column exists at all in the imputed data\n",
        "            if esg_col not in available_esg_cols_imputed:\n",
        "                 print(f\"Note: Skipping '{model_name}' for '{group_name}' group as ESG column '{esg_col}' is not available in the data.\")\n",
        "                 del model_specs_group_filtered[model_name]\n",
        "                 continue\n",
        "\n",
        "            # Filter the post_change_panel_df to the current group\n",
        "            group_post_change_df = post_change_panel_df[post_change_panel_df.index.get_level_values('Ticker').isin(group_tickers)].copy()\n",
        "\n",
        "            # Check if the ESG column has any non-NaN data in this specific group/period subset\n",
        "            if group_post_change_df[esg_col].isnull().all():\n",
        "                 print(f\"Note: Skipping '{model_name}' for '{group_name}' group due to lack of non-missing ESG data in the post-change period.\")\n",
        "                 del model_specs_group_filtered[model_name]\n",
        "            elif group_name == \"High ESG Risk\": # Explicitly skip ESG models for High group due to lack of original data/imputation issues\n",
        "                 print(f\"Note: Skipping '{model_name}' for 'High ESG Risk' group due to data limitations (only JYD, no original ESG data via yesg).\")\n",
        "                 del model_specs_group_filtered[model_name]\n",
        "\n",
        "\n",
        "    # Iterate through applicable model specifications for the current group\n",
        "    for model_name, spec in model_specs_group_filtered.items():\n",
        "        print(f\"\\n--- Model: {model_name} ---\")\n",
        "\n",
        "        esg_predictors = spec['esg_predictors']\n",
        "        period_df = spec['period_df']\n",
        "\n",
        "        # Filter data for the current group AND the specific period\n",
        "        # The period_df is already filtered by date range, now filter by ticker group\n",
        "        model_df = period_df[period_df.index.get_level_values('Ticker').isin(group_tickers)].copy()\n",
        "\n",
        "        predictors = ff_factors + esg_predictors\n",
        "\n",
        "        # Drop rows with any NaN in the selected predictors or dependent variable for THIS specific model\n",
        "        model_df = model_df.dropna(subset=['Excess_Return'] + predictors).copy()\n",
        "\n",
        "        if model_df.empty:\n",
        "            print(f\"Not enough data left for '{model_name}' analysis in this group/period after dropping NaNs.\")\n",
        "            continue\n",
        "\n",
        "        # --- VIF Calculation (on pooled data for the specific model's predictors) ---\n",
        "        print(\"\\nPerforming VIF Calculation:\")\n",
        "        X_vif = model_df[predictors]\n",
        "        # Add a constant for VIF calculation\n",
        "        X_vif = sm.add_constant(X_vif, has_constant='add')\n",
        "\n",
        "        # Ensure no NaNs left in X_vif before VIF (should be handled by dropna above)\n",
        "        if X_vif.isnull().sum().sum() > 0:\n",
        "             print(\"Warning: NaNs found in VIF predictors. Skipping VIF.\")\n",
        "        else:\n",
        "            vif_data = pd.DataFrame()\n",
        "            vif_data['feature'] = X_vif.columns\n",
        "            try:\n",
        "                # Check for sufficient data points for VIF calculation (must be > number of predictors)\n",
        "                if X_vif.shape[0] > X_vif.shape[1]:\n",
        "                    vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
        "                    print(vif_data.round(2))\n",
        "                    print(\"Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\")\n",
        "                else:\n",
        "                    print(\"Not enough observations to compute VIFs.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not compute VIFs: {e}\")\n",
        "                print(\"This might be due to perfect multicollinearity or insufficient data points.\")\n",
        "\n",
        "\n",
        "        # --- Panel Data Regression ---\n",
        "        print(f\"\\nPerforming Regression ({'Fixed Effects PanelOLS with Robust Standard Errors' if panelols_available else 'Pooled OLS with Robust Standard Errors'}):\")\n",
        "        y = model_df['Excess_Return']\n",
        "        X_reg = model_df[predictors] # Use predictors *without* constant for PanelOLS\n",
        "\n",
        "        num_entities = len(X_reg.index.get_level_values('Ticker').unique())\n",
        "        num_dates = len(X_reg.index.get_level_values('Date').unique())\n",
        "        num_obs = len(X_reg)\n",
        "        num_model_params = X_reg.shape[1] # Number of predictor variables\n",
        "\n",
        "\n",
        "        # Check if PanelOLS is applicable (needs >1 entity and >1 date, and enough observations)\n",
        "        # linearmodels requires the index to be sorted\n",
        "        model_df_sorted = model_df.sort_index()\n",
        "        y_sorted = model_df_sorted['Excess_Return']\n",
        "        X_reg_sorted = model_df_sorted[predictors]\n",
        "\n",
        "        if panelols_available and num_entities > 1 and num_dates > 1:\n",
        "            try:\n",
        "                 # Heuristic check for sufficient observations for FE identification\n",
        "                 # A stricter check might be needed in research, but this avoids common errors\n",
        "                 if num_obs > num_model_params + num_entities:\n",
        "                    model = PanelOLS(y_sorted, X_reg_sorted, entity_effects=True)\n",
        "                    # Use robust standard errors\n",
        "                    results = model.fit(cov_type='robust')\n",
        "                    print(results)\n",
        "                 else:\n",
        "                     print(f\"Warning: Not enough observations ({num_obs}) relative to number of entities ({num_entities}) and predictors ({num_model_params}) for Fixed Effects ({num_entities + num_model_params} minimum recommended). Running OLS instead.\")\n",
        "                     # Fallback to OLS with robust standard errors\n",
        "                     X_reg_ols = sm.add_constant(X_reg_sorted, has_constant='add')\n",
        "                     try:\n",
        "                         model_ols = OLS(y_sorted, X_reg_ols)\n",
        "                         results_ols = model_ols.fit(cov_type='HC1') # HC1 is a common robust estimator for OLS\n",
        "                         print(results_ols.summary())\n",
        "                     except Exception as e_ols:\n",
        "                         print(f\"Error running OLS fallback for {group_name} - {model_name}: {e_ols}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"Error running PanelOLS for {group_name} - {model_name}: {e}\")\n",
        "                 print(\"Attempting PooledOLS with robust standard errors instead.\")\n",
        "                 # Fallback to PooledOLS with robust standard errors if FixedEffects fails or conditions not met strictly\n",
        "                 X_reg_pooled = sm.add_constant(X_reg_sorted, has_constant='add')\n",
        "                 try:\n",
        "                     model_pooled = OLS(y_sorted, X_reg_pooled)\n",
        "                     results_pooled = model_pooled.fit(cov_type='HC1') # HC1 is a common robust estimator for OLS\n",
        "                     print(results_pooled.summary())\n",
        "                 except Exception as e_pooled:\n",
        "                     print(f\"Error running PooledOLS fallback for {group_name} - {model_name}: {e_pooled}\")\n",
        "\n",
        "        else:\n",
        "            # Run OLS with robust standard errors if PanelOLS conditions are not met\n",
        "            if not panelols_available:\n",
        "                 print(\"linearmodels not installed. Running PooledOLS with robust errors.\")\n",
        "            elif num_entities <= 1:\n",
        "                 print(\"Warning: Only one entity (firm) left in this subset. Panel regression is not meaningful. Running OLS with robust errors instead.\")\n",
        "            elif num_dates <= 1:\n",
        "                 print(\"Warning: Only one time period left in this subset. Panel regression is not meaningful. Running OLS with robust errors instead.\")\n",
        "\n",
        "            X_reg_ols = sm.add_constant(X_reg_sorted, has_constant='add')\n",
        "            try:\n",
        "                model_ols = OLS(y_sorted, X_reg_ols)\n",
        "                results_ols = model_ols.fit(cov_type='HC1') # HC1 is a common robust estimator for OLS\n",
        "                print(results_ols.summary())\n",
        "            except Exception as e_ols:\n",
        "                print(f\"Error running OLS for {group_name} - {model_name}: {e_ols}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Analysis Complete ---\")\n",
        "print(\"\\nNote on Reliability:\")\n",
        "print(\"- Stock data download failed for DPSGY.\")\n",
        "print(\"- ESG data was retrieved using the 'yesg' library. Download failed or no data found for DPSGY, AMKBY, KHNGY, DSDVY, JYD.\")\n",
        "print(\"- The ESG rating methodology and scale changed in November 2019 (before: high score = low risk; after: high score = high risk).\")\n",
        "print(\"- FF6-only models are run on the full period (Sept 2014 - Mar 2025).\")\n",
        "print(f\"- FF6 + ESG models are run only on the post-change period ({start_date_post_change} - {end_date_post_change}) to maintain a consistent interpretation of ESG scores.\")\n",
        "print(\"- MICE imputation was applied to fill remaining gaps in the ESG data (in the full dataset) using other numerical columns as predictors.\")\n",
        "print(\"- Imputation for firms/periods with very limited original historical ESG data (e.g., for the earlier periods of ZIM, XPO, ODFL, and entirely for the downloaded tickers with failed yesg download) is speculative and reduces the reliability of ESG coefficient estimates for groups containing these firms.\")\n",
        "print(\"- The High ESG Risk group (JYD) had no original ESG data from 'yesg' for the entire period, so ESG models are skipped for this group.\")\n",
        "print(\"- VIF analysis helps assess multicollinearity *within the predictors of each specific model*.\")\n",
        "print(\"- Panel Regression (Fixed Effects) with robust standard errors is used where appropriate, controlling for unobserved firm-specific effects. Fallback to OLS/PooledOLS with robust errors occurs if FE conditions aren't met or only one entity remains.\")\n",
        "print(\"\\nConclusion regarding research reliability:\")\n",
        "print(\"The findings regarding the Fama-French factors are reasonably reliable, especially for the groups where PanelOLS was successfully applied. However, the ability to draw strong, reliable conclusions about the impact of ESG scores is severely limited by:\")\n",
        "print(\"  1. The necessity to restrict ESG analysis to the shorter post-change period.\")\n",
        "print(\"  2. The substantial reliance on MICE imputation for firms/periods with sparse or no original historical ESG data from the 'yesg' source.\")\n",
        "print(\"Relying solely on these ESG results for definitive research conclusions or generalizations about ESG's impact across the full period or different firms is not recommended without more complete original historical ESG data from a reliable source.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puqgsg_UWKxx",
        "outputId": "b03e51d4-569d-474e-e293-fbcb8401ea99"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linearmodels PanelOLS imported successfully.\n",
            "\n",
            "Downloading historic ESG data using yesg...\n",
            "Attempting to download ESG data for DPSGY...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "No historic ESG data found for DPSGY using yesg.\n",
            "Attempting to download ESG data for AMKBY...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "No historic ESG data found for AMKBY using yesg.\n",
            "Attempting to download ESG data for KHNGY...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "No historic ESG data found for KHNGY using yesg.\n",
            "Attempting to download ESG data for DSDVY...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "No historic ESG data found for DSDVY using yesg.\n",
            "Attempting to download ESG data for CNI...\n",
            "Successfully downloaded ESG data for CNI.\n",
            "Attempting to download ESG data for CP...\n",
            "Successfully downloaded ESG data for CP.\n",
            "Attempting to download ESG data for UPS...\n",
            "Successfully downloaded ESG data for UPS.\n",
            "Attempting to download ESG data for FDX...\n",
            "Successfully downloaded ESG data for FDX.\n",
            "Attempting to download ESG data for UNP...\n",
            "Successfully downloaded ESG data for UNP.\n",
            "Attempting to download ESG data for CSX...\n",
            "Successfully downloaded ESG data for CSX.\n",
            "Attempting to download ESG data for XPO...\n",
            "Successfully downloaded ESG data for XPO.\n",
            "Attempting to download ESG data for ODFL...\n",
            "Successfully downloaded ESG data for ODFL.\n",
            "Attempting to download ESG data for JBHT...\n",
            "Successfully downloaded ESG data for JBHT.\n",
            "Attempting to download ESG data for ZIM...\n",
            "Successfully downloaded ESG data for ZIM.\n",
            "Attempting to download ESG data for JYD...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "No historic ESG data found for JYD using yesg.\n",
            "\n",
            "Downloading stock data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['DPSGY']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying MICE imputation to: ['Total-Score', 'E-Score', 'S-Score', 'G-Score']\n",
            "MICE imputation complete.\n",
            "\n",
            "Analysis period for FF6 Only: 2014-09-01 to 2025-03-01\n",
            "Analysis period for FF6 + ESG: 2019-12-01 to 2025-03-01\n",
            "\n",
            "--- Analyzing: Low ESG Risk ---\n",
            "\n",
            "--- Model: FF6 Only ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "  feature    VIF\n",
            "0   const 1.1500\n",
            "1  Mkt_RF 1.4100\n",
            "2     SMB 1.2300\n",
            "3     HML 5.2200\n",
            "4     RMW 1.8200\n",
            "5     CMA 3.7600\n",
            "6     WML 1.6000\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3836\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.8134\n",
            "No. Observations:                 593   R-squared (Within):               0.3836\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3884\n",
            "Time:                        09:02:19   Log-likelihood                    806.32\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      60.369\n",
            "Entities:                           5   P-value                           0.0000\n",
            "Avg Obs:                       118.60   Distribution:                   F(6,582)\n",
            "Min Obs:                       85.000                                           \n",
            "Max Obs:                       127.00   F-statistic (robust):             50.449\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     127   Distribution:                   F(6,582)\n",
            "Avg Obs:                       4.6693                                           \n",
            "Min Obs:                       4.0000                                           \n",
            "Max Obs:                       5.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.0844     0.0737     14.718     0.0000      0.9397      1.2291\n",
            "SMB            0.4097     0.1784     2.2962     0.0220      0.0593      0.7601\n",
            "HML           -0.2050     0.2235    -0.9173     0.3593     -0.6439      0.2339\n",
            "RMW            0.5857     0.2562     2.2859     0.0226      0.0825      1.0890\n",
            "CMA            0.5323     0.2872     1.8534     0.0643     -0.0318      1.0964\n",
            "WML           -0.2278     0.1317    -1.7306     0.0841     -0.4864      0.0307\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 0.3992\n",
            "P-value: 0.8092\n",
            "Distribution: F(4,582)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + Total-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "       feature     VIF\n",
            "0        const 20.9900\n",
            "1       Mkt_RF  1.4200\n",
            "2          SMB  1.3600\n",
            "3          HML  5.4100\n",
            "4          RMW  1.8700\n",
            "5          CMA  4.0400\n",
            "6          WML  1.5500\n",
            "7  Total-Score  1.0400\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3997\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -3.4302\n",
            "No. Observations:                 320   R-squared (Within):               0.3997\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3366\n",
            "Time:                        09:02:19   Log-likelihood                    404.86\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      29.291\n",
            "Entities:                           5   P-value                           0.0000\n",
            "Avg Obs:                       64.000   Distribution:                   F(7,308)\n",
            "Min Obs:                       64.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             27.558\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,308)\n",
            "Avg Obs:                       5.0000                                           \n",
            "Min Obs:                       5.0000                                           \n",
            "Max Obs:                       5.0000                                           \n",
            "                                                                                \n",
            "                              Parameter Estimates                              \n",
            "===============================================================================\n",
            "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-------------------------------------------------------------------------------\n",
            "Mkt_RF          1.0433     0.0939     11.107     0.0000      0.8584      1.2281\n",
            "SMB             0.5955     0.2362     2.5216     0.0122      0.1308      1.0602\n",
            "HML            -0.1667     0.2688    -0.6200     0.5357     -0.6956      0.3623\n",
            "RMW             0.7919     0.3181     2.4899     0.0133      0.1661      1.4178\n",
            "CMA             0.5438     0.3546     1.5335     0.1262     -0.1539      1.2415\n",
            "WML            -0.1457     0.1777    -0.8203     0.4127     -0.4953      0.2039\n",
            "Total-Score    -0.0005     0.0003    -1.5414     0.1242     -0.0010      0.0001\n",
            "===============================================================================\n",
            "\n",
            "F-test for Poolability: 0.7403\n",
            "P-value: 0.5651\n",
            "Distribution: F(4,308)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + E-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature     VIF\n",
            "0    const 10.9400\n",
            "1   Mkt_RF  1.4200\n",
            "2      SMB  1.3600\n",
            "3      HML  5.4100\n",
            "4      RMW  1.8700\n",
            "5      CMA  4.0400\n",
            "6      WML  1.5500\n",
            "7  E-Score  1.0400\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3995\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -1.4875\n",
            "No. Observations:                 320   R-squared (Within):               0.3995\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3684\n",
            "Time:                        09:02:19   Log-likelihood                    404.82\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      29.273\n",
            "Entities:                           5   P-value                           0.0000\n",
            "Avg Obs:                       64.000   Distribution:                   F(7,308)\n",
            "Min Obs:                       64.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             27.544\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,308)\n",
            "Avg Obs:                       5.0000                                           \n",
            "Min Obs:                       5.0000                                           \n",
            "Max Obs:                       5.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.0435     0.0940     11.102     0.0000      0.8586      1.2285\n",
            "SMB            0.5972     0.2362     2.5283     0.0120      0.1324      1.0619\n",
            "HML           -0.1656     0.2688    -0.6162     0.5382     -0.6946      0.3633\n",
            "RMW            0.7916     0.3184     2.4860     0.0134      0.1650      1.4181\n",
            "CMA            0.5440     0.3545     1.5345     0.1259     -0.1536      1.2416\n",
            "WML           -0.1456     0.1777    -0.8191     0.4134     -0.4952      0.2041\n",
            "E-Score       -0.0004     0.0002    -1.4418     0.1504     -0.0008      0.0001\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 0.7264\n",
            "P-value: 0.5745\n",
            "Distribution: F(4,308)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + S-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature     VIF\n",
            "0    const 12.9200\n",
            "1   Mkt_RF  1.4200\n",
            "2      SMB  1.3600\n",
            "3      HML  5.4100\n",
            "4      RMW  1.8700\n",
            "5      CMA  4.0400\n",
            "6      WML  1.5500\n",
            "7  S-Score  1.0400\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3996\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -1.9670\n",
            "No. Observations:                 320   R-squared (Within):               0.3996\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3606\n",
            "Time:                        09:02:19   Log-likelihood                    404.86\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      29.290\n",
            "Entities:                           5   P-value                           0.0000\n",
            "Avg Obs:                       64.000   Distribution:                   F(7,308)\n",
            "Min Obs:                       64.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             27.557\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,308)\n",
            "Avg Obs:                       5.0000                                           \n",
            "Min Obs:                       5.0000                                           \n",
            "Max Obs:                       5.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.0434     0.0939     11.107     0.0000      0.8586      1.2283\n",
            "SMB            0.5964     0.2362     2.5253     0.0121      0.1317      1.0612\n",
            "HML           -0.1664     0.2688    -0.6192     0.5362     -0.6953      0.3624\n",
            "RMW            0.7918     0.3182     2.4886     0.0134      0.1657      1.4179\n",
            "CMA            0.5440     0.3545     1.5346     0.1259     -0.1535      1.2416\n",
            "WML           -0.1457     0.1777    -0.8200     0.4128     -0.4953      0.2039\n",
            "S-Score       -0.0004     0.0003    -1.5195     0.1297     -0.0009      0.0001\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 0.7383\n",
            "P-value: 0.5664\n",
            "Distribution: F(4,308)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + G-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature    VIF\n",
            "0    const 9.1000\n",
            "1   Mkt_RF 1.4200\n",
            "2      SMB 1.3600\n",
            "3      HML 5.4100\n",
            "4      RMW 1.8700\n",
            "5      CMA 4.0400\n",
            "6      WML 1.5500\n",
            "7  G-Score 1.0400\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3995\n",
            "Estimator:                   PanelOLS   R-squared (Between):             -1.1234\n",
            "No. Observations:                 320   R-squared (Within):               0.3995\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3744\n",
            "Time:                        09:02:19   Log-likelihood                    404.81\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      29.268\n",
            "Entities:                           5   P-value                           0.0000\n",
            "Avg Obs:                       64.000   Distribution:                   F(7,308)\n",
            "Min Obs:                       64.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             27.536\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,308)\n",
            "Avg Obs:                       5.0000                                           \n",
            "Min Obs:                       5.0000                                           \n",
            "Max Obs:                       5.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.0433     0.0940     11.100     0.0000      0.8583      1.2282\n",
            "SMB            0.5975     0.2362     2.5297     0.0119      0.1327      1.0622\n",
            "HML           -0.1661     0.2689    -0.6179     0.5371     -0.6952      0.3629\n",
            "RMW            0.7908     0.3184     2.4834     0.0135      0.1642      1.4173\n",
            "CMA            0.5443     0.3546     1.5351     0.1258     -0.1534      1.2419\n",
            "WML           -0.1454     0.1777    -0.8180     0.4140     -0.4951      0.2043\n",
            "G-Score       -0.0003     0.0002    -1.4214     0.1562     -0.0006      0.0001\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 0.7217\n",
            "P-value: 0.5777\n",
            "Distribution: F(4,308)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Analyzing: Medium ESG Risk ---\n",
            "\n",
            "--- Model: FF6 Only ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "  feature    VIF\n",
            "0   const 1.1500\n",
            "1  Mkt_RF 1.4000\n",
            "2     SMB 1.2300\n",
            "3     HML 5.1900\n",
            "4     RMW 1.8400\n",
            "5     CMA 3.7500\n",
            "6     WML 1.5800\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3130\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.3757\n",
            "No. Observations:                 939   R-squared (Within):               0.3130\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3181\n",
            "Time:                        09:02:19   Log-likelihood                    978.17\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      70.249\n",
            "Entities:                           8   P-value                           0.0000\n",
            "Avg Obs:                       117.38   Distribution:                   F(6,925)\n",
            "Min Obs:                       50.000                                           \n",
            "Max Obs:                       127.00   F-statistic (robust):             67.507\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                     127   Distribution:                   F(6,925)\n",
            "Avg Obs:                       7.3937                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       8.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.2979     0.0807     16.079     0.0000      1.1395      1.4563\n",
            "SMB           -0.0418     0.1939    -0.2157     0.8292     -0.4225      0.3388\n",
            "HML            0.1265     0.2517     0.5025     0.6154     -0.3674      0.6204\n",
            "RMW            0.0952     0.2600     0.3662     0.7143     -0.4150      0.6054\n",
            "CMA           -0.1903     0.3307    -0.5755     0.5651     -0.8392      0.4586\n",
            "WML           -0.1233     0.1351    -0.9124     0.3618     -0.3884      0.1419\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 2.6700\n",
            "P-value: 0.0097\n",
            "Distribution: F(7,925)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + Total-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "       feature     VIF\n",
            "0        const 13.2000\n",
            "1       Mkt_RF  1.4400\n",
            "2          SMB  1.3700\n",
            "3          HML  5.4900\n",
            "4          RMW  1.8900\n",
            "5          CMA  4.1400\n",
            "6          WML  1.5400\n",
            "7  Total-Score  1.1000\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3102\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.2171\n",
            "No. Observations:                 498   R-squared (Within):               0.3102\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3077\n",
            "Time:                        09:02:19   Log-likelihood                    444.87\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      31.029\n",
            "Entities:                           8   P-value                           0.0000\n",
            "Avg Obs:                       62.250   Distribution:                   F(7,483)\n",
            "Min Obs:                       50.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             34.826\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,483)\n",
            "Avg Obs:                       7.7812                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       8.0000                                           \n",
            "                                                                                \n",
            "                              Parameter Estimates                              \n",
            "===============================================================================\n",
            "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-------------------------------------------------------------------------------\n",
            "Mkt_RF          1.3003     0.1053     12.352     0.0000      1.0935      1.5072\n",
            "SMB            -0.0802     0.2612    -0.3071     0.7589     -0.5934      0.4329\n",
            "HML             0.3932     0.3235     1.2154     0.2248     -0.2424      1.0288\n",
            "RMW             0.4301     0.3170     1.3570     0.1754     -0.1927      1.0529\n",
            "CMA            -0.4656     0.4255    -1.0942     0.2744     -1.3018      0.3705\n",
            "WML             0.0693     0.1819     0.3809     0.7034     -0.2882      0.4268\n",
            "Total-Score    -0.0001     0.0003    -0.4300     0.6674     -0.0008      0.0005\n",
            "===============================================================================\n",
            "\n",
            "F-test for Poolability: 1.7983\n",
            "P-value: 0.0855\n",
            "Distribution: F(7,483)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + E-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature    VIF\n",
            "0    const 5.7700\n",
            "1   Mkt_RF 1.4400\n",
            "2      SMB 1.3700\n",
            "3      HML 5.4900\n",
            "4      RMW 1.8900\n",
            "5      CMA 4.1400\n",
            "6      WML 1.5400\n",
            "7  E-Score 1.1000\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3101\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.3211\n",
            "No. Observations:                 498   R-squared (Within):               0.3101\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3111\n",
            "Time:                        09:02:19   Log-likelihood                    444.85\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      31.021\n",
            "Entities:                           8   P-value                           0.0000\n",
            "Avg Obs:                       62.250   Distribution:                   F(7,483)\n",
            "Min Obs:                       50.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             34.802\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,483)\n",
            "Avg Obs:                       7.7812                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       8.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.2993     0.1055     12.313     0.0000      1.0919      1.5066\n",
            "SMB           -0.0806     0.2612    -0.3085     0.7578     -0.5937      0.4326\n",
            "HML            0.3956     0.3234     1.2235     0.2217     -0.2397      1.0310\n",
            "RMW            0.4284     0.3166     1.3530     0.1767     -0.1937      1.0506\n",
            "CMA           -0.4687     0.4255    -1.1013     0.2713     -1.3048      0.3675\n",
            "WML            0.0694     0.1820     0.3813     0.7032     -0.2882      0.4269\n",
            "E-Score    -9.479e-05     0.0003    -0.3558     0.7222     -0.0006      0.0004\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 1.7912\n",
            "P-value: 0.0869\n",
            "Distribution: F(7,483)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + S-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature    VIF\n",
            "0    const 7.1300\n",
            "1   Mkt_RF 1.4400\n",
            "2      SMB 1.3700\n",
            "3      HML 5.4900\n",
            "4      RMW 1.8900\n",
            "5      CMA 4.1400\n",
            "6      WML 1.5400\n",
            "7  S-Score 1.1000\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3102\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.2928\n",
            "No. Observations:                 498   R-squared (Within):               0.3102\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3102\n",
            "Time:                        09:02:19   Log-likelihood                    444.87\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      31.026\n",
            "Entities:                           8   P-value                           0.0000\n",
            "Avg Obs:                       62.250   Distribution:                   F(7,483)\n",
            "Min Obs:                       50.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             34.816\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,483)\n",
            "Avg Obs:                       7.7812                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       8.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.3000     0.1056     12.315     0.0000      1.0926      1.5074\n",
            "SMB           -0.0804     0.2612    -0.3077     0.7584     -0.5935      0.4328\n",
            "HML            0.3940     0.3237     1.2174     0.2240     -0.2419      1.0300\n",
            "RMW            0.4295     0.3165     1.3572     0.1754     -0.1923      1.0513\n",
            "CMA           -0.4665     0.4259    -1.0953     0.2739     -1.3033      0.3703\n",
            "WML            0.0692     0.1819     0.3806     0.7037     -0.2883      0.4268\n",
            "S-Score       -0.0001     0.0003    -0.3877     0.6984     -0.0007      0.0005\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 1.8045\n",
            "P-value: 0.0843\n",
            "Distribution: F(7,483)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Model: FF6 + G-Score ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "   feature    VIF\n",
            "0    const 5.2800\n",
            "1   Mkt_RF 1.4400\n",
            "2      SMB 1.3700\n",
            "3      HML 5.4800\n",
            "4      RMW 1.8900\n",
            "5      CMA 4.1400\n",
            "6      WML 1.5400\n",
            "7  G-Score 1.1000\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "                          PanelOLS Estimation Summary                           \n",
            "================================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                        0.3101\n",
            "Estimator:                   PanelOLS   R-squared (Between):              0.3331\n",
            "No. Observations:                 498   R-squared (Within):               0.3101\n",
            "Date:                Tue, Apr 29 2025   R-squared (Overall):              0.3115\n",
            "Time:                        09:02:19   Log-likelihood                    444.85\n",
            "Cov. Estimator:                Robust                                           \n",
            "                                        F-statistic:                      31.019\n",
            "Entities:                           8   P-value                           0.0000\n",
            "Avg Obs:                       62.250   Distribution:                   F(7,483)\n",
            "Min Obs:                       50.000                                           \n",
            "Max Obs:                       64.000   F-statistic (robust):             34.813\n",
            "                                        P-value                           0.0000\n",
            "Time periods:                      64   Distribution:                   F(7,483)\n",
            "Avg Obs:                       7.7812                                           \n",
            "Min Obs:                       7.0000                                           \n",
            "Max Obs:                       8.0000                                           \n",
            "                                                                                \n",
            "                             Parameter Estimates                              \n",
            "==============================================================================\n",
            "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "------------------------------------------------------------------------------\n",
            "Mkt_RF         1.2988     0.1054     12.318     0.0000      1.0916      1.5060\n",
            "SMB           -0.0805     0.2611    -0.3084     0.7579     -0.5936      0.4326\n",
            "HML            0.3964     0.3231     1.2267     0.2205     -0.2385      1.0313\n",
            "RMW            0.4277     0.3168     1.3500     0.1776     -0.1948      1.0501\n",
            "CMA           -0.4698     0.4252    -1.1049     0.2698     -1.3052      0.3657\n",
            "WML            0.0695     0.1820     0.3816     0.7029     -0.2882      0.4271\n",
            "G-Score    -7.181e-05     0.0002    -0.3397     0.7343     -0.0005      0.0003\n",
            "==============================================================================\n",
            "\n",
            "F-test for Poolability: 1.7859\n",
            "P-value: 0.0880\n",
            "Distribution: F(7,483)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "--- Analyzing: High ESG Risk ---\n",
            "Note: Skipping 'FF6 + Total-Score' for 'High ESG Risk' group due to data limitations (only JYD, no original ESG data via yesg).\n",
            "Note: Skipping 'FF6 + E-Score' for 'High ESG Risk' group due to data limitations (only JYD, no original ESG data via yesg).\n",
            "Note: Skipping 'FF6 + S-Score' for 'High ESG Risk' group due to data limitations (only JYD, no original ESG data via yesg).\n",
            "Note: Skipping 'FF6 + G-Score' for 'High ESG Risk' group due to data limitations (only JYD, no original ESG data via yesg).\n",
            "\n",
            "--- Model: FF6 Only ---\n",
            "\n",
            "Performing VIF Calculation:\n",
            "  feature    VIF\n",
            "0   const 1.7500\n",
            "1  Mkt_RF 1.6600\n",
            "2     SMB 3.5000\n",
            "3     HML 2.9600\n",
            "4     RMW 1.8900\n",
            "5     CMA 2.6800\n",
            "6     WML 2.8200\n",
            "Thresholds: Typically VIF > 5 or 10 indicates high multicollinearity.\n",
            "\n",
            "Performing Regression (Fixed Effects PanelOLS with Robust Standard Errors):\n",
            "Warning: Only one entity (firm) left in this subset. Panel regression is not meaningful. Running OLS with robust errors instead.\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:          Excess_Return   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                 -0.260\n",
            "Method:                 Least Squares   F-statistic:                    0.3739\n",
            "Date:                Tue, 29 Apr 2025   Prob (F-statistic):              0.885\n",
            "Time:                        09:02:19   Log-Likelihood:                -14.543\n",
            "No. Observations:                  23   AIC:                             43.09\n",
            "Df Residuals:                      16   BIC:                             51.03\n",
            "Df Model:                           6                                         \n",
            "Covariance Type:                  HC1                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1645      0.197      0.837      0.402      -0.221       0.550\n",
            "Mkt_RF        -3.2767      3.076     -1.065      0.287      -9.306       2.753\n",
            "SMB            6.2489      8.735      0.715      0.474     -10.871      23.369\n",
            "HML           -1.0870      8.188     -0.133      0.894     -17.135      14.961\n",
            "RMW            3.7991     11.537      0.329      0.742     -18.813      26.411\n",
            "CMA           -0.4669      8.979     -0.052      0.959     -18.066      17.133\n",
            "WML            1.3456      7.482      0.180      0.857     -13.319      16.010\n",
            "==============================================================================\n",
            "Omnibus:                       19.621   Durbin-Watson:                   1.465\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.683\n",
            "Skew:                           1.676   Prob(JB):                     1.61e-06\n",
            "Kurtosis:                       7.075   Cond. No.                         126.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
            "\n",
            "--- Analysis Complete ---\n",
            "\n",
            "Note on Reliability:\n",
            "- Stock data download failed for DPSGY.\n",
            "- ESG data was retrieved using the 'yesg' library. Download failed or no data found for DPSGY, AMKBY, KHNGY, DSDVY, JYD.\n",
            "- The ESG rating methodology and scale changed in November 2019 (before: high score = low risk; after: high score = high risk).\n",
            "- FF6-only models are run on the full period (Sept 2014 - Mar 2025).\n",
            "- FF6 + ESG models are run only on the post-change period (2019-12-01 - 2025-03-01) to maintain a consistent interpretation of ESG scores.\n",
            "- MICE imputation was applied to fill remaining gaps in the ESG data (in the full dataset) using other numerical columns as predictors.\n",
            "- Imputation for firms/periods with very limited original historical ESG data (e.g., for the earlier periods of ZIM, XPO, ODFL, and entirely for the downloaded tickers with failed yesg download) is speculative and reduces the reliability of ESG coefficient estimates for groups containing these firms.\n",
            "- The High ESG Risk group (JYD) had no original ESG data from 'yesg' for the entire period, so ESG models are skipped for this group.\n",
            "- VIF analysis helps assess multicollinearity *within the predictors of each specific model*.\n",
            "- Panel Regression (Fixed Effects) with robust standard errors is used where appropriate, controlling for unobserved firm-specific effects. Fallback to OLS/PooledOLS with robust errors occurs if FE conditions aren't met or only one entity remains.\n",
            "\n",
            "Conclusion regarding research reliability:\n",
            "The findings regarding the Fama-French factors are reasonably reliable, especially for the groups where PanelOLS was successfully applied. However, the ability to draw strong, reliable conclusions about the impact of ESG scores is severely limited by:\n",
            "  1. The necessity to restrict ESG analysis to the shorter post-change period.\n",
            "  2. The substantial reliance on MICE imputation for firms/periods with sparse or no original historical ESG data from the 'yesg' source.\n",
            "Relying solely on these ESG results for definitive research conclusions or generalizations about ESG's impact across the full period or different firms is not recommended without more complete original historical ESG data from a reliable source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Electronics Panel Regression\n",
        "#### The script will:\n",
        "Download stock and ESG data for the tickers it can find on Yahoo Finance within the specified date range (Nov 2019 to Mar 2025).\n",
        "Load and process the Fama-French data for the same period.\n",
        "Merge the data, creating a panel structure.\n",
        "Perform MICE imputation if there are missing numerical values.\n",
        "Split the data based on the initial, static ESG risk grouping.\n",
        "For each group (Low, Medium, High), it will:\n",
        "Calculate and print VIFs for the independent variables.\n",
        "Run and print the summary of a Pooled OLS regression.\n",
        "Run and print the results of Fixed Effects and Random Effects panel regressions.\n"
      ],
      "metadata": {
        "id": "FRua_-fXGrL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas yfinance yesg statsmodels linearmodels scikit-learn openpyxl # openpyxl for excel reading if needed, added as safeguard\n",
        "!pip install yesg\n",
        "!pip install linearmodels\n",
        "!pip install scikit-learn\n",
        "!pip install openpyxl\n",
        "!pip install statsmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS4NBjnMG6z0",
        "outputId": "5e388791-7a42-4bbd-8670-2d17127f13e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.56)\n",
            "Collecting yesg\n",
            "  Downloading yesg-2.1.1.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Collecting linearmodels\n",
            "  Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Collecting mypy-extensions>=0.4 (from linearmodels)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Collecting pyhdfe>=0.1 (from linearmodels)\n",
            "  Downloading pyhdfe-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting formulaic>=1.0.0 (from linearmodels)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting setuptools-scm<9.0.0,>=8.0.0 (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=1.0.0->linearmodels)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyhdfe-0.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: yesg\n",
            "  Building wheel for yesg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yesg: filename=yesg-2.1.1-py3-none-any.whl size=6105 sha256=746ef1b3040c4153184935636cf3986c3a659230d3c91d235d3653ea2cdc6b5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/8d/48/f5e8ff0315a46301e15c68371e297b460b33e1c846117725bc\n",
            "Successfully built yesg\n",
            "Installing collected packages: yesg, setuptools-scm, mypy-extensions, interface-meta, pyhdfe, formulaic, linearmodels\n",
            "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 linearmodels-6.1 mypy-extensions-1.1.0 pyhdfe-0.2.0 setuptools-scm-8.3.1 yesg-2.1.1\n",
            "Requirement already satisfied: yesg in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: linearmodels in /usr/local/lib/python3.11/dist-packages (6.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.15.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.14.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.4 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.1.0)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Requirement already satisfied: pyhdfe>=0.1 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.2.0)\n",
            "Requirement already satisfied: formulaic>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.1.1)\n",
            "Requirement already satisfied: setuptools-scm<9.0.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (8.3.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "\n",
        "# Try a standard ticker\n",
        "ticker = \"AAPL\"\n",
        "# Try a recent, short date range\n",
        "start = dt.date.today() - dt.timedelta(days=30) # Last 30 days\n",
        "end = dt.date.today()\n",
        "print(f\"Attempting to download {ticker} from {start} to {end}\")\n",
        "\n",
        "try:\n",
        "    data = yf.download(ticker, start=start, end=end)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"\\nTest download failed: Data is empty.\")\n",
        "    elif 'Adj Close' not in data.columns:\n",
        "        print(\"\\nTest download failed: 'Adj Close' column not found.\")\n",
        "        print(\"Available columns:\", data.columns.tolist()) # See what columns *are* there\n",
        "    else:\n",
        "        print(\"\\nTest download successful. Head of data:\")\n",
        "        print(data.head())\n",
        "        print(\"\\nColumns:\", data.columns.tolist()) # Confirm Adj Close is there\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nTest download failed with an error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4wn1EGjTAIF",
        "outputId": "d3ac24db-a8c4-4996-d9a4-da1c262c76a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download AAPL from 2025-03-30 to 2025-04-29\n",
            "\n",
            "Test download failed: 'Adj Close' column not found.\n",
            "Available columns: [('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import yesg\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from linearmodels.panel import PanelOLS\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.experimental import enable_iterative_imputer # Enable IterativeImputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from io import StringIO # To read FF data from string\n",
        "import time # To add a small delay if needed\n",
        "import datetime as dt # Import datetime\n",
        "\n",
        "# Suppress specific warnings for cleaner output, be cautious in production code\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# 1. Identify Target Companies and Tickers\n",
        "company_ticker_map = {\n",
        "    \"Apple Inc.\": \"AAPL\",\n",
        "    \"Samsung Electro-Mechanics\": \"009150.KS\", # South Korea\n",
        "    \"Sony Group Corp.\": \"SONY\", # US Listing (ADR)\n",
        "    \"LG Display Co., Ltd.\": \"034220.KS\", # South Korea\n",
        "    \"Foxconn Industrial Internet Co., Ltd.\": \"601138.SS\", # Shanghai\n",
        "    \"Siemens Energy\": \"ENR.DE\", # Frankfurt (FWB) - using .DE suffix\n",
        "    \"Philips\": \"PHG\", # US Listing (ADR)\n",
        "    \"GoerTek Inc.\": \"002241.SZ\", # Shenzhen\n",
        "    \"STMicroelectronics\": \"STM\", # US Listing (ADR)\n",
        "    \"Elite Material Co., Ltd.\": None, # Need to find ticker, assuming not readily available on Yahoo\n",
        "    \"Garmin Ltd.\": \"GRMN\", # US Listing\n",
        "    \"Hamamatsu Photonics KK\": None, # Need to find ticker, assuming not readily available on Yahoo\n",
        "    \"Amazon\": \"AMZN\", # US Listing (for comparison)\n",
        "    \"OFILM Group Co., Ltd.\": None, # Need to find ticker, assuming not readily available on Yahoo\n",
        "    \"TCL Technology Group Corp.\": \"000100.SZ\", # Shenzhen - Added based on search, might not be the right entity or ticker\n",
        "    \"Shenzhen Huaqiang Industry Co., Ltd.\": None # Need to find ticker, assuming not readily available on Yahoo\n",
        "}\n",
        "\n",
        "tickers_to_analyze = {k: v for k, v in company_ticker_map.items() if v is not None}\n",
        "initial_tickers_list = list(tickers_to_analyze.values())\n",
        "print(f\"Attempting to download data for tickers: {initial_tickers_list}\")\n",
        "\n",
        "# 2. Define ESG Risk Groups (Static based on the prompt)\n",
        "ticker_esg_group_map = {\n",
        "    \"AAPL\": \"Low\",\n",
        "    \"009150.KS\": \"Low\",\n",
        "    \"SONY\": \"Low\",\n",
        "    \"034220.KS\": \"Low\",\n",
        "    \"601138.SS\": \"Low\",\n",
        "    \"ENR.DE\": \"Low\",\n",
        "    \"PHG\": \"Low\",\n",
        "    \"002241.SZ\": \"Medium\",\n",
        "    \"STM\": \"Medium\",\n",
        "    \"GRMN\": \"Medium\",\n",
        "    \"AMZN\": \"High\",\n",
        "    \"000100.SZ\": \"High\",\n",
        "}\n",
        "\n",
        "# Filter the group map to only include tickers we are attempting to analyze\n",
        "ticker_esg_group_map_working = ticker_esg_group_map.copy() # Create a working copy\n",
        "\n",
        "# Define the analysis date range\n",
        "start_date = '2019-11-01'\n",
        "end_date = '2025-04-01' # Align with FF data end + 1 day\n",
        "\n",
        "print(f\"\\nAnalysis period: {start_date} to {end_date}\")\n",
        "\n",
        "# Helper function to get stock price data robustly\n",
        "def get_stock_prices(ticker, start, end):\n",
        "    \"\"\"Downloads stock data and attempts to get 'Adj Close', handling MultiIndex.\"\"\"\n",
        "    try:\n",
        "        # Try with auto_adjust=True explicitly\n",
        "        # Use timezone=None to avoid potential issues later\n",
        "        data = yf.download(ticker, start=str(pd.to_datetime(start).date()), end=str(pd.to_datetime(end).date()), progress=False, auto_adjust=True, ignore_tz=True)\n",
        "\n",
        "        if not data.empty and 'Close' in data.columns:\n",
        "             # If auto_adjust=True worked, 'Close' should be the adjusted close\n",
        "             # Rename 'Close' to 'Adj Close' for consistency\n",
        "             data = data.rename(columns={'Close': 'Adj Close'})\n",
        "\n",
        "        # If auto_adjust=True didn't work as expected or returned a MultiIndex\n",
        "        if 'Adj Close' not in data.columns:\n",
        "             # Try downloading without auto_adjust and check for MultiIndex\n",
        "             data = yf.download(ticker, start=str(pd.to_datetime(start).date()), end=str(pd.to_datetime(end).date()), progress=False, auto_adjust=False, ignore_tz=True)\n",
        "\n",
        "             if not data.empty:\n",
        "                 # Check for MultiIndex columns like ('AAPL', 'Adj Close') or ('AAPL', 'Close')\n",
        "                 if isinstance(data.columns, pd.MultiIndex):\n",
        "                     # Flatten the MultiIndex columns to single level, e.g., 'AAPL_Adj Close'\n",
        "                     # Or try to directly access levels if we know the ticker\n",
        "                     try:\n",
        "                         # Try accessing using the ticker directly if it's the first level\n",
        "                         if ticker in data.columns.get_level_values(0):\n",
        "                            # Prioritize ('Ticker', 'Adj Close')\n",
        "                            if (ticker, 'Adj Close') in data.columns:\n",
        "                                data['Adj Close'] = data[(ticker, 'Adj Close')]\n",
        "                                print(f\"Accessed MultiIndex ('{ticker}', 'Adj Close') for {ticker}\")\n",
        "                            # Fallback to ('Ticker', 'Close') if Adj Close is missing in MultiIndex\n",
        "                            elif (ticker, 'Close') in data.columns:\n",
        "                                data['Adj Close'] = data[(ticker, 'Close')]\n",
        "                                print(f\"Accessed MultiIndex ('{ticker}', 'Close') for {ticker}, using as Adj Close\")\n",
        "                            else:\n",
        "                                print(f\"Could not find ('{ticker}', 'Adj Close') or ('{ticker}', 'Close') in MultiIndex for {ticker}. Columns: {data.columns.tolist()}\")\n",
        "                                return pd.DataFrame() # Indicate failure\n",
        "\n",
        "                         else:\n",
        "                            # Fallback to trying to find 'Adj Close' or 'Close' in the second level\n",
        "                            if 'Adj Close' in data.columns.get_level_values(1):\n",
        "                                # Find the column tuple where the second level is 'Adj Close'\n",
        "                                adj_close_col = [col for col in data.columns if col[1] == 'Adj Close']\n",
        "                                if adj_close_col:\n",
        "                                     data['Adj Close'] = data[adj_close_col[0]]\n",
        "                                     print(f\"Accessed MultiIndex with 'Adj Close' at level 1 for {ticker}\")\n",
        "                                elif 'Close' in data.columns.get_level_values(1):\n",
        "                                    # Find the column tuple where the second level is 'Close'\n",
        "                                    close_col = [col for col in data.columns if col[1] == 'Close']\n",
        "                                    if close_col:\n",
        "                                         data['Adj Close'] = data[close_col[0]]\n",
        "                                         print(f\"Accessed MultiIndex with 'Close' at level 1 for {ticker}, using as Adj Close\")\n",
        "                                    else:\n",
        "                                        print(f\"Could not find 'Adj Close' or 'Close' in MultiIndex level 1 for {ticker}. Columns: {data.columns.tolist()}\")\n",
        "                                        return pd.DataFrame() # Indicate failure\n",
        "                                else:\n",
        "                                    print(f\"Could not find 'Adj Close' or 'Close' in MultiIndex level 1 for {ticker}. Columns: {data.columns.tolist()}\")\n",
        "                                    return pd.DataFrame() # Indicate failure\n",
        "                            else:\n",
        "                                print(f\"Could not find 'Adj Close' or 'Close' in MultiIndex level 1 for {ticker}. Columns: {data.columns.tolist()}\")\n",
        "                                return pd.DataFrame() # Indicate failure\n",
        "\n",
        "\n",
        "                     except Exception as e_multiindex:\n",
        "                         print(f\"Error processing MultiIndex columns for {ticker}: {e_multiindex}. Columns: {data.columns.tolist()}\")\n",
        "                         return pd.DataFrame() # Indicate failure\n",
        "\n",
        "                 # Check for standard column names if not MultiIndex\n",
        "                 elif 'Adj Close' not in data.columns and 'Close' in data.columns:\n",
        "                     data['Adj Close'] = data['Close'] # Use 'Close' if 'Adj Close' is missing\n",
        "                     print(f\"Using 'Close' column as 'Adj Close' for {ticker}\")\n",
        "                 elif 'Adj Close' not in data.columns:\n",
        "                     print(f\"'Adj Close' and 'Close' columns not found for {ticker}. Available columns: {data.columns.tolist()}\")\n",
        "                     return pd.DataFrame() # Indicate failure\n",
        "                 # If 'Adj Close' was found directly, the column exists, continue.\n",
        "\n",
        "        # Ensure index is timezone naive\n",
        "        if not data.empty and data.index.tz is not None:\n",
        "            data.index = data.index.tz_convert(None)\n",
        "\n",
        "\n",
        "        # If data is empty or 'Adj Close' is still missing after checks\n",
        "        if data.empty or 'Adj Close' not in data.columns:\n",
        "             print(f\"Failed to get usable 'Adj Close' data for {ticker}.\")\n",
        "             return pd.DataFrame() # Indicate failure\n",
        "\n",
        "\n",
        "        # Return only the 'Adj Close' column\n",
        "        return data[['Adj Close']]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during yfinance download or processing for {ticker}: {e}\")\n",
        "        return pd.DataFrame() # Indicate failure\n",
        "\n",
        "\n",
        "# --- Quick Test Download ---\n",
        "# Add a small buffer for the quick test end date\n",
        "quick_test_end_date = pd.to_datetime(start_date) + pd.Timedelta(days=30) # Test for one month starting from start_date\n",
        "\n",
        "print(\"\\nPerforming quick test download for first ticker...\")\n",
        "test_ticker = initial_tickers_list[0] if initial_tickers_list else None\n",
        "if test_ticker:\n",
        "    test_data = get_stock_prices(test_ticker, start_date, quick_test_end_date)\n",
        "    if test_data.empty:\n",
        "         print(f\"Quick test download failed for {test_ticker}. yfinance might be having issues or data is unavailable.\")\n",
        "         print(\"Please try troubleshooting steps (check internet, retry later, verify ticker on Yahoo Finance website).\")\n",
        "         # If test fails for the first ticker, remove it from consideration\n",
        "         if test_ticker in ticker_esg_group_map_working:\n",
        "             del ticker_esg_group_map_working[test_ticker]\n",
        "    else:\n",
        "         print(f\"Quick test download successful for {test_ticker}. Found {test_data.shape[0]} data points.\")\n",
        "    print(\"-\" * 30) # Separator\n",
        "else:\n",
        "    print(\"No tickers provided to attempt download.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Data Acquisition\n",
        "\n",
        "# --- Stock Data ---\n",
        "print(\"\\nDownloading stock data for analysis period...\")\n",
        "stock_data = {}\n",
        "successful_stock_tickers = []\n",
        "\n",
        "tickers_to_process_stock = list(ticker_esg_group_map_working.keys()) # Use working map after quick test\n",
        "\n",
        "for ticker in tickers_to_process_stock:\n",
        "    print(f\"Processing stock data for {ticker}...\")\n",
        "    daily_data = get_stock_prices(ticker, start_date, end_date)\n",
        "\n",
        "    if not daily_data.empty:\n",
        "        # Resample to monthly, taking the last trading day's adjusted close\n",
        "        # Ensure we have enough data points before resampling\n",
        "        if daily_data.shape[0] < 2:\n",
        "             print(f\"Warning: Insufficient daily data points ({daily_data.shape[0]}) for {ticker} to calculate monthly returns. Excluding from analysis.\")\n",
        "             if ticker in ticker_esg_group_map_working:\n",
        "                del ticker_esg_group_map_working[ticker]\n",
        "             continue\n",
        "\n",
        "        monthly_close = daily_data['Adj Close'].resample('M').last()\n",
        "        # Calculate monthly returns\n",
        "        monthly_returns = monthly_close.pct_change()\n",
        "\n",
        "        # --- FIX: Renaming Series Name ---\n",
        "        # The error \"TypeError: 'str' object is not callable\" occurs because\n",
        "        # series.rename('Name') is meant to rename the Series *name*, not its index.\n",
        "        # However, in some Pandas versions or contexts, passing a string\n",
        "        # might incorrectly trigger index value renaming using the string\n",
        "        # as a mapper, leading to the error.\n",
        "        # The correct and unambiguous way to rename the Series is:\n",
        "        monthly_returns.name = 'Return'\n",
        "        # Or using rename method explicitly for name: monthly_returns = monthly_returns.rename(None, name='Return')\n",
        "        # --- END FIX ---\n",
        "\n",
        "        # Shift dates to the first of the month for consistency\n",
        "        # Ensure index is timezone naive before converting to period/timestamp\n",
        "        if monthly_returns.index.tz is not None:\n",
        "             monthly_returns.index = monthly_returns.index.tz_convert(None)\n",
        "        monthly_returns.index = monthly_returns.index.to_period('M').to_timestamp('D')\n",
        "\n",
        "\n",
        "        # Store the monthly returns if calculation was successful and there's data\n",
        "        if not monthly_returns.dropna().empty and monthly_returns.shape[0] > 1: # Need at least 2 monthly points for a return\n",
        "            stock_data[ticker] = monthly_returns # Name is already set to 'Return'\n",
        "            successful_stock_tickers.append(ticker)\n",
        "            print(f\"Successfully processed monthly stock returns for {ticker}\")\n",
        "        else:\n",
        "             print(f\"Warning: Processed monthly stock returns for {ticker} are all NaN or insufficient data points ({monthly_returns.shape[0]}). Excluding from analysis.\")\n",
        "             if ticker in ticker_esg_group_map_working:\n",
        "                del ticker_esg_group_map_working[ticker]\n",
        "    else:\n",
        "        print(f\"Could not retrieve usable daily stock data for {ticker}. Excluding from analysis.\")\n",
        "        if ticker in ticker_esg_group_map_working:\n",
        "            del ticker_esg_group_map_working[ticker]\n",
        "\n",
        "\n",
        "# Check if any stock data was successfully downloaded for the remaining tickers\n",
        "if not stock_data:\n",
        "    print(\"\\nFATAL ERROR: No stock data was successfully downloaded for any ticker after trying. Cannot proceed with analysis.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Combine successful stock returns into a single DataFrame\n",
        "# The 'Return' column name is now correctly set on the Series before concat\n",
        "stock_returns_df = pd.concat(stock_data, names=['Ticker', 'Date']).reset_index()\n",
        "# Filter to only include tickers that were successfully processed for stock data\n",
        "stock_returns_df = stock_returns_df[stock_returns_df['Ticker'].isin(stock_data.keys())].copy()\n",
        "\n",
        "\n",
        "# --- ESG Data ---\n",
        "print(\"\\nDownloading ESG data...\")\n",
        "esg_data = {}\n",
        "successful_esg_tickers = []\n",
        "\n",
        "tickers_to_process_esg = list(stock_data.keys()) # Use keys from successfully downloaded stock data\n",
        "\n",
        "for ticker in tickers_to_process_esg:\n",
        "     if ticker not in ticker_esg_group_map_working:\n",
        "         continue\n",
        "\n",
        "     print(f\"Processing ESG data for {ticker}...\")\n",
        "     try:\n",
        "        esg_hist = yesg.get_historic_esg(ticker)\n",
        "        # Optional: Add a small delay\n",
        "        # time.sleep(0.5)\n",
        "\n",
        "        if esg_hist is None or esg_hist.empty:\n",
        "            print(f\"Warning: No ESG data found for {ticker} from yesg. Excluding from analysis.\")\n",
        "            if ticker in ticker_esg_group_map_working:\n",
        "                del ticker_esg_group_map_working[ticker]\n",
        "            continue\n",
        "\n",
        "        # Filter to the analysis period (post-Nov 2019 for risk rating scale)\n",
        "        esg_hist.index = pd.to_datetime(esg_hist.index)\n",
        "        # Ensure index is timezone naive BEFORE filtering and resampling\n",
        "        if esg_hist.index.tz is not None:\n",
        "             esg_hist.index = esg_hist.index.tz_convert(None)\n",
        "\n",
        "        esg_filtered = esg_hist.loc[(esg_hist.index >= start_date) & (esg_hist.index <= end_date)].copy() # Use .copy()\n",
        "\n",
        "        # Ensure index is on the first day of the month\n",
        "        esg_filtered.index = esg_filtered.index.to_period('M').to_timestamp('D')\n",
        "\n",
        "        # Keep only the Total-Score\n",
        "        if 'Total-Score' in esg_filtered.columns:\n",
        "            # Store ESG data if there's at least some non-NaN data in the score\n",
        "            if not esg_filtered['Total-Score'].dropna().empty:\n",
        "                esg_data[ticker] = esg_filtered['Total-Score'].rename('Total_ESG_Risk_Score')\n",
        "                successful_esg_tickers.append(ticker)\n",
        "                print(f\"Successfully processed ESG data for {ticker}\")\n",
        "            else:\n",
        "                print(f\"Warning: Processed ESG total scores for {ticker} are all NaN in the period {start_date} to {end_date}. Excluding from analysis.\")\n",
        "                if ticker in ticker_esg_group_map_working:\n",
        "                    del ticker_esg_group_map_working[ticker]\n",
        "        else:\n",
        "             print(f\"Warning: 'Total-Score' column not found in ESG data for {ticker}. Excluding from analysis.\")\n",
        "             if ticker in ticker_esg_group_map_working:\n",
        "                del ticker_esg_group_map_working[ticker]\n",
        "\n",
        "     except Exception as e:\n",
        "        print(f\"Error downloading or processing ESG data for {ticker}: {e}. Excluding from analysis.\")\n",
        "        if ticker in ticker_esg_group_map_working:\n",
        "             del ticker_esg_group_map_working[ticker]\n",
        "\n",
        "# Check if any ESG data was successfully downloaded for the remaining tickers\n",
        "if not esg_data and successful_stock_tickers:\n",
        "     print(\"\\nFATAL ERROR: No ESG data was successfully downloaded for any ticker that had stock data. Cannot proceed with analysis.\")\n",
        "     # exit() # Let the merge fail naturally\n",
        "\n",
        "# Update the group map one final time based on successful stock AND ESG download\n",
        "final_tickers_for_analysis = list(esg_data.keys())\n",
        "ticker_esg_group_map_final = {k: v for k, v in ticker_esg_group_map_working.items() if k in final_tickers_for_analysis}\n",
        "\n",
        "if not ticker_esg_group_map_final:\n",
        "     print(\"\\nFATAL ERROR: No tickers remain for analysis after attempting stock and ESG data downloads. Cannot proceed.\")\n",
        "     exit()\n",
        "\n",
        "# Combine ESG scores into a single DataFrame\n",
        "esg_scores_df = pd.concat(esg_data, names=['Ticker', 'Date']).reset_index()\n",
        "\n",
        "\n",
        "# --- Fama-French Data ---\n",
        "print(\"\\nLoading Fama-French data...\")\n",
        "try:\n",
        "    # Use the provided string data directly\n",
        "    ff_data_string = \"\"\"Date\tMkt_RF\tSMB\tHML\tRMW\tCMA\tRF\tWML\n",
        "9/1/14\t-3.09\t-2.77\t-0.83\t0.18\t-0.23\t0\t1.08\n",
        "10/1/14\t0.33\t-0.58\t-2.81\t0.84\t-0.23\t0\t-0.14\n",
        "11/1/14\t1.65\t-2.44\t-2\t0.76\t-0.27\t0\t0.98\n",
        "12/1/14\t-1.44\t1.78\t0.04\t-0.31\t0.13\t0\t1.12\n",
        "1/1/15\t-1.74\t-0.05\t-2.49\t2.09\t-0.69\t0\t3.9\n",
        "2/1/15\t5.91\t-0.51\t-0.3\t-0.99\t-0.88\t0\t-2.43\n",
        "3/1/15\t-1.17\t1.44\t-0.8\t0.05\t-0.4\t0\t1.75\n",
        "4/1/15\t2.71\t0.9\t2.27\t-0.7\t-0.48\t0\t-3.3\n",
        "5/1/15\t0.5\t1.05\t-1.12\t-1.06\t-0.41\t0\t4.29\n",
        "6/1/15\t-2.09\t2.06\t-0.66\t0\t-0.27\t0\t1.99\n",
        "7/1/15\t1.13\t-3.29\t-3.25\t1.76\t-0.72\t0\t3.53\n",
        "8/1/15\t-6.18\t1.37\t1.27\t0.51\t0.84\t0\t-0.68\n",
        "9/1/15\t-3.91\t-0.1\t-1.02\t1.86\t0.53\t0\t3.48\n",
        "10/1/15\t7.32\t-2.36\t0.34\t0.82\t-0.49\t0\t-2.61\n",
        "11/1/15\t-0.3\t1.46\t-1.99\t0.55\t-1.39\t0\t2.07\n",
        "12/1/15\t-1.73\t0.84\t-1.7\t0.76\t-0.43\t0.01\t3.39\n",
        "1/1/16\t-6.32\t-1.75\t0.76\t2.73\t2.75\t0.01\t0.58\n",
        "2/1/16\t-0.5\t0.93\t-0.21\t1.17\t1.83\t0.02\t-2.71\n",
        "3/1/16\t6.92\t1.34\t0.42\t0.66\t-0.67\t0.02\t-2.74\n",
        "4/1/16\t1.91\t1.25\t2.98\t-2.77\t0.77\t0.01\t-3.26\n",
        "5/1/16\t0.45\t-0.23\t-2.11\t0.64\t-1.25\t0.01\t2.93\n",
        "6/1/16\t-1.67\t-0.41\t-0.73\t1.59\t2.11\t0.02\t6.09\n",
        "7/1/16\t4.48\t1.38\t0.46\t0.02\t-1.4\t0.02\t-1.71\n",
        "8/1/16\t0.29\t0.56\t2.19\t-1.38\t-0.44\t0.02\t-3.85\n",
        "9/1/16\t0.9\t2.1\t-1.23\t-0.52\t0.13\t0.02\t1.63\n",
        "10/1/16\t-1.88\t-1.09\t4.09\t-1.07\t1.27\t0.02\t-1.14\n",
        "11/1/16\t1.39\t0.61\t4.39\t-2.02\t1.77\t0.01\t-4.14\n",
        "12/1/16\t2.25\t-0.14\t2.76\t-0.11\t0.79\t0.03\t-1.49\n",
        "1/1/17\t2.72\t0.4\t-0.53\t-0.16\t-0.77\t0.04\t1.76\n",
        "2/1/17\t2.28\t-0.71\t-1.74\t1.14\t-0.99\t0.04\t-1.88\n",
        "3/1/17\t1.48\t0.18\t-1.81\t1.01\t-0.54\t0.03\t0.12\n",
        "4/1/17\t1.86\t0.33\t-1.39\t1.03\t-1.01\t0.05\t0.06\n",
        "5/1/17\t2.2\t-0.61\t-2.59\t2.09\t-1.03\t0.06\t1.11\n",
        "6/1/17\t0.6\t1.64\t1.74\t-1.9\t0.05\t0.06\t0.5\n",
        "7/1/17\t2.51\t0.02\t1.02\t-0.76\t0.34\t0.07\t1.77\n",
        "8/1/17\t0.13\t-0.15\t-1.34\t1.21\t-1.3\t0.09\t1.95\n",
        "9/1/17\t2.3\t1.35\t1.64\t-1.17\t1.09\t0.09\t0.01\n",
        "10/1/17\t1.8\t-0.85\t-0.9\t0.99\t-1.92\t0.09\t3.44\n",
        "11/1/17\t1.93\t-0.67\t-0.13\t1.43\t-0.21\t0.08\t0.46\n",
        "12/1/17\t1.38\t0.83\t0.11\t-0.19\t0.81\t0.09\t-1.07\n",
        "1/1/18\t5.15\t-1.21\t-1.43\t-0.17\t-0.54\t0.11\t3.27\n",
        "2/1/18\t-4.00\t0.77\t-1.79\t1.10\t-1.77\t0.11\t2.43\n",
        "3/1/18\t-1.76\t1.20\t-0.42\t0.40\t-0.55\t0.12\t-1.11\n",
        "4/1/18\t0.93\t-0.40\t1.76\t-1.53\t1.02\t0.14\t-0.49\n",
        "5/1/18\t0.49\t1.51\t-4.52\t1.21\t-2.58\t0.14\t3.11\n",
        "6/1/18\t-0.49\t-0.80\t-1.39\t0.45\t-0.08\t0.14\t-1.38\n",
        "7/1/18\t2.53\t-2.48\t1.19\t-0.10\t0.25\t0.16\t-1.63\n",
        "8/1/18\t0.87\t-0.71\t-4.19\t1.20\t-2.61\t0.16\t3.46\n",
        "9/1/18\t0.32\t-1.43\t0.30\t0.16\t0.57\t0.15\t0.59\n",
        "10/1/18\t-8.19\t-2.28\t2.44\t0.28\t2.31\t0.19\t-2.46\n",
        "11/1/18\t0.88\t-0.62\t-1.22\t0.46\t0.08\t0.18\t-1.58\n",
        "12/1/18\t-7.67\t-0.96\t0.56\t-0.22\t0.13\t0.19\t1.46\n",
        "1/1/19\t7.47\t-0.14\t-0.71\t0.15\t-1.19\t0.21\t-3.85\n",
        "2/1/19\t2.94\t0.10\t-2.09\t0.21\t-0.93\t0.18\t0.36\n",
        "3/1/19\t0.76\t-2.08\t-3.28\t1.32\t-1.49\t0.19\t2.89\n",
        "4/1/19\t3.44\t-1.18\t-0.26\t0.80\t-1.09\t0.21\t-2.81\n",
        "5/1/19\t-5.99\t0.54\t-1.70\t0.45\t0.80\t0.21\t6.68\n",
        "6/1/19\t6.13\t-1.99\t-0.17\t0.43\t-0.02\t0.18\t-1.07\n",
        "7/1/19\t-0.07\t-1.13\t-0.63\t0.64\t-0.33\t0.19\t1.01\n",
        "8/1/19\t-2.55\t-1.64\t-2.70\t1.19\t-0.18\t0.16\t4.32\n",
        "9/1/19\t1.90\t0.19\t4.15\t0.41\t2.17\t0.18\t-4.43\n",
        "10/1/19\t2.56\t0.81\t-0.53\t0.96\t-0.72\t0.15\t-1.23\n",
        "11/1/19\t2.74\t0.04\t-2.22\t-0.09\t-1.20\t0.12\t-1.02\n",
        "12/1/19\t3.05\t1.34\t0.62\t-0.32\t0.48\t0.14\t0.27\n",
        "1/1/20\t-1.20\t-2.77\t-4.33\t0.07\t-1.97\t0.13\t4.15\n",
        "2/1/20\t-8.54\t-1.51\t-1.07\t-1.13\t-1.73\t0.12\t0.19\n",
        "3/1/20\t-13.77\t-4.44\t-9.30\t1.47\t-0.88\t0.12\t6.23\n",
        "4/1/20\t10.68\t1.76\t-4.62\t2.66\t-3.00\t0.00\t0.30\n",
        "5/1/20\t5.30\t2.02\t-5.10\t1.92\t-3.18\t0.01\t1.57\n",
        "6/1/20\t2.76\t-0.03\t-1.73\t-0.09\t-0.67\t0.01\t2.21\n",
        "7/1/20\t4.34\t-1.76\t-3.64\t1.33\t-1.21\t0.01\t6.45\n",
        "8/1/20\t6.84\t0.98\t-3.88\t2.27\t-1.52\t0.01\t0.71\n",
        "9/1/20\t-2.99\t1.98\t-2.46\t0.74\t-1.19\t0.01\t2.40\n",
        "10/1/20\t-2.60\t1.43\t2.04\t-1.04\t-0.54\t0.01\t-1.69\n",
        "11/1/20\t13.34\t1.29\t4.34\t-2.35\t0.79\t0.01\t-10.92\n",
        "12/1/20\t4.82\t3.16\t-1.20\t-0.64\t0.14\t0.01\t-0.09\n",
        "1/1/21\t-0.54\t2.93\t1.01\t-2.43\t1.31\t0.00\t2.64\n",
        "2/1/21\t2.88\t1.55\t7.46\t-1.92\t0.61\t0.00\t-6.52\n",
        "3/1/21\t3.12\t-1.32\t5.58\t1.98\t3.12\t0.00\t-4.83\n",
        "4/1/21\t4.49\t-1.22\t-1.97\t1.39\t-2.30\t0.00\t2.05\n",
        "5/1/21\t1.65\t0.24\t4.14\t0.20\t2.29\t0.00\t0.69\n",
        "6/1/21\t1.01\t-1.01\t-5.23\t0.70\t-2.28\t0.00\t0.17\n",
        "7/1/21\t1.04\t-2.16\t-1.67\t4.20\t-0.22\t0.00\t-0.84\n",
        "8/1/21\t2.42\t-0.13\t-1.71\t0.08\t-2.07\t0.00\t1.30\n",
        "9/1/21\t-3.81\t1.05\t3.86\t-2.21\t1.88\t0.00\t0.79\n",
        "10/1/21\t5.04\t-3.31\t-1.48\t1.51\t-1.32\t0.00\t3.85\n",
        "11/1/21\t-2.64\t-2.02\t-2.21\t4.59\t-0.36\t0.00\t0.83\n",
        "12/1/21\t3.63\t-0.91\t3.90\t2.18\t4.98\t0.01\t-0.73\n",
        "1/1/22\t-5.58\t-2.57\t11.96\t-2.91\t8.09\t0.00\t-3.38\n",
        "2/1/22\t-2.25\t2.11\t2.89\t-1.35\t2.27\t0.00\t0.60\n",
        "3/1/22\t2.16\t-1.89\t-1.10\t0.36\t0.79\t0.00\t3.59\n",
        "4/1/22\t-8.17\t0.48\t4.95\t0.94\t5.97\t0.00\t2.32\n",
        "5/1/22\t0.28\t-0.50\t6.14\t-1.04\t4.04\t0.03\t0.12\n",
        "6/1/22\t-8.69\t-0.02\t-1.41\t1.75\t-0.64\t0.06\t1.27\n",
        "7/1/22\t7.79\t-0.41\t-5.48\t2.37\t-5.36\t0.08\t-3.59\n",
        "8/1/22\t-4.22\t0.55\t2.49\t-2.57\t1.34\t0.19\t3.33\n",
        "9/1/22\t-9.46\t-1.49\t1.89\t-1.21\t1.17\t0.19\t3.36\n",
        "10/1/22\t6.79\t-1.35\t4.41\t0.36\t3.27\t0.23\t2.63\n",
        "11/1/22\t7.25\t-0.05\t-0.28\t2.18\t0.98\t0.29\t-1.86\n",
        "12/1/22\t-4.33\t2.42\t2.51\t-0.47\t3.25\t0.33\t3.80\n",
        "1/1/23\t6.92\t0.68\t-1.89\t-1.02\t-3.11\t0.35\t-9.00\n",
        "2/1/23\t-2.67\t0.05\t0.69\t-0.10\t0.48\t0.34\t0.50\n",
        "3/1/23\t2.27\t-3.68\t-6.92\t4.27\t-1.12\t0.36\t-1.79\n",
        "4/1/23\t1.22\t-1.52\t1.00\t0.56\t1.87\t0.35\t0.80\n",
        "5/1/23\t-1.61\t-1.31\t-4.76\t0.27\t-3.99\t0.36\t0.32\n",
        "6/1/23\t5.60\t-1.49\t1.04\t0.35\t-0.52\t0.40\t-0.14\n",
        "7/1/23\t2.96\t1.20\t3.43\t-1.05\t0.08\t0.45\t-2.46\n",
        "8/1/23\t-3.06\t-1.63\t-0.36\t2.49\t0.55\t0.45\t2.45\n",
        "9/1/23\t-4.79\t-0.76\t3.30\t0.04\t1.08\t0.43\t-0.55\n",
        "10/1/23\t-3.76\t-2.62\t0.36\t1.41\t1.52\t0.47\t1.28\n",
        "11/1/23\t8.63\t-0.88\t-1.68\t-0.77\t-2.72\t0.44\t2.04\n",
        "12/1/23\t4.97\t3.59\t1.11\t-1.40\t-1.34\t0.43\t-3.67\n",
        "1/1/24\t0.09\t-3.49\t-0.03\t1.20\t0.02\t0.47\t4.05\n",
        "2/1/24\t3.75\t-2.59\t-1.35\t0.14\t-2.56\t0.42\t5.02\n",
        "3/1/24\t2.87\t-0.43\t4.06\t-0.16\t1.17\t0.43\t0.34\n",
        "4/1/24\t-4.19\t-0.60\t1.33\t-0.08\t0.31\t0.47\t-1.63\n",
        "5/1/24\t4.05\t0.16\t-0.95\t1.03\t-1.63\t0.44\t1.89\n",
        "6/1/24\t0.83\t-3.27\t-4.46\t1.12\t-1.33\t0.41\t1.55\n",
        "7/1/24\t1.81\t4.21\t3.36\t-2.55\t2.02\t0.45\t-2.83\n",
        "8/1/24\t1.83\t-2.23\t-1.93\t1.25\t-0.09\t0.48\t1.35\n",
        "9/1/24\t1.51\t-0.07\t-0.72\t-0.46\t0.32\t0.40\t-0.64\n",
        "10/1/24\t-2.47\t-2.01\t1.29\t-1.44\t0.91\t0.39\t2.82\n",
        "11/1/24\t4.08\t-0.94\t0.61\t-2.47\t-0.54\t0.40\t2.40\n",
        "12/1/24\t-3.15\t-1.21\t-2.20\t2.07\t0.05\t0.37\t-0.56\n",
        "1/1/25\t3.22\t-1.40\t0.82\t-1.52\t-2.70\t0.37\t0.66\n",
        "2/1/25\t-1.22\t-1.38\t3.75\t0.79\t2.60\t0.33\t-1.59\n",
        "3/1/25\t-4.35\t2.43\t4.40\t0.39\t0.35\t0.34\t-1.57\"\"\"\n",
        "    ff_df = pd.read_csv(StringIO(ff_data_string), sep='\\t')\n",
        "\n",
        "    # Convert 'Date' to datetime, handle potential 2-digit year ambiguity\n",
        "    # Use infer_datetime_format=True for robustness if format isn't fixed\n",
        "    ff_df['Date'] = pd.to_datetime(ff_df['Date'], infer_datetime_format=True)\n",
        "\n",
        "    ff_df = ff_df.set_index('Date')\n",
        "    # Ensure index is timezone naive\n",
        "    if ff_df.index.tz is not None:\n",
        "        ff_df.index = ff_df.index.tz_convert(None)\n",
        "\n",
        "    ff_df = ff_df.loc[(ff_df.index >= start_date) & (ff_df.index <= end_date)]\n",
        "    ff_df.columns = ff_df.columns.str.replace('[ .]', '_', regex=True)\n",
        "    ff_factors = ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF', 'WML']\n",
        "    # Ensure columns exist before dividing\n",
        "    existing_ff_factors = [f for f in ff_factors if f in ff_df.columns]\n",
        "    if existing_ff_factors:\n",
        "        ff_df[existing_ff_factors] = ff_df[existing_ff_factors] / 100\n",
        "    else:\n",
        "        print(\"Warning: No Fama-French factor columns found in loaded data.\")\n",
        "\n",
        "    ff_df = ff_df.reset_index()\n",
        "    print(\"Successfully loaded and processed Fama-French data.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR: Error loading or processing Fama-French data: {e}. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Data Preprocessing and Merging\n",
        "\n",
        "# Use a left merge from stock_returns_df_filtered to ensure we only keep rows for tickers\n",
        "# and dates where we successfully got stock data and the ticker is in the final map.\n",
        "stock_returns_df_filtered = stock_returns_df[stock_returns_df['Ticker'].isin(ticker_esg_group_map_final.keys())].copy()\n",
        "esg_scores_df_filtered = esg_scores_df[esg_scores_df['Ticker'].isin(ticker_esg_group_map_final.keys())].copy()\n",
        "\n",
        "# Ensure Date columns are datetime and timezone-naive before merging\n",
        "stock_returns_df_filtered['Date'] = pd.to_datetime(stock_returns_df_filtered['Date']).dt.tz_localize(None)\n",
        "esg_scores_df_filtered['Date'] = pd.to_datetime(esg_scores_df_filtered['Date']).dt.tz_localize(None)\n",
        "ff_df['Date'] = pd.to_datetime(ff_df['Date']).dt.tz_localize(None)\n",
        "\n",
        "\n",
        "merged_df = pd.merge(stock_returns_df_filtered, esg_scores_df_filtered, on=['Ticker', 'Date'], how='left')\n",
        "\n",
        "# Merge with Fama-French factors (broadcast on Date)\n",
        "merged_df = pd.merge(merged_df, ff_df, on='Date', how='left')\n",
        "\n",
        "# Calculate Excess Return\n",
        "# Ensure 'RF' column exists after merging\n",
        "if 'RF' in merged_df.columns:\n",
        "    merged_df['Excess_Return'] = merged_df['Return'] - merged_df['RF']\n",
        "    # Drop the original 'Return' and 'RF' columns\n",
        "    merged_df = merged_df.drop(columns=['Return', 'RF'])\n",
        "else:\n",
        "    print(\"Warning: 'RF' column not found after merging. Cannot calculate Excess_Return.\")\n",
        "    # Cannot proceed with model if Excess_Return is the dependent variable\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Add ESG Risk Group column\n",
        "merged_df['ESG_Risk_Group'] = merged_df['Ticker'].map(ticker_esg_group_map_final)\n",
        "\n",
        "# Set MultiIndex (Ticker, Date) for panel analysis\n",
        "# Ensure 'Date' is still datetime and timezone-naive\n",
        "merged_df['Date'] = pd.to_datetime(merged_df['Date']).dt.tz_localize(None)\n",
        "merged_df = merged_df.set_index(['Ticker', 'Date']).sort_index()\n",
        "\n",
        "# Final check if any data remains after merging and filtering\n",
        "if merged_df.empty:\n",
        "    print(\"\\nFATAL ERROR: No combined data available for analysis after merging and filtering successful downloads. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nMerged DataFrame head:\")\n",
        "print(merged_df.head())\n",
        "print(\"\\nMerged DataFrame info:\")\n",
        "merged_df.info()\n",
        "print(\"\\nMissing values before MICE:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "\n",
        "# 5. MICE Imputation\n",
        "\n",
        "print(\"\\nPerforming MICE imputation...\")\n",
        "\n",
        "numerical_cols_to_impute = ['Excess_Return', 'Total_ESG_Risk_Score', 'Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'WML']\n",
        "# Filter to columns that actually exist in merged_df\n",
        "numerical_cols_to_impute = [col for col in numerical_cols_to_impute if col in merged_df.columns]\n",
        "\n",
        "if merged_df[numerical_cols_to_impute].isnull().sum().sum() > 0:\n",
        "    try:\n",
        "        imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "\n",
        "        index = merged_df.index\n",
        "        columns_to_impute = merged_df[numerical_cols_to_impute].columns\n",
        "        other_cols = merged_df.drop(columns=numerical_cols_to_impute).columns\n",
        "\n",
        "        # Perform imputation on the numerical subset\n",
        "        # IterativeImputer works on numpy arrays\n",
        "        imputed_data = imputer.fit_transform(merged_df[numerical_cols_to_impute].values)\n",
        "\n",
        "        imputed_df_numerical = pd.DataFrame(imputed_data, columns=columns_to_impute, index=index)\n",
        "\n",
        "        # Recombine with non-imputed columns (like ESG_Risk_Group)\n",
        "        df_imputed = imputed_df_numerical.join(merged_df[other_cols])\n",
        "\n",
        "        print(\"MICE imputation complete.\")\n",
        "        print(\"\\nMissing values after MICE:\")\n",
        "        print(df_imputed.isnull().sum())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during MICE imputation: {e}\")\n",
        "        print(\"Proceeding with analysis on data with original NaNs (might cause errors in regressions).\")\n",
        "        df_imputed = merged_df.copy()\n",
        "else:\n",
        "    print(\"No missing numerical values found in specified columns. Skipping MICE imputation.\")\n",
        "    df_imputed = merged_df.copy()\n",
        "\n",
        "# Ensure ESG_Risk_Group is still category, especially if it was dropped and re-added\n",
        "if 'ESG_Risk_Group' in df_imputed.columns:\n",
        "    df_imputed['ESG_Risk_Group'] = df_imputed['ESG_Risk_Group'].astype('category')\n",
        "\n",
        "\n",
        "# 6. Analysis by ESG Group\n",
        "\n",
        "esg_groups = ['Low', 'Medium', 'High']\n",
        "results = {}\n",
        "\n",
        "# Check which groups actually have tickers with data in the final filtered data\n",
        "# Use the index to get the available tickers from the imputed data\n",
        "available_tickers_in_imputed_data = df_imputed.index.get_level_values('Ticker').unique()\n",
        "available_groups = df_imputed['ESG_Risk_Group'].dropna().unique() # Get groups present in the imputed data\n",
        "\n",
        "print(f\"\\nTickers with data after MICE: {available_tickers_in_imputed_data.tolist()}\")\n",
        "print(f\"ESG Groups with data after MICE: {available_groups.tolist()}\")\n",
        "\n",
        "\n",
        "for group in esg_groups:\n",
        "    # Check if the group exists in the data *after* all filtering and MICE\n",
        "    if group not in available_groups:\n",
        "        print(f\"\\n--- Skipping {group} ESG Risk Group (No data available after filtering/imputation) ---\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Analyzing {group} ESG Risk Group ---\")\n",
        "\n",
        "    group_df = df_imputed[df_imputed['ESG_Risk_Group'] == group].copy()\n",
        "\n",
        "    # Ensure the DataFrame is a MultiIndex DataFrame suitable for PanelOLS\n",
        "    # This should already be the case from previous steps, but add a check\n",
        "    if not isinstance(group_df.index, pd.MultiIndex) or 'Ticker' not in group_df.index.names or 'Date' not in group_df.index.names:\n",
        "        print(f\"Error: Could not retain MultiIndex for {group} group. Skipping analysis for this group.\")\n",
        "        continue\n",
        "\n",
        "    # Define dependent and independent variables from the group_df\n",
        "    dependent_var_col = 'Excess_Return'\n",
        "    independent_vars_cols = ['Total_ESG_Risk_Score', 'Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'WML']\n",
        "    # Filter independent_vars_cols to only include columns that exist in the group_df\n",
        "    independent_vars_cols = [col for col in independent_vars_cols if col in group_df.columns]\n",
        "\n",
        "    if dependent_var_col not in group_df.columns:\n",
        "        print(f\"Error: Dependent variable '{dependent_var_col}' not found in {group} group data. Skipping.\")\n",
        "        continue\n",
        "    if not independent_vars_cols:\n",
        "        print(f\"Error: No independent variables found in {group} group data. Skipping.\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    # Prepare data for regression: select columns and handle NaNs again as MICE might not fill all (e.g., if a row was all NaN)\n",
        "    # Or if the independent variables list changed.\n",
        "    analysis_cols = [dependent_var_col] + independent_vars_cols\n",
        "    analysis_data_for_reg = group_df[analysis_cols].dropna(how='any') # Drop rows with any NaN in regression columns\n",
        "\n",
        "    if analysis_data_for_reg.empty:\n",
        "         print(f\"No complete data rows for regression analysis in {group} group after handling NaNs.\")\n",
        "         continue\n",
        "\n",
        "    dependent_var_clean = analysis_data_for_reg[dependent_var_col]\n",
        "    independent_vars_clean = analysis_data_for_reg[independent_vars_cols]\n",
        "\n",
        "    # Ensure the cleaned data retains the MultiIndex for PanelOLS\n",
        "    panel_data_clean = analysis_data_for_reg # It should already have the MultiIndex\n",
        "\n",
        "\n",
        "    # 7. VIF Analysis\n",
        "    print(f\"\\nCalculating VIF for {group} group:\")\n",
        "    # Check for sufficient data points relative to variables for VIF\n",
        "    # Need at least k+1 data points for OLS, k predictors + constant. VIF needs slightly more stability.\n",
        "    # A common heuristic is N > k+1, ideally N >> k.\n",
        "    if independent_vars_clean.shape[0] < independent_vars_clean.shape[1] + 2:\n",
        "         print(f\"Insufficient data points ({independent_vars_clean.shape[0]}) relative to predictors ({independent_vars_clean.shape[1]}) to reliably calculate VIF for {group} group.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Add a constant for VIF calculation\n",
        "            independent_vars_vif = sm.add_constant(independent_vars_clean, has_constant='add')\n",
        "            vif_data = pd.DataFrame()\n",
        "            vif_data['Variable'] = independent_vars_vif.columns\n",
        "            vif_data['VIF'] = [variance_inflation_factor(independent_vars_vif.values, i)\n",
        "                               for i in range(independent_vars_vif.shape[1])]\n",
        "            print(vif_data)\n",
        "            results[f'{group}_VIF'] = vif_data\n",
        "        except Exception as e:\n",
        "            print(f\"Could not calculate VIF for {group} group: {e}\")\n",
        "\n",
        "\n",
        "    # 8. Regression Analysis (Pooled OLS)\n",
        "    print(f\"\\nPerforming Pooled OLS Regression for {group} group:\")\n",
        "    # Check for sufficient data points for OLS\n",
        "    if dependent_var_clean.shape[0] <= independent_vars_clean.shape[1]: # Need at least n > k\n",
        "        print(f\"Insufficient data points ({dependent_var_clean.shape[0]}) for Pooled OLS regression for {group} group (Need > {independent_vars_clean.shape[1]} observations).\")\n",
        "    else:\n",
        "        try:\n",
        "            independent_vars_ols = sm.add_constant(independent_vars_clean, has_constant='add')\n",
        "            model_ols = sm.OLS(dependent_var_clean, independent_vars_ols)\n",
        "            results_ols = model_ols.fit()\n",
        "            print(results_ols.summary())\n",
        "            results[f'{group}_Pooled_OLS'] = results_ols\n",
        "        except Exception as e:\n",
        "            print(f\"Could not perform Pooled OLS for {group} group: {e}\")\n",
        "\n",
        "\n",
        "    # 9. Panel Data Regression\n",
        "    print(f\"\\nPerforming Panel Regression (Fixed Effects) for {group} group:\")\n",
        "    n_entities = panel_data_clean.index.get_level_values('Ticker').nunique()\n",
        "    n_time_periods_overall = panel_data_clean.index.get_level_values('Date').nunique()\n",
        "    n_observations = panel_data_clean.shape[0]\n",
        "    n_predictors = independent_vars_clean.shape[1] # Number of independent variables\n",
        "\n",
        "    # Check if there's at least one entity with more than one time period for PanelOLS\n",
        "    entity_counts = panel_data_clean.index.get_level_values('Ticker').value_counts()\n",
        "    has_multi_period_entity = (entity_counts > 1).any()\n",
        "\n",
        "\n",
        "    # PanelOLS requires at least 2 entities *with data* or at least one entity with >1 time period *with data*\n",
        "    # And generally N*T > k+1. The effective number of observations is n_observations.\n",
        "    if n_entities < 1:\n",
        "         print(f\"Insufficient entities (< 1) with data for Panel Regressions for {group} group.\")\n",
        "    elif n_observations <= n_predictors: # Check degrees of freedom\n",
        "         print(f\"Insufficient total observations ({n_observations}) relative to predictors ({n_predictors}) for Panel Regressions for {group} group.\")\n",
        "    elif n_entities < 2 and not has_multi_period_entity:\n",
        "         print(f\"Insufficient entities (< 2) AND no single entity with multi-period data for basic PanelOLS structure for {group} group (Entities: {n_entities}, Has Multi-period Entity: {has_multi_period_entity}).\")\n",
        "         # If there is only one entity *but* it has multiple periods, PanelOLS still works.\n",
        "         # If there are multiple entities *but* each has only one period, PanelOLS Fixed Effects won't work correctly (absorbs the single point). Random Effects might still be possible but unusual.\n",
        "         # Let's allow it if there's at least one multi-period entity or at least 2 entities.\n",
        "         if n_entities >= 2 or has_multi_period_entity:\n",
        "              pass # Continue to attempt PanelOLS\n",
        "         else:\n",
        "              print(f\"Skipping Panel regressions for {group} group due to insufficient panel structure.\")\n",
        "              continue # Skip panel regressions for this group\n",
        "    else: # Sufficient structure to attempt PanelOLS\n",
        "        try:\n",
        "            # Fixed Effects model\n",
        "            # PanelOLS adds the constant automatically when entity_effects=True\n",
        "            model_fe = PanelOLS(dependent_var_clean, independent_vars_clean, entity_effects=True)\n",
        "            results_fe = model_fe.fit()\n",
        "            print(\"Fixed Effects Results:\")\n",
        "            print(results_fe)\n",
        "            results[f'{group}_Fixed_Effects'] = results_fe\n",
        "        except Exception as e:\n",
        "            print(f\"Could not perform Fixed Effects regression for {group} group: {e}\")\n",
        "\n",
        "        # Only attempt Random Effects if Fixed Effects was attempted (implies sufficient basic structure)\n",
        "        # Random effects also requires at least 2 entities or one multi-period entity\n",
        "        if n_entities >= 2 or has_multi_period_entity:\n",
        "            print(f\"\\nPerforming Panel Regression (Random Effects) for {group} group:\")\n",
        "            try:\n",
        "                # Random Effects model\n",
        "                # PanelOLS adds the constant automatically when random_effects=True\n",
        "                model_re = PanelOLS(dependent_var_clean, independent_vars_clean, random_effects=True)\n",
        "                results_re = model_re.fit()\n",
        "                print(\"Random Effects Results:\")\n",
        "                print(results_re)\n",
        "                results[f'{group}_Random_Effects'] = results_re\n",
        "            except Exception as e:\n",
        "                print(f\"Could not perform Random Effects regression for {group} group: {e}\")\n",
        "        else:\n",
        "             print(f\"\\nSkipping Random Effects for {group} group (requires at least 2 entities or one multi-period entity with data).\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Analysis Complete ---\")\n",
        "print(f\"\\nAnalysis was attempted for groups: {list(esg_groups)}. Successful groups with data for analysis: {list(available_groups)}\")\n",
        "print(\"\\nResults stored in 'results' dictionary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XFEYTslmHDas",
        "outputId": "905910c3-7d81-4717-8dcf-7b0c79d933bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download data for tickers: ['AAPL', '009150.KS', 'SONY', '034220.KS', '601138.SS', 'ENR.DE', 'PHG', '002241.SZ', 'STM', 'GRMN', 'AMZN', '000100.SZ']\n",
            "\n",
            "Analysis period: 2019-11-01 to 2025-04-01\n",
            "\n",
            "Performing quick test download for first ticker...\n",
            "Quick test download successful for AAPL. Found 20 data points.\n",
            "------------------------------\n",
            "\n",
            "Downloading stock data for analysis period...\n",
            "Processing stock data for AAPL...\n",
            "Successfully processed monthly stock returns for AAPL\n",
            "Processing stock data for 009150.KS...\n",
            "Successfully processed monthly stock returns for 009150.KS\n",
            "Processing stock data for SONY...\n",
            "Successfully processed monthly stock returns for SONY\n",
            "Processing stock data for 034220.KS...\n",
            "Successfully processed monthly stock returns for 034220.KS\n",
            "Processing stock data for 601138.SS...\n",
            "Successfully processed monthly stock returns for 601138.SS\n",
            "Processing stock data for ENR.DE...\n",
            "Successfully processed monthly stock returns for ENR.DE\n",
            "Processing stock data for PHG...\n",
            "Successfully processed monthly stock returns for PHG\n",
            "Processing stock data for 002241.SZ...\n",
            "Successfully processed monthly stock returns for 002241.SZ\n",
            "Processing stock data for STM...\n",
            "Successfully processed monthly stock returns for STM\n",
            "Processing stock data for GRMN...\n",
            "Successfully processed monthly stock returns for GRMN\n",
            "Processing stock data for AMZN...\n",
            "Successfully processed monthly stock returns for AMZN\n",
            "Processing stock data for 000100.SZ...\n",
            "Successfully processed monthly stock returns for 000100.SZ\n",
            "\n",
            "Downloading ESG data...\n",
            "Processing ESG data for AAPL...\n",
            "Successfully processed ESG data for AAPL\n",
            "Processing ESG data for 009150.KS...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for 009150.KS from yesg. Excluding from analysis.\n",
            "Processing ESG data for SONY...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for SONY from yesg. Excluding from analysis.\n",
            "Processing ESG data for 034220.KS...\n",
            "Successfully processed ESG data for 034220.KS\n",
            "Processing ESG data for 601138.SS...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for 601138.SS from yesg. Excluding from analysis.\n",
            "Processing ESG data for ENR.DE...\n",
            "Successfully processed ESG data for ENR.DE\n",
            "Processing ESG data for PHG...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for PHG from yesg. Excluding from analysis.\n",
            "Processing ESG data for 002241.SZ...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for 002241.SZ from yesg. Excluding from analysis.\n",
            "Processing ESG data for STM...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for STM from yesg. Excluding from analysis.\n",
            "Processing ESG data for GRMN...\n",
            "Successfully processed ESG data for GRMN\n",
            "Processing ESG data for AMZN...\n",
            "Successfully processed ESG data for AMZN\n",
            "Processing ESG data for 000100.SZ...\n",
            "An error has occurred. The ticker symbol might be wrong or you might need to wait to continue.\n",
            "Warning: No ESG data found for 000100.SZ from yesg. Excluding from analysis.\n",
            "\n",
            "Loading Fama-French data...\n",
            "Successfully loaded and processed Fama-French data.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Return'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Return'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d3c78a5e45a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;31m# Ensure 'RF' column exists after merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'RF'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Excess_Return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;31m# Drop the original 'Return' and 'RF' columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Return'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GEM dataset Imputation and calibration"
      ],
      "metadata": {
        "id": "K8BzFTlrORhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "file_path = '/content/GEM2024_2014.csv'\n",
        "imputation_method = 'median' # or 'mean' or 'mode'\n",
        "output_file_path = '/content/GEM2024_2014_imputed.csv' # Define the output file path\n",
        "\n",
        "# List of columns that should NOT be imputed (identifiers)\n",
        "identifier_columns = ['country', 'year']\n",
        "\n",
        "# --- Load the dataset ---\n",
        "try:\n",
        "    df = pd.read_csv(file_path, na_values='')\n",
        "\n",
        "    print(f\"Successfully loaded data from {file_path}\")\n",
        "    print(\"\\nInitial data information:\")\n",
        "    df.info()\n",
        "\n",
        "    # --- Check for missing values before imputation ---\n",
        "    print(\"\\nMissing values per column before imputation:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # --- Identify columns for imputation ---\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    cols_to_impute = [col for col in numerical_cols if col not in identifier_columns]\n",
        "\n",
        "    print(f\"\\nColumns identified for {imputation_method} imputation: {cols_to_impute}\")\n",
        "\n",
        "    # --- Perform Imputation ---\n",
        "    print(f\"\\nPerforming {imputation_method} imputation...\")\n",
        "\n",
        "    for col in cols_to_impute:\n",
        "        if imputation_method == 'median':\n",
        "            fill_value = df[col].median()\n",
        "        elif imputation_method == 'mean':\n",
        "            fill_value = df[col].mean()\n",
        "        elif imputation_method == 'mode':\n",
        "            mode_values = df[col].mode()\n",
        "            if not mode_values.empty:\n",
        "                 fill_value = mode_values[0]\n",
        "            else:\n",
        "                 print(f\"Warning: No mode found for column {col}. Using median instead.\")\n",
        "                 fill_value = df[col].median() # Fallback if mode is problematic\n",
        "        else:\n",
        "            print(f\"Error: Unknown imputation method '{imputation_method}'.\")\n",
        "            # Decide how to handle this error - maybe skip imputation for this column?\n",
        "            # For simplicity, we'll just continue without filling this specific column\n",
        "            continue # Skip to the next column\n",
        "\n",
        "        # Fill missing values in the column\n",
        "        if pd.notna(fill_value):\n",
        "            df[col].fillna(fill_value, inplace=True)\n",
        "            # print(f\"Filled missing values in '{col}' with {fill_value:.2f}\") # Optional: detailed log\n",
        "\n",
        "    # --- Verify Imputation ---\n",
        "    print(\"\\nMissing values per column after imputation:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # --- Display results ---\n",
        "    print(\"\\nData head after imputation:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nData info after imputation:\")\n",
        "    df.info()\n",
        "\n",
        "    # --- Save the imputed data to a new CSV file ---\n",
        "    # These lines are now uncommented to save the file\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "    print(f\"\\nImputed data saved to {output_file_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please make sure you have saved the data into a CSV file with this name.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EINyif3OapW",
        "outputId": "c842fc56-fce3-4c83-ed95-5fefc6104ec1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from /content/GEM2024_2014.csv\n",
            "\n",
            "Initial data information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 582 entries, 0 to 581\n",
            "Data columns (total 17 columns):\n",
            " #   Column                                          Non-Null Count  Dtype  \n",
            "---  ------                                          --------------  -----  \n",
            " 0   country                                         582 non-null    object \n",
            " 1   year                                            582 non-null    int64  \n",
            " 2   Perceived_opportunities                         582 non-null    float64\n",
            " 3   Perceived_capabilities                          582 non-null    float64\n",
            " 4   Fear_of_failure_rate                            582 non-null    float64\n",
            " 5   Entrepreneurial_intentions                      582 non-null    float64\n",
            " 6   Total_early_stage_Entrepreneurial_Activity_TEA  582 non-null    float64\n",
            " 7   Established_Business_Ownership                  582 non-null    float64\n",
            " 8   Entrepreneurial_Employee_Activity               432 non-null    float64\n",
            " 9   Motivational_Index                              297 non-null    float64\n",
            " 10  Female_Male_TEA                                 582 non-null    float64\n",
            " 11  Female_Male_Opportunity_Driven_TEA              297 non-null    float64\n",
            " 12  High_Job_Creation_Expectation                   582 non-null    float64\n",
            " 13  Innovation                                      297 non-null    float64\n",
            " 14  Business_Services_Sector                        582 non-null    float64\n",
            " 15  High_Status_to_Successful_Entrepreneurs         530 non-null    float64\n",
            " 16  Entrepreneurship_Good_Career_Choice             533 non-null    float64\n",
            "dtypes: float64(15), int64(1), object(1)\n",
            "memory usage: 77.4+ KB\n",
            "\n",
            "Missing values per column before imputation:\n",
            "country                                             0\n",
            "year                                                0\n",
            "Perceived_opportunities                             0\n",
            "Perceived_capabilities                              0\n",
            "Fear_of_failure_rate                                0\n",
            "Entrepreneurial_intentions                          0\n",
            "Total_early_stage_Entrepreneurial_Activity_TEA      0\n",
            "Established_Business_Ownership                      0\n",
            "Entrepreneurial_Employee_Activity                 150\n",
            "Motivational_Index                                285\n",
            "Female_Male_TEA                                     0\n",
            "Female_Male_Opportunity_Driven_TEA                285\n",
            "High_Job_Creation_Expectation                       0\n",
            "Innovation                                        285\n",
            "Business_Services_Sector                            0\n",
            "High_Status_to_Successful_Entrepreneurs            52\n",
            "Entrepreneurship_Good_Career_Choice                49\n",
            "dtype: int64\n",
            "\n",
            "Columns identified for median imputation: ['Perceived_opportunities', 'Perceived_capabilities', 'Fear_of_failure_rate', 'Entrepreneurial_intentions', 'Total_early_stage_Entrepreneurial_Activity_TEA', 'Established_Business_Ownership', 'Entrepreneurial_Employee_Activity', 'Motivational_Index', 'Female_Male_TEA', 'Female_Male_Opportunity_Driven_TEA', 'High_Job_Creation_Expectation', 'Innovation', 'Business_Services_Sector', 'High_Status_to_Successful_Entrepreneurs', 'Entrepreneurship_Good_Career_Choice']\n",
            "\n",
            "Performing median imputation...\n",
            "\n",
            "Missing values per column after imputation:\n",
            "country                                           0\n",
            "year                                              0\n",
            "Perceived_opportunities                           0\n",
            "Perceived_capabilities                            0\n",
            "Fear_of_failure_rate                              0\n",
            "Entrepreneurial_intentions                        0\n",
            "Total_early_stage_Entrepreneurial_Activity_TEA    0\n",
            "Established_Business_Ownership                    0\n",
            "Entrepreneurial_Employee_Activity                 0\n",
            "Motivational_Index                                0\n",
            "Female_Male_TEA                                   0\n",
            "Female_Male_Opportunity_Driven_TEA                0\n",
            "High_Job_Creation_Expectation                     0\n",
            "Innovation                                        0\n",
            "Business_Services_Sector                          0\n",
            "High_Status_to_Successful_Entrepreneurs           0\n",
            "Entrepreneurship_Good_Career_Choice               0\n",
            "dtype: int64\n",
            "\n",
            "Data head after imputation:\n",
            "                  country  year  Perceived_opportunities  \\\n",
            "0               Argentina  2024                    57.00   \n",
            "1                 Armenia  2024                    51.39   \n",
            "2                 Austria  2024                    45.96   \n",
            "3                 Belarus  2024                    46.95   \n",
            "4  Bosnia and Herzegovina  2024                    55.09   \n",
            "\n",
            "   Perceived_capabilities  Fear_of_failure_rate  Entrepreneurial_intentions  \\\n",
            "0                   74.80                 18.78                       17.40   \n",
            "1                   59.63                 38.42                       36.23   \n",
            "2                   55.38                 44.76                        5.42   \n",
            "3                   52.11                 48.17                       32.40   \n",
            "4                   73.34                 46.79                       29.87   \n",
            "\n",
            "   Total_early_stage_Entrepreneurial_Activity_TEA  \\\n",
            "0                                           23.36   \n",
            "1                                           17.59   \n",
            "2                                            6.58   \n",
            "3                                           16.59   \n",
            "4                                           22.71   \n",
            "\n",
            "   Established_Business_Ownership  Entrepreneurial_Employee_Activity  \\\n",
            "0                            6.42                               2.62   \n",
            "1                           10.78                               2.62   \n",
            "2                            7.88                               2.62   \n",
            "3                            5.15                               2.62   \n",
            "4                            5.86                               2.62   \n",
            "\n",
            "   Motivational_Index  Female_Male_TEA  Female_Male_Opportunity_Driven_TEA  \\\n",
            "0                2.14             0.93                                0.94   \n",
            "1                2.14             0.59                                0.94   \n",
            "2                2.14             0.93                                0.94   \n",
            "3                2.14             0.94                                0.94   \n",
            "4                2.14             0.77                                0.94   \n",
            "\n",
            "   High_Job_Creation_Expectation  Innovation  Business_Services_Sector  \\\n",
            "0                          14.78       25.54                     15.93   \n",
            "1                          27.67       25.54                     11.60   \n",
            "2                           8.26       25.54                     31.28   \n",
            "3                          29.56       25.54                     18.76   \n",
            "4                          26.25       25.54                     22.05   \n",
            "\n",
            "   High_Status_to_Successful_Entrepreneurs  \\\n",
            "0                                    72.19   \n",
            "1                                    73.97   \n",
            "2                                    76.18   \n",
            "3                                    78.36   \n",
            "4                                    75.79   \n",
            "\n",
            "   Entrepreneurship_Good_Career_Choice  \n",
            "0                                65.60  \n",
            "1                                87.56  \n",
            "2                                51.10  \n",
            "3                                78.69  \n",
            "4                                65.76  \n",
            "\n",
            "Data info after imputation:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 582 entries, 0 to 581\n",
            "Data columns (total 17 columns):\n",
            " #   Column                                          Non-Null Count  Dtype  \n",
            "---  ------                                          --------------  -----  \n",
            " 0   country                                         582 non-null    object \n",
            " 1   year                                            582 non-null    int64  \n",
            " 2   Perceived_opportunities                         582 non-null    float64\n",
            " 3   Perceived_capabilities                          582 non-null    float64\n",
            " 4   Fear_of_failure_rate                            582 non-null    float64\n",
            " 5   Entrepreneurial_intentions                      582 non-null    float64\n",
            " 6   Total_early_stage_Entrepreneurial_Activity_TEA  582 non-null    float64\n",
            " 7   Established_Business_Ownership                  582 non-null    float64\n",
            " 8   Entrepreneurial_Employee_Activity               582 non-null    float64\n",
            " 9   Motivational_Index                              582 non-null    float64\n",
            " 10  Female_Male_TEA                                 582 non-null    float64\n",
            " 11  Female_Male_Opportunity_Driven_TEA              582 non-null    float64\n",
            " 12  High_Job_Creation_Expectation                   582 non-null    float64\n",
            " 13  Innovation                                      582 non-null    float64\n",
            " 14  Business_Services_Sector                        582 non-null    float64\n",
            " 15  High_Status_to_Successful_Entrepreneurs         582 non-null    float64\n",
            " 16  Entrepreneurship_Good_Career_Choice             582 non-null    float64\n",
            "dtypes: float64(15), int64(1), object(1)\n",
            "memory usage: 77.4+ KB\n",
            "\n",
            "Imputed data saved to /content/GEM2024_2014_imputed.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b6303277b039>:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(fill_value, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl\n",
        "!pip install statsmodels\n",
        "!pip install linearmodels\n",
        "\n",
        "#python calibrate_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYnhbRITYDFg",
        "outputId": "7ed2ea39-5989-466f-fcae-7d8b28fcbf4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Collecting linearmodels\n",
            "  Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.15.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.14.4)\n",
            "Collecting mypy-extensions>=0.4 (from linearmodels)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Collecting pyhdfe>=0.1 (from linearmodels)\n",
            "  Downloading pyhdfe-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting formulaic>=1.0.0 (from linearmodels)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting setuptools-scm<9.0.0,>=8.0.0 (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=1.0.0->linearmodels)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n",
            "Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyhdfe-0.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: setuptools-scm, mypy-extensions, interface-meta, pyhdfe, formulaic, linearmodels\n",
            "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 linearmodels-6.1 mypy-extensions-1.1.0 pyhdfe-0.2.0 setuptools-scm-8.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibrate GEM dataset\n",
        "\n",
        "#### *Explanation:\n",
        "Import Libraries: Imports pandas for data manipulation and numpy for detecting numeric columns.\n",
        "#### Configuration: Sets the paths for input/output files and the percentiles for calibration (5, 50, 95).\n",
        "#### calibrate_percentile Function: This function takes a single data value and the calculated 5th, 50th, and 95th percentiles for its variable. It implements the piecewise linear calibration logic:\n",
        "#### Values at or below the 5th percentile get a score of 0.05.\n",
        "#### Values at or above the 95th percentile get a score of 0.95.\n",
        "#### Values between the 5th and 50th percentiles are linearly interpolated between 0.05 and 0.5.\n",
        "#### Values between the 50th and 95th percentiles are linearly interpolated between 0.5 and 0.95.\n",
        "#### Includes basic checks to prevent division by zero if percentiles happen to be equal (which can occur with variables having very limited unique values).\n",
        "#### Load Data: Reads your specified CSV file into a pandas DataFrame. Includes error handling for FileNotFoundError.\n",
        "#### Identify Columns: Separates the columns into identifiers, those to be excluded (like the constant one), and the remaining numeric columns that will be calibrated.\n",
        "#### Perform Calibration:\n",
        "#### Creates a new DataFrame calibrated_df starting with just the identifier columns (country, year).\n",
        "#### Loops through each col in cols_to_calibrate.\n",
        "#### Calculates the 5th, 50th, and 95th percentiles (p5, p50, p95) for the entire column using df[col].quantile().\n",
        "#### Applies the calibrate_percentile function to each value in the current column df[col].\n",
        "#### Stores the resulting fuzzy scores in a new column in calibrated_df, prefixed with f_.\n",
        "#### Save Data:\n",
        "#### Saves the calibrated_df to a new CSV file using to_csv(). index=False prevents writing the DataFrame index as a column.\n",
        "#### Saves the calibrated_df to a new XLSX file using to_excel(). This requires the openpyxl library. Error handling is included to remind you to install it if needed.\n",
        "#### To use this code:\n",
        "#### Save the code: Copy the code block and save it as a Python file (e.g., calibrate_data.py).\n",
        "#### Ensure the input file: Make sure your imputed data file (GEM2024_2014_imputed.csv) is in the location specified by input_csv_path. If you run this code in a Colab or Jupyter environment and uploaded the file, /content/GEM2024_2014_imputed.csv might be correct. If running locally, update the path.\n",
        "#### Install pandas and openpyxl: If you don't have them installed, open your terminal or command prompt and run:\n",
        "#### pip install pandas openpyxl\n",
        "#### Run the script: Open your terminal or command prompt, navigate to the directory where you saved the Python file, and run:\n",
        "#### python calibrate_data.py"
      ],
      "metadata": {
        "id": "b2cZdnvbYYor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "input_csv_path = '/content/GEM2024_2014_imputed.csv'\n",
        "output_csv_path = 'calibrated_data.csv'\n",
        "output_excel_path = 'calibrated_data.xlsx'\n",
        "percentiles = [5, 50, 95] # Percentiles for calibration anchors\n",
        "\n",
        "# Define the calibration function (piecewise linear based on percentiles)\n",
        "def calibrate_percentile(value, p5, p50, p95):\n",
        "    \"\"\"Calibrates a single value based on 5th, 50th, and 95th percentiles.\"\"\"\n",
        "    # Handle edge cases where percentiles might be equal (e.g., low variation)\n",
        "    if p5 == p95:\n",
        "        # Variable is constant, return 0.5 (or could exclude, but calibrating to 0.5 is also an option)\n",
        "        # Based on previous output, we will exclude this variable entirely later.\n",
        "        return 0.5 # This case shouldn't be reached if constant variables are excluded from cols_to_calibrate\n",
        "\n",
        "    if value <= p5:\n",
        "        return 0.05\n",
        "    elif value >= p95:\n",
        "        return 0.95\n",
        "    elif value < p50:\n",
        "        # Linear interpolation between p5 (0.05) and p50 (0.5)\n",
        "        if p50 == p5: # Handle division by zero if p50 equals p5\n",
        "            return 0.05 # If value is less than p50 (which equals p5), it must be <= p5\n",
        "        return 0.05 + (value - p5) / (p50 - p5) * (0.5 - 0.05)\n",
        "    else: # value >= p50\n",
        "        # Linear interpolation between p50 (0.5) and p95 (0.95)\n",
        "        if p95 == p50: # Handle division by zero if p95 equals p50\n",
        "             return 0.95 # If value is >= p50 (which equals p95), it must be >= p95\n",
        "        return 0.5 + (value - p50) / (p95 - p50) * (0.95 - 0.5)\n",
        "\n",
        "# --- Load Data ---\n",
        "try:\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    print(f\"Successfully loaded data from {input_csv_path}. Shape: {df.shape}\")\n",
        "    print(\"Original columns:\", df.columns.tolist())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {input_csv_path}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Identify Columns to Calibrate ---\n",
        "# Exclude non-numeric and obviously constant columns\n",
        "identifier_cols = ['country', 'year']\n",
        "constant_or_excluded_cols = ['Female_Male_Opportunity_Driven_TEA'] # Identified as constant\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Variables to calibrate are numeric columns minus identifiers and excluded ones\n",
        "cols_to_calibrate = [col for col in numeric_cols if col not in identifier_cols and col not in constant_or_excluded_cols]\n",
        "\n",
        "print(\"\\nColumns to calibrate:\", cols_to_calibrate)\n",
        "print(\"Columns to keep as is (identifiers):\", identifier_cols)\n",
        "print(\"Columns excluded:\", constant_or_excluded_cols)\n",
        "\n",
        "\n",
        "# --- Perform Calibration ---\n",
        "calibrated_df = df[identifier_cols].copy() # Start with identifier columns\n",
        "\n",
        "for col in cols_to_calibrate:\n",
        "    if col in df.columns:\n",
        "        # Calculate percentiles across the entire dataset for this variable\n",
        "        p5 = df[col].quantile(percentiles[0] / 100)\n",
        "        p50 = df[col].quantile(percentiles[1] / 100)\n",
        "        p95 = df[col].quantile(percentiles[2] / 100)\n",
        "\n",
        "        # Print percentiles for transparency (optional)\n",
        "        # print(f\"Calibrating '{col}': 5th={p5:.2f}, 50th={p50:.2f}, 95th={p95:.2f}\")\n",
        "\n",
        "        # Apply the calibration function to the column\n",
        "        # Create a new column with 'f_' prefix for fuzzy scores\n",
        "        calibrated_col_name = f'f_{col}'\n",
        "        calibrated_df[calibrated_col_name] = df[col].apply(\n",
        "            lambda x: calibrate_percentile(x, p5, p50, p95)\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found in the dataset. Skipping calibration.\")\n",
        "\n",
        "# --- Save Calibrated Data ---\n",
        "\n",
        "# 1) Save to CSV\n",
        "try:\n",
        "    calibrated_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"\\nCalibrated data saved successfully to CSV: {output_csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to CSV: {e}\")\n",
        "\n",
        "\n",
        "# 2) Save to XLSX\n",
        "try:\n",
        "    # You might need to install openpyxl: pip install openpyxl\n",
        "    calibrated_df.to_excel(output_excel_path, index=False)\n",
        "    print(f\"Calibrated data saved successfully to XLSX: {output_excel_path}\")\n",
        "except ImportError:\n",
        "    print(\"\\nError: 'openpyxl' library not found.\")\n",
        "    print(\"Please install it to save to Excel (.xlsx) format: pip install openpyxl\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to XLSX: {e}\")\n",
        "\n",
        "print(\"\\nCalibration process finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "7BICW3YhYujf",
        "outputId": "560da496-0b9c-4273-a4e5-f9411c35be5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at /content/GEM2024_2014_imputed.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e37d1242eb42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mconstant_or_excluded_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Female_Male_Opportunity_Driven_TEA'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Identified as constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mnumeric_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Variables to calibrate are numeric columns minus identifiers and excluded ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyfca\n",
        "!pip install openpyxl\n",
        "!pip install linearmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnYlxWIlui5",
        "outputId": "cfad84f4-e2b7-4b6c-ac59-a7018ea87fa8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyfca\n",
            "  Downloading pyfca-0.3.3-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting svgwrite (from pyfca)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading pyfca-0.3.3-py3-none-any.whl (13 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, pyfca\n",
            "Successfully installed pyfca-0.3.3 svgwrite-1.4.3\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting linearmodels\n",
            "  Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.15.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.14.4)\n",
            "Collecting mypy-extensions>=0.4 (from linearmodels)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Collecting pyhdfe>=0.1 (from linearmodels)\n",
            "  Downloading pyhdfe-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting formulaic>=1.0.0 (from linearmodels)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting setuptools-scm<9.0.0,>=8.0.0 (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=1.0.0->linearmodels)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n",
            "Downloading linearmodels-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyhdfe-0.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: setuptools-scm, mypy-extensions, interface-meta, pyhdfe, formulaic, linearmodels\n",
            "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 linearmodels-6.1 mypy-extensions-1.1.0 pyhdfe-0.2.0 setuptools-scm-8.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "input_csv_file = '/content/PLS_SEM_SE_Pre.csv'\n",
        "output_excel_file = 'PLS_SEM_Calibrated_Output.xlsx'\n",
        "\n",
        "# List of columns to calibrate\n",
        "columns_to_calibrate = [\n",
        "    'Perceived_opportunities',\n",
        "    'Perceived_capabilities',\n",
        "    'Fear_failure_rate',\n",
        "    'Entrepreneurial_intentions',\n",
        "    'Entrepreneurial_TEA',\n",
        "    'Established_Business_Ownership',\n",
        "    'Entrepreneurial_Employee_Activity',\n",
        "    'Motivational_Index',\n",
        "    'Female_Male_TEA',\n",
        "    'Female_Male_Opportunity_Driven_TEA',\n",
        "    'High_Job_Creation_Expectation',\n",
        "    'Innovation',\n",
        "    'Business_Services_Sector',\n",
        "    'High_Status_Successful_Entrepreneurs',\n",
        "    'Entrepreneurship_Good_Career_Choice'\n",
        "]\n",
        "\n",
        "# --- Manual Calibration Function (Min=0, Mean=0.5, Max=1 Membership) ---\n",
        "def calibrate_min_mean_max(series):\n",
        "    \"\"\"\n",
        "    Calibrates a pandas Series to a fuzzy set using min, mean, and max\n",
        "    as anchor points for 0, 0.5, and 1 membership respectively.\n",
        "    Handles edge cases where min, mean, or max are identical.\n",
        "    \"\"\"\n",
        "    # Handle potential NaN values\n",
        "    series_clean = series.dropna()\n",
        "\n",
        "    if series_clean.empty:\n",
        "        return pd.Series(np.nan, index=series.index) # Return NaNs if no data\n",
        "\n",
        "    min_val = series_clean.min()\n",
        "    mean_val = series_clean.mean()\n",
        "    max_val = series_clean.max()\n",
        "\n",
        "    # Initialize calibrated series with NaNs to preserve original index and NaNs\n",
        "    calibrated_series = pd.Series(np.nan, index=series.index)\n",
        "\n",
        "    # Handle edge case where all values are the same\n",
        "    if min_val == max_val:\n",
        "        # Membership is 0.5 for all non-NaN values if min==mean==max\n",
        "        calibrated_series[series.notna()] = 0.5\n",
        "        return calibrated_series\n",
        "\n",
        "    # Handle cases where min, mean, or max overlap\n",
        "    # Use slightly adjusted points if mean coincides with min or max, but not all three\n",
        "    c1, c2, c3 = min_val, mean_val, max_val\n",
        "\n",
        "    # Apply calibration logic element-wise, handling NaNs implicitly\n",
        "    # We'll use boolean indexing and vectorized operations for efficiency\n",
        "\n",
        "    # Values less than or equal to c1 (min) get membership 0\n",
        "    calibrated_series[series <= c1] = 0.0\n",
        "\n",
        "    # Values greater than or equal to c3 (max) get membership 1\n",
        "    calibrated_series[series >= c3] = 1.0\n",
        "\n",
        "    # Values between c1 and c3 require interpolation\n",
        "    mask_between = (series > c1) & (series < c3)\n",
        "\n",
        "    # Linear interpolation between c1 (0 membership) and c2 (0.5 membership)\n",
        "    if c1 < c2: # Avoid division by zero if c1 == c2\n",
        "        mask_lower_half = (series > c1) & (series <= c2)\n",
        "        calibrated_series[mask_lower_half] = 0.5 * (series[mask_lower_half] - c1) / (c2 - c1)\n",
        "    elif c1 == c2 and c2 < c3: # Case like 0, 0, 100\n",
        "        # If values are > c1 (which is also c2), they are in the upper half\n",
        "        mask_between &= (series > c1) # All values > c1 go into upper half logic below\n",
        "\n",
        "    # Linear interpolation between c2 (0.5 membership) and c3 (1 membership)\n",
        "    if c2 < c3: # Avoid division by zero if c2 == c3\n",
        "         mask_upper_half = (series > c2) & (series < c3) # Only strictly > c2\n",
        "         calibrated_series[mask_upper_half] = 0.5 + 0.5 * (series[mask_upper_half] - c2) / (c3 - c2)\n",
        "    elif c1 < c2 and c2 == c3: # Case like 0, 100, 100\n",
        "        # If values are < c2 (which is also c3), they are in the lower half\n",
        "        mask_between &= (series < c2) # All values < c2 go into lower half logic above\n",
        "\n",
        "\n",
        "    # Handle the point exactly at c2 if it wasn't min or max\n",
        "    if c1 < c2 < c3:\n",
        "        calibrated_series[series == c2] = 0.5\n",
        "\n",
        "\n",
        "    # Note: Cases exactly at c1 or c3 are handled by the >= c3 and <= c1 conditions.\n",
        "    # If c1=c2<c3 or c1<c2=c3, the interpolation logic covers the transitions correctly\n",
        "    # for values not equal to the problematic point. Values exactly at the problematic point\n",
        "    # (like c1 if c1=c2) might need explicit handling if the previous logic missed them.\n",
        "    # Let's ensure points at the boundaries are covered explicitly if needed, though\n",
        "    # the >= and <= masks should handle c1 and c3 correctly.\n",
        "\n",
        "    # The logic `series <= c1` and `series >= c3` correctly assigns 0 and 1\n",
        "    # The intermediate values are handled by the linear interpolation.\n",
        "    # The case c1=c2=c3 returns 0.5 for non-NaNs.\n",
        "    # If c1=c2 < c3, values > c1 interpolate from 0 to 1 between c1 and c3. Our code uses 0 to 0.5 c1-c2 and 0.5 to 1 c2-c3. If c1=c2, (series-c1)/(c2-c1) is division by zero. This needs fixing.\n",
        "\n",
        "    # Let's refine the interpolation logic for edge cases where endpoints coincide\n",
        "    mask = series.notna() # Only process non-NaN values\n",
        "    values = series[mask]\n",
        "    calibrated_values = pd.Series(np.nan, index=values.index) # To store results\n",
        "\n",
        "    if min_val == max_val:\n",
        "        calibrated_values = 0.5\n",
        "    elif min_val == mean_val: # Case like 0, 0, 100 -> linear from 0 (at min) to 1 (at max)\n",
        "        calibrated_values[values <= min_val] = 0.0\n",
        "        calibrated_values[values > min_val] = (values[values > min_val] - min_val) / (max_val - min_val)\n",
        "    elif mean_val == max_val: # Case like 0, 100, 100 -> linear from 0 (at min) to 1 (at max)\n",
        "        calibrated_values[values < max_val] = (values[values < max_val] - min_val) / (max_val - min_val)\n",
        "        calibrated_values[values >= max_val] = 1.0\n",
        "    else: # Standard case: min < mean < max\n",
        "        calibrated_values[values <= min_val] = 0.0\n",
        "        calibrated_values[(values > min_val) & (values < mean_val)] = 0.5 * (values[(values > min_val) & (values < mean_val)] - min_val) / (mean_val - min_val)\n",
        "        calibrated_values[values == mean_val] = 0.5\n",
        "        calibrated_values[(values > mean_val) & (values < max_val)] = 0.5 + 0.5 * (values[(values > mean_val) & (values < max_val)] - mean_val) / (max_val - mean_val)\n",
        "        calibrated_values[values >= max_val] = 1.0\n",
        "\n",
        "    calibrated_series[mask] = calibrated_values # Assign back to the original series structure\n",
        "    return calibrated_series\n",
        "\n",
        "\n",
        "# --- Load Data ---\n",
        "try:\n",
        "    # Use keep_default_na=True and na_values='' explicitly if needed, pandas usually handles common NaNs\n",
        "    df = pd.read_csv(input_csv_file)\n",
        "    print(f\"Successfully loaded data from '{input_csv_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{input_csv_file}' not found.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calibration Process ---\n",
        "print(\"Starting calibration...\")\n",
        "\n",
        "for col_name in columns_to_calibrate:\n",
        "    if col_name in df.columns:\n",
        "        # Ensure the column is numeric. Attempt conversion if necessary.\n",
        "        try:\n",
        "            # Attempt to convert to numeric, coercing errors to NaN\n",
        "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
        "        except Exception as e:\n",
        "            print(f\" - Warning: Could not convert column '{col_name}' to numeric: {e}. Skipping calibration.\")\n",
        "            df[f'{col_name}_calibrated'] = np.nan\n",
        "            continue # Skip to the next column\n",
        "\n",
        "        # Perform calibration using the custom function\n",
        "        calibrated_data = calibrate_min_mean_max(df[col_name])\n",
        "\n",
        "        # Store the calibrated data in a new column\n",
        "        df[f'{col_name}_calibrated'] = calibrated_data\n",
        "\n",
        "        # Provide feedback on calibration points used\n",
        "        col_data_clean = df[col_name].dropna()\n",
        "        if not col_data_clean.empty:\n",
        "             min_val = col_data_clean.min()\n",
        "             mean_val = col_data_clean.mean()\n",
        "             max_val = col_data_clean.max()\n",
        "             print(f\" - Calibrated '{col_name}' (Min={min_val:.2f}, Mean={mean_val:.2f}, Max={max_val:.2f}).\")\n",
        "        else:\n",
        "             print(f\" - Calibrated '{col_name}' (No valid data for points).\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(f\" - Warning: Column '{col_name}' not found in the dataset. Skipping.\")\n",
        "        df[f'{col_name}_calibrated'] = np.nan # Add NaN column if skipped\n",
        "\n",
        "\n",
        "print(\"Calibration complete.\")\n",
        "\n",
        "# --- Save Calibrated Data to Excel ---\n",
        "try:\n",
        "    # Use ExcelWriter to specify the engine for potentially larger files\n",
        "    with pd.ExcelWriter(output_excel_file, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    print(f\"Successfully saved calibrated data to '{output_excel_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to Excel: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EJcY9A6lg49",
        "outputId": "b169c44f-d02c-42b4-c485-e4f6449e1cb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from '/content/PLS_SEM_SE_Pre.csv'.\n",
            "Starting calibration...\n",
            " - Calibrated 'Perceived_opportunities' (Min=7.27, Mean=48.64, Max=95.38).\n",
            " - Calibrated 'Perceived_capabilities' (Min=10.05, Mean=54.67, Max=92.63).\n",
            " - Calibrated 'Fear_failure_rate' (Min=7.14, Mean=39.55, Max=75.42).\n",
            " - Calibrated 'Entrepreneurial_intentions' (Min=2.12, Mean=22.81, Max=83.00).\n",
            " - Calibrated 'Entrepreneurial_TEA' (Min=1.56, Mean=13.12, Max=49.60).\n",
            " - Calibrated 'Established_Business_Ownership' (Min=1.25, Mean=7.86, Max=35.94).\n",
            " - Calibrated 'Entrepreneurial_Employee_Activity' (Min=0.00, Mean=2.52, Max=11.47).\n",
            " - Calibrated 'Motivational_Index' (Min=0.00, Mean=1.43, Max=19.50).\n",
            " - Calibrated 'Female_Male_TEA' (Min=0.25, Mean=0.72, Max=1.34).\n",
            " - Calibrated 'Female_Male_Opportunity_Driven_TEA' (Min=0.00, Mean=0.48, Max=1.18).\n",
            " - Calibrated 'High_Job_Creation_Expectation' (Min=0.50, Mean=22.40, Max=76.74).\n",
            " - Calibrated 'Innovation' (Min=0.00, Mean=13.34, Max=58.70).\n",
            " - Calibrated 'Business_Services_Sector' (Min=0.30, Mean=18.58, Max=46.84).\n",
            " - Calibrated 'High_Status_Successful_Entrepreneurs' (Min=0.00, Mean=70.45, Max=96.73).\n",
            " - Calibrated 'Entrepreneurship_Good_Career_Choice' (Min=0.00, Mean=63.75, Max=96.55).\n",
            "Calibration complete.\n",
            "Successfully saved calibrated data to 'PLS_SEM_Calibrated_Output.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "input_csv_file = '/Users/henryefeonomakpo/Downloads/1_SE_Prep/PLS_SEM_Calibrated_Output.csv' # Use the CSV with the calibrated columns from your PLS-SEM runs\n",
        "output_excel_file = 'FSQCA_Calibrated_Data.xlsx'\n",
        "\n",
        "# List of columns to calibrate for fsQCA.\n",
        "# Use the *original* raw data columns here, NOT the _calibrated columns from PLS-SEM.\n",
        "# The calibration points (min, mean, max) will be calculated from these original columns.\n",
        "columns_to_calibrate = [\n",
        "    'Perceived_opportunities',\n",
        "    'Perceived_capabilities',\n",
        "    'Fear_failure_rate',\n",
        "    'Entrepreneurial_intentions',\n",
        "    'Entrepreneurial_TEA',\n",
        "    'Established_Business_Ownership',\n",
        "    'Entrepreneurial_Employee_Activity',\n",
        "    'Motivational_Index',\n",
        "    'Female_Male_TEA',\n",
        "    'Female_Male_Opportunity_Driven_TEA',\n",
        "    'High_Job_Creation_Expectation',\n",
        "    'Innovation',\n",
        "    'Business_Services_Sector',\n",
        "    'High_Status_Successful_Entrepreneurs',\n",
        "    'Entrepreneurship_Good_Career_Choice'\n",
        "    # Include any other relevant numerical columns from your dataset\n",
        "]\n",
        "\n",
        "# --- Manual Calibration Function (Min=0, Mean=0.5, Max=1 Membership) ---\n",
        "# This function correctly implements the 3-anchor calibration with interpolation\n",
        "def calibrate_min_mean_max(series, col_name):\n",
        "    \"\"\"\n",
        "    Calibrates a pandas Series to a fuzzy set using min, mean, and max\n",
        "    as anchor points for 0, 0.5, and 1 membership respectively.\n",
        "    Uses piecewise linear interpolation and handles edge cases.\n",
        "    Prints calibration points used.\n",
        "    \"\"\"\n",
        "    # Handle potential NaN values\n",
        "    series_clean = series.dropna()\n",
        "\n",
        "    if series_clean.empty:\n",
        "        print(f\"    No valid data found for calibration of '{col_name}'.\")\n",
        "        return pd.Series(np.nan, index=series.index) # Return NaNs if no data\n",
        "\n",
        "    min_val = series_clean.min()\n",
        "    mean_val = series_clean.mean()\n",
        "    max_val = series_clean.max()\n",
        "\n",
        "    # Print calibration points for the user\n",
        "    print(f\"    Calibrating '{col_name}' with points (0, 0.5, 1): ({min_val:.2f}, {mean_val:.2f}, {max_val:.2f})\")\n",
        "\n",
        "    # Initialize calibrated series with NaNs to preserve original index and NaNs\n",
        "    calibrated_series = pd.Series(np.nan, index=series.index)\n",
        "    mask = pd.notna(series) # Mask for non-NaN original values\n",
        "    values = series[mask] # Non-NaN values\n",
        "\n",
        "    # Handle edge case: all values identical\n",
        "    if min_val == max_val:\n",
        "        calibrated_values = np.full(values.shape, 0.5) # Membership is 0.5 for all non-NaNs\n",
        "    # Handle edge case: min == mean but < max\n",
        "    elif min_val == mean_val and mean_val < max_val:\n",
        "        # Linear interpolation from 0 (at min/mean) to 1 (at max)\n",
        "        calibrated_values = (values - min_val) / (max_val - min_val)\n",
        "    # Handle edge case: mean == max but > min\n",
        "    elif min_val < mean_val and mean_val == max_val:\n",
        "        # Linear interpolation from 0 (at min) to 1 (at mean/max)\n",
        "        calibrated_values = (values - min_val) / (mean_val - min_val)\n",
        "    # Standard case: min < mean < max (piecewise linear)\n",
        "    else:\n",
        "        calibrated_values = np.empty(values.shape)\n",
        "        # Lower half (0 to 0.5 between min and mean)\n",
        "        mask_lower_half = values <= mean_val\n",
        "        calibrated_values[mask_lower_half] = 0.5 * (values[mask_lower_half] - min_val) / (mean_val - min_val)\n",
        "        # Upper half (0.5 to 1 between mean and max)\n",
        "        mask_upper_half = values > mean_val\n",
        "        calibrated_values[mask_upper_half] = 0.5 + 0.5 * (values[mask_upper_half] - mean_val) / (max_val - mean_val)\n",
        "\n",
        "    # Ensure memberships are strictly between 0 and 1 (numerical precision)\n",
        "    calibrated_values = np.clip(calibrated_values, 0.0, 1.0)\n",
        "\n",
        "    # Assign calibrated values back, preserving original NaNs\n",
        "    calibrated_series[mask] = calibrated_values\n",
        "\n",
        "    return calibrated_series\n",
        "\n",
        "\n",
        "# --- Load Data ---\n",
        "try:\n",
        "    # Use keep_default_na=True and na_values='' explicitly if needed, pandas usually handles common NaNs\n",
        "    df = pd.read_csv(input_csv_file)\n",
        "    print(f\"Successfully loaded data from '{input_csv_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{input_csv_file}' not found.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calibration Process ---\n",
        "print(\"Starting calibration...\")\n",
        "\n",
        "# Create new columns for calibrated data\n",
        "for col_name in columns_to_calibrate:\n",
        "    if col_name in df.columns:\n",
        "        # Ensure the column is numeric. Attempt conversion if necessary.\n",
        "        try:\n",
        "            # Attempt to convert to numeric, coercing errors to NaN\n",
        "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
        "        except Exception as e:\n",
        "            print(f\" - Warning: Could not convert column '{col_name}' to numeric: {e}. Skipping calibration.\")\n",
        "            df[f'f_{col_name}'] = np.nan # Use 'f_' prefix for fuzzy set columns\n",
        "            continue # Skip to the next column\n",
        "\n",
        "        # Perform calibration using the custom function\n",
        "        calibrated_data = calibrate_min_mean_max(df[col_name], col_name)\n",
        "\n",
        "        # Store the calibrated data in a new column (use 'f_' prefix for fuzzy set)\n",
        "        df[f'f_{col_name}'] = calibrated_data\n",
        "\n",
        "    else:\n",
        "        print(f\" - Warning: Column '{col_name}' not found in the dataset. Skipping.\")\n",
        "        df[f'f_{col_name}'] = np.nan # Add NaN column if skipped\n",
        "\n",
        "\n",
        "print(\"Calibration complete.\")\n",
        "print(\"Added fuzzy set columns (e.g., 'f_Perceived_opportunities', 'f_Entrepreneurial_intentions') to the DataFrame.\")\n",
        "\n",
        "\n",
        "# --- Save Calibrated Data to Excel ---\n",
        "try:\n",
        "    # Use ExcelWriter to specify the engine for potentially larger files\n",
        "    with pd.ExcelWriter(output_excel_file, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    print(f\"Successfully saved calibrated data to '{output_excel_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to Excel: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJi5jNIpujr0",
        "outputId": "dcd78aaa-0c96-4208-97c2-788b9370da66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Input file '/Users/henryefeonomakpo/Downloads/1_SE_Prep/PLS_SEM_Calibrated_Output.csv' not found.\n",
            "Starting calibration...\n",
            "    Calibrating 'Perceived_opportunities' with points (0, 0.5, 1): (7.27, 48.64, 95.38)\n",
            "    Calibrating 'Perceived_capabilities' with points (0, 0.5, 1): (10.05, 54.67, 92.63)\n",
            "    Calibrating 'Fear_failure_rate' with points (0, 0.5, 1): (7.14, 39.55, 75.42)\n",
            "    Calibrating 'Entrepreneurial_intentions' with points (0, 0.5, 1): (2.12, 22.81, 83.00)\n",
            "    Calibrating 'Entrepreneurial_TEA' with points (0, 0.5, 1): (1.56, 13.12, 49.60)\n",
            "    Calibrating 'Established_Business_Ownership' with points (0, 0.5, 1): (1.25, 7.86, 35.94)\n",
            "    Calibrating 'Entrepreneurial_Employee_Activity' with points (0, 0.5, 1): (0.00, 2.52, 11.47)\n",
            "    Calibrating 'Motivational_Index' with points (0, 0.5, 1): (0.00, 1.43, 19.50)\n",
            "    Calibrating 'Female_Male_TEA' with points (0, 0.5, 1): (0.25, 0.72, 1.34)\n",
            "    Calibrating 'Female_Male_Opportunity_Driven_TEA' with points (0, 0.5, 1): (0.00, 0.48, 1.18)\n",
            "    Calibrating 'High_Job_Creation_Expectation' with points (0, 0.5, 1): (0.50, 22.40, 76.74)\n",
            "    Calibrating 'Innovation' with points (0, 0.5, 1): (0.00, 13.34, 58.70)\n",
            "    Calibrating 'Business_Services_Sector' with points (0, 0.5, 1): (0.30, 18.58, 46.84)\n",
            "    Calibrating 'High_Status_Successful_Entrepreneurs' with points (0, 0.5, 1): (0.00, 70.45, 96.73)\n",
            "    Calibrating 'Entrepreneurship_Good_Career_Choice' with points (0, 0.5, 1): (0.00, 63.75, 96.55)\n",
            "Calibration complete.\n",
            "Added fuzzy set columns (e.g., 'f_Perceived_opportunities', 'f_Entrepreneurial_intentions') to the DataFrame.\n",
            "Successfully saved calibrated data to 'FSQCA_Calibrated_Data.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SE -ESG Panel Regression"
      ],
      "metadata": {
        "id": "Fvsx_KuI0KPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#                               Complete Data Analysis Script\n",
        "#           Merging, Imputation, VIF, Panel Regression, ML Models, Visualization\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. Setup and Imports ---\n",
        "\n",
        "print(\"--- Starting Setup and Imports ---\")\n",
        "\n",
        "# Install necessary libraries\n",
        "# Use %pip instead of !pip in notebook environments for better integration\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    from linearmodels.panel import PanelOLS\n",
        "    from sklearn.experimental import enable_iterative_imputer # Required for IterativeImputer\n",
        "    from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import matplotlib.pyplot as plt # Import matplotlib.pyplot as plt\n",
        "    import seaborn as sns # Import seaborn as sns\n",
        "    import warnings\n",
        "    print(\"Required libraries already installed or successfully imported.\")\n",
        "except ImportError:\n",
        "    print(\"Installing required libraries...\")\n",
        "    %pip install pandas openpyxl statsmodels linearmodels scikit-learn xgboost matplotlib seaborn\n",
        "    # Re-import after installing\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    from linearmodels.panel import PanelOLS\n",
        "    from sklearn.experimental import enable_iterative_imputer # Required for IterativeImputer\n",
        "    from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    print(\"Libraries installed and imported.\")\n",
        "\n",
        "\n",
        "# Suppress warnings for cleaner output in the notebook (optional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"--- Setup and Imports Complete ---\")\n",
        "\n",
        "# --- 2. Load and Merge Data ---\n",
        "\n",
        "print(\"\\n--- Starting Data Loading and Merging ---\")\n",
        "\n",
        "# --- File Paths ---\n",
        "# Adjust these paths if your files are in Google Drive or a different location\n",
        "pls_sem_file = '/content/PLS_SEM_SE_Pre.csv'\n",
        "esg_panel_file = '/content/ESG_panel_processed.csv'\n",
        "\n",
        "# Initialize df variables to None\n",
        "df_pls = None\n",
        "df_esg = None\n",
        "\n",
        "# --- Load Datasets ---\n",
        "try:\n",
        "    df_pls = pd.read_csv(pls_sem_file)\n",
        "    print(f\"Successfully loaded data from '{pls_sem_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{pls_sem_file}' not found. Make sure it's uploaded or the path is correct.\")\n",
        "    # No exit here, let's try loading the second file anyway, maybe only one is needed for some steps\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during loading '{pls_sem_file}': {e}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    df_esg = pd.read_csv(esg_panel_file)\n",
        "    print(f\"Successfully loaded data from '{esg_panel_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{esg_panel_file}' not found. Make sure it's uploaded or the path is correct.\")\n",
        "    # No exit here\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during loading '{esg_panel_file}': {e}\")\n",
        "\n",
        "# --- Check if both DataFrames were loaded successfully ---\n",
        "if df_pls is None or df_esg is None:\n",
        "    print(\"\\n--- Data Loading Failed ---\")\n",
        "    print(\"Cannot proceed with merging and analysis as one or both input files failed to load.\")\n",
        "    # Skip subsequent steps by using a flag or exiting more cleanly\n",
        "    data_loading_successful = False\n",
        "else:\n",
        "    data_loading_successful = True\n",
        "    print(\"Both input files loaded successfully. Proceeding with merging.\")\n",
        "\n",
        "    # --- Clean Column Names (Remove leading/trailing whitespace) ---\n",
        "    df_pls.columns = df_pls.columns.str.strip()\n",
        "    df_esg.columns = df_esg.columns.str.strip()\n",
        "    print(\"Cleaned column names.\")\n",
        "\n",
        "    # --- Merge Datasets ---\n",
        "    # Assuming 'Country' and 'Year' are the common columns for merging\n",
        "    # Use an inner merge to keep only country-year pairs present in both datasets\n",
        "    identifier_cols = ['Country', 'Year']\n",
        "    # Check if identifier columns exist in both dataframes before merging\n",
        "    if all(col in df_pls.columns for col in identifier_cols) and all(col in df_esg.columns for col in identifier_cols):\n",
        "        merged_df = pd.merge(df_pls, df_esg, on=identifier_cols, how='inner')\n",
        "        print(f\"Merged data shape: {merged_df.shape}\")\n",
        "        # print(f\"Merged data columns: {merged_df.columns.tolist()}\") # Optional: Print all columns\n",
        "\n",
        "        # --- Identify Potential Predictors and Target ---\n",
        "        # For simplicity, let's use Entrepreneurial_intentions as the target for now.\n",
        "        # You can change this or loop through multiple targets if needed.\n",
        "        target_variable = 'Entrepreneurial_intentions' # Example target - ENSURE THIS IS CORRECT\n",
        "\n",
        "        all_cols = merged_df.columns.tolist()\n",
        "\n",
        "        # Attempt to convert all potentially numeric columns (excluding identifiers and the target)\n",
        "        # This helps identify non-numeric columns that might cause errors later\n",
        "        potential_predictor_cols = [col for col in all_cols if col not in identifier_cols + [target_variable]]\n",
        "\n",
        "        for col in potential_predictor_cols:\n",
        "            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
        "\n",
        "        # Define predictor columns based on those that are now numeric and were in the potential list\n",
        "        predictor_cols = [col for col in potential_predictor_cols if pd.api.types.is_numeric_dtype(merged_df[col])]\n",
        "\n",
        "        # Check if target variable exists and is numeric\n",
        "        if target_variable not in merged_df.columns:\n",
        "            print(f\"Error: Target variable '{target_variable}' not found in the merged dataset.\")\n",
        "            data_loading_successful = False # Set flag to false if target is missing\n",
        "        else:\n",
        "            merged_df[target_variable] = pd.to_numeric(merged_df[target_variable], errors='coerce')\n",
        "            if not pd.api.types.is_numeric_dtype(merged_df[target_variable]):\n",
        "                 print(f\"Error: Target variable '{target_variable}' is not numeric after conversion attempt.\")\n",
        "                 data_loading_successful = False # Set flag to false if target is not numeric\n",
        "\n",
        "\n",
        "        if data_loading_successful:\n",
        "            # Drop rows where the target variable is missing, as imputation is typically for predictors\n",
        "            merged_df.dropna(subset=[target_variable], inplace=True)\n",
        "            print(f\"Shape after dropping rows with missing target: {merged_df.shape}\")\n",
        "\n",
        "            # Select only the relevant columns for the analysis\n",
        "            # Ensure all predictor_cols are actually in the DataFrame after potential dropping\n",
        "            analysis_df = merged_df[identifier_cols + [target_variable] + [col for col in predictor_cols if col in merged_df.columns]].copy()\n",
        "            # Update predictor_cols list to only include those actually in the analysis_df\n",
        "            predictor_cols = [col for col in analysis_df.columns if col not in identifier_cols + [target_variable]]\n",
        "\n",
        "            print(\"\\n--- Data Loading and Merging Complete ---\")\n",
        "            print(\"Analysis DataFrame Head:\")\n",
        "            print(analysis_df.head())\n",
        "            print(f\"\\nTarget Variable: {target_variable}\")\n",
        "            print(f\"Predictor Variables ({len(predictor_cols)}): {predictor_cols}\")\n",
        "\n",
        "        else:\n",
        "            # If target variable was missing or not numeric, stop analysis\n",
        "            print(\"\\n--- Data Loading and Merging Failed due to target variable ---\")\n",
        "            print(f\"Could not find or use the target variable: {target_variable}. Analysis stopped.\")\n",
        "            # analysis_df will not be created or used in subsequent steps\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: Identifier columns {identifier_cols} not found in one or both dataframes.\")\n",
        "        data_loading_successful = False\n",
        "\n",
        "\n",
        "# --- 3. Imputation (Handling Missing Data) ---\n",
        "\n",
        "# Proceed only if data loading was successful and there are columns to impute\n",
        "if data_loading_successful and (identifier_cols + [target_variable] + predictor_cols):\n",
        "    print(\"\\n--- Starting Imputation ---\")\n",
        "\n",
        "    # Select columns to impute (all analysis columns except identifiers)\n",
        "    cols_to_impute = [col for col in analysis_df.columns if col not in identifier_cols]\n",
        "\n",
        "    # Check initial NaN counts\n",
        "    initial_nan_counts = analysis_df[cols_to_impute].isnull().sum()\n",
        "    initial_nan_counts = initial_nan_counts[initial_nan_counts > 0]\n",
        "\n",
        "    if not initial_nan_counts.empty:\n",
        "        print(\"NaN counts before imputation:\")\n",
        "        print(initial_nan_counts)\n",
        "\n",
        "        # Initialize the imputer\n",
        "        # max_iter controls the number of imputation rounds\n",
        "        # random_state for reproducibility\n",
        "        # IterativeImputer is more advanced, SimpleImputer('mean') is simpler\n",
        "        imputer = IterativeImputer(max_iter=10, random_state=42) # Using a fixed random_state for reproducibility\n",
        "        # imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "        # Fit and transform the selected columns\n",
        "        # Use .values to work with the numpy array expected by imputer\n",
        "        try:\n",
        "            imputed_data = imputer.fit_transform(analysis_df[cols_to_impute])\n",
        "\n",
        "            # Put the imputed data back into the DataFrame\n",
        "            analysis_df[cols_to_impute] = imputed_data\n",
        "            print(\"Imputation complete.\")\n",
        "\n",
        "            # Check NaN counts after imputation\n",
        "            final_nan_counts = analysis_df[cols_to_impute].isnull().sum()\n",
        "            final_nan_counts = final_nan_counts[final_nan_counts > 0]\n",
        "            if not final_nan_counts.empty:\n",
        "                 print(\"Warning: NaNs still found after imputation:\")\n",
        "                 print(final_nan_counts)\n",
        "                 # Decide whether to continue or stop if NaNs remain\n",
        "                 # data_loading_successful = False # Uncomment to stop if NaNs remain\n",
        "            else:\n",
        "                print(\"No missing values found after imputation.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error during imputation: {e}\")\n",
        "             print(\"Imputation failed.\")\n",
        "             # If imputation fails, subsequent steps requiring clean data will likely fail\n",
        "             data_loading_successful = False # Set flag to false\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No missing values found in columns to impute. Imputation skipped.\")\n",
        "\n",
        "    print(\"--- Imputation Complete ---\")\n",
        "    # print(analysis_df.head()) # Optional: Check head after imputation\n",
        "else:\n",
        "    print(\"\\nSkipping Imputation as data loading failed or no columns to impute.\")\n",
        "\n",
        "\n",
        "# --- 4. VIF Analysis (Checking for Multicollinearity) ---\n",
        "\n",
        "# Proceed only if data loading and (optional) imputation were successful and there are predictors\n",
        "if data_loading_successful and predictor_cols:\n",
        "    print(\"\\n--- Starting VIF Analysis ---\")\n",
        "\n",
        "    # Select only the predictor columns for VIF\n",
        "    X_vif = analysis_df[predictor_cols].copy()\n",
        "\n",
        "    # Drop rows with NaN in X_vif - should be handled by imputation, but safety check\n",
        "    # X_vif.dropna(inplace=True) # Should ideally not drop rows if imputation worked\n",
        "\n",
        "    if X_vif.empty:\n",
        "         print(\"Warning: Predictor data is empty after handling NaNs. Skipping VIF.\")\n",
        "    else:\n",
        "        # Add a constant term for the intercept, required for VIF calculation\n",
        "        # Use has_constant='add' to avoid adding it multiple times if the code is re-run\n",
        "        X_vif = sm.add_constant(X_vif, has_constant='add')\n",
        "\n",
        "        # Ensure no NaNs remain in X_vif values used for VIF calculation\n",
        "        if np.isnan(X_vif.values).any():\n",
        "            print(\"Warning: NaNs detected in VIF input data after imputation. VIF calculation may fail or be inaccurate.\")\n",
        "            print(X_vif.isnull().sum()[X_vif.isnull().sum() > 0]) # Show which columns have NaNs\n",
        "            # Decide whether to continue or stop if NaNs remain\n",
        "            # data_loading_successful = False # Uncomment to stop if NaNs remain\n",
        "        else:\n",
        "            try:\n",
        "                # Calculate VIF for each predictor\n",
        "                vif_data = pd.DataFrame()\n",
        "                vif_data['Variable'] = X_vif.columns\n",
        "                # Use the values attribute for the numpy array needed by variance_inflation_factor\n",
        "                vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
        "\n",
        "                # Sort by VIF\n",
        "                vif_data = vif_data.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "                print(\"VIF Results:\")\n",
        "                print(vif_data)\n",
        "\n",
        "                # --- Interpretation ---\n",
        "                print(\"\\nVIF Interpretation:\")\n",
        "                # Exclude 'const' when interpreting VIFs for actual variables\n",
        "                high_vif_threshold = 10 # Common strict threshold\n",
        "                # high_vif_threshold = 5 # More lenient threshold\n",
        "\n",
        "                high_vif_vars = vif_data[(vif_data['VIF'] > high_vif_threshold) & (vif_data['Variable'] != 'const')]\n",
        "\n",
        "                if not high_vif_vars.empty:\n",
        "                    print(f\"Warning: The following variables have VIF > {high_vif_threshold}, indicating potential multicollinearity:\")\n",
        "                    print(high_vif_vars)\n",
        "                    print(\"Consider removing one or more of these highly correlated predictors, combining them, or using dimension reduction techniques.\")\n",
        "                else:\n",
        "                    print(f\"No variables found with VIF > {high_vif_threshold}. Multicollinearity does not appear to be a major issue based on VIF.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during VIF calculation: {e}\")\n",
        "                print(\"This might happen if there are still NaNs or perfect multicollinearity.\")\n",
        "                # Decide whether to continue or stop if VIF calculation failed\n",
        "                # data_loading_successful = False # Uncomment to stop\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping VIF analysis as data loading/imputation failed or no predictor variables were identified.\")\n",
        "\n",
        "print(\"--- VIF Analysis Complete ---\")\n",
        "\n",
        "\n",
        "# --- 5. Panel Data Regression (PanelOLS) ---\n",
        "\n",
        "# Proceed only if data loading/imputation was successful, there are predictors, and the target is available\n",
        "if data_loading_successful and predictor_cols and target_variable in analysis_df.columns and not analysis_df[target_variable].isnull().any():\n",
        "    print(\"\\n--- Starting Panel Data Regression (PanelOLS) ---\")\n",
        "\n",
        "    # Create a MultiIndex using 'Country' and 'Year' for linearmodels\n",
        "    # Use the imputed data DataFrame\n",
        "    panel_df = analysis_df.set_index(identifier_cols)\n",
        "\n",
        "    # Define the dependent (target) and independent (predictor) variables\n",
        "    y_panel = panel_df[target_variable]\n",
        "    X_panel = panel_df[predictor_cols].copy() # Use the same predictors as VIF\n",
        "\n",
        "    # Add a constant for the intercept\n",
        "    # Use has_constant='add' to avoid adding it multiple times if the code is re-run\n",
        "    X_panel = sm.add_constant(X_panel, has_constant='add')\n",
        "\n",
        "    # Ensure no NaNs remain in PanelOLS input - Should be handled by imputation\n",
        "    if np.isnan(X_panel.values).any() or np.isnan(y_panel.values).any():\n",
        "         print(\"Warning: NaNs detected in PanelOLS input data. This should not happen after successful imputation and dropping missing target rows.\")\n",
        "         print(\"Skipping PanelOLS.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Fit the PanelOLS model\n",
        "            # entity_effects=True: Fixed effects for Country\n",
        "            # time_effects=False: No fixed effects for Year (change to True if needed)\n",
        "            # Other options: `model='random'` for Random Effects (check linearmodels docs)\n",
        "            # Note: entity_effects=True requires variations WITHIN entities over time for predictors\n",
        "            # If running Random Effects, use model='random' instead of default 'within' (fixed effects)\n",
        "            panel_model = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=False)\n",
        "            # panel_model = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=True) # Fixed Entity and Time Effects\n",
        "            # panel_model = PanelOLS(y_panel, X_panel, entity_effects=False, time_effects=False, model='random') # Random Effects\n",
        "\n",
        "            panel_results = panel_model.fit()\n",
        "\n",
        "            # Print the results summary\n",
        "            print(\"\\nPanelOLS Results Summary:\")\n",
        "            print(panel_results)\n",
        "\n",
        "            # --- Interpretation ---\n",
        "            print(\"\\nPanelOLS Interpretation Notes:\")\n",
        "            print(f\"- Dependent Variable: {target_variable}\")\n",
        "            print(\"- Coefficients: Show the estimated effect of each predictor on the target, controlling for entity fixed effects.\")\n",
        "            print(\"- P-values (P>|t|): Indicate statistical significance (typically < 0.05).\")\n",
        "            print(\"- R-squared (Within): Measures the proportion of within-entity variance explained.\")\n",
        "            print(\"- F-statistic (Within): Tests the overall significance of the predictors in explaining within-entity variation.\")\n",
        "            if panel_model.entity_effects:\n",
        "                print(\"- Entity Effects: Account for unobserved, time-invariant country-specific factors.\")\n",
        "            if panel_model.time_effects:\n",
        "                print(\"- Time Effects: Account for unobserved, entity-invariant year-specific factors.\")\n",
        "            # Reliability of coefficients depends on significance, but overall model validity relies on global fit (not directly assessed here for PanelOLS in the same way as PLS-SEM).\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError fitting PanelOLS model: {e}\")\n",
        "            print(\"Check the model specification, data (e.g., insufficient within-entity variation for fixed effects), and error messages.\")\n",
        "            # Decide whether to continue or stop if PanelOLS failed\n",
        "            # data_loading_successful = False # Uncomment to stop\n",
        "\n",
        "\n",
        "    # Reset index for subsequent analyses if needed (ML models don't use MultiIndex)\n",
        "    analysis_df = analysis_df.reset_index()\n",
        "\n",
        "else:\n",
        "     print(\"\\nSkipping Panel Data Regression as data loading/imputation failed, predictors/target missing, or data contains NaNs.\")\n",
        "\n",
        "\n",
        "print(\"--- Panel Data Regression Complete ---\")\n",
        "\n",
        "\n",
        "# --- 6. Machine Learning Models (Random Forest and XGBoost) ---\n",
        "\n",
        "# Proceed only if data loading/imputation was successful and there are predictors and the target is available\n",
        "if data_loading_successful and predictor_cols and target_variable in analysis_df.columns and not analysis_df[target_variable].isnull().any():\n",
        "    print(\"\\n--- Starting Machine Learning Models ---\")\n",
        "\n",
        "    # Define features (X) and target (y) for ML models\n",
        "    # Use the imputed data DataFrame\n",
        "    X_ml = analysis_df[predictor_cols].select_dtypes(include=np.number).copy()\n",
        "    y_ml = analysis_df[target_variable].select_dtypes(include=np.number).copy()\n",
        "\n",
        "    # Ensure no NaNs remain in X or y - Should be handled by imputation\n",
        "    if np.isnan(X_ml.values).any() or np.isnan(y_ml.values).any():\n",
        "         print(\"Warning: NaNs detected in ML input data after imputation. This should not happen after successful imputation and dropping missing target rows.\")\n",
        "         print(\"Skipping ML models.\")\n",
        "         # Decide whether to continue or stop if NaNs remain\n",
        "         # data_loading_successful = False # Uncomment to stop if NaNs remain\n",
        "         y_test_df = None # Clear variables to skip visualization\n",
        "         rf_feature_importance = None\n",
        "         xgb_feature_importance = None\n",
        "\n",
        "    else:\n",
        "        # Split data into training and testing sets\n",
        "        # test_size: proportion of the dataset to include in the test split\n",
        "        # random_state: for reproducibility\n",
        "        try:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.25, random_state=42) # 75% train, 25% test\n",
        "            print(f\"\\nData split: Train ({X_train.shape[0]} samples), Test ({X_test.shape[0]} samples)\")\n",
        "\n",
        "            # --- Random Forest Regressor ---\n",
        "            print(\"\\nFitting Random Forest Regressor...\")\n",
        "            # Initialize and train the model\n",
        "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "            rf_model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False) # squared=False gives RMSE\n",
        "            mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "            r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "            print(f\"Random Forest - RMSE: {rmse_rf:.2f}\")\n",
        "            print(f\"Random Forest - MAE: {mae_rf:.2f}\")\n",
        "            print(f\"Random Forest - R-squared: {r2_rf:.2f}\")\n",
        "\n",
        "            # Get Feature Importance\n",
        "            rf_feature_importance = pd.DataFrame({\n",
        "                'Feature': X_ml.columns,\n",
        "                'Importance': rf_model.feature_importances_\n",
        "            }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "            # --- XGBoost Regressor ---\n",
        "            print(\"\\nFitting XGBoost Regressor...\")\n",
        "            # Initialize and train the model\n",
        "            # objective='reg:squarederror' for regression\n",
        "            # n_estimators: number of boosting rounds\n",
        "            # learning_rate: step size shrinkage\n",
        "            # random_state: for reproducibility\n",
        "            xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
        "            xgb_model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False) # squared=False gives RMSE\n",
        "            mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "            r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "            print(f\"XGBoost - RMSE: {rmse_xgb:.2f}\")\n",
        "            print(f\"XGBoost - MAE: {mae_xgb:.2f}\")\n",
        "            print(f\"XGBoost - R-squared: {r2_xgb:.2f}\")\n",
        "\n",
        "            # Get Feature Importance\n",
        "            xgb_feature_importance = pd.DataFrame({\n",
        "                'Feature': X_ml.columns,\n",
        "                'Importance': xgb_model.feature_importances_\n",
        "            }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "            # --- Store predictions for visualization ---\n",
        "            # Create a DataFrame for visualization data to link predictions back to original indices if needed\n",
        "            # For simplicity here, we'll just use the test set indices\n",
        "            y_test_df = y_test.to_frame(name='Actual')\n",
        "            y_test_df['Predicted_RF'] = y_pred_rf\n",
        "            y_test_df['Predicted_XGB'] = y_pred_xgb\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during ML model fitting or splitting: {e}\")\n",
        "            print(\"Skipping ML results visualization.\")\n",
        "            # Clear variables to skip visualization step\n",
        "            y_test_df = None\n",
        "            rf_feature_importance = None\n",
        "            xgb_feature_importance = None\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Machine Learning models as data loading/imputation failed, predictors/target missing, or data contains NaNs.\")\n",
        "    # Clear variables to skip visualization step\n",
        "    y_test_df = None\n",
        "    rf_feature_importance = None\n",
        "    xgb_feature_importance = None\n",
        "\n",
        "\n",
        "print(\"--- Machine Learning Models Complete ---\")\n",
        "\n",
        "\n",
        "# --- 7. Visualization ---\n",
        "\n",
        "# Proceed only if data loading/imputation was successful and analysis_df is not empty\n",
        "if data_loading_successful and not analysis_df.empty:\n",
        "    print(\"\\n--- Generating visualizations ---\")\n",
        "\n",
        "    try:\n",
        "        # Plot distributions of target and a few predictors\n",
        "        # Select top N predictors by variance or mean/median for visualization diversity\n",
        "        analysis_df_numeric = analysis_df[predictor_cols + [target_variable]].select_dtypes(include=np.number) # Include target here for variance check\n",
        "        if not analysis_df_numeric.empty:\n",
        "            # Calculate variance only if there's more than one row\n",
        "            if analysis_df_numeric.shape[0] > 1:\n",
        "                col_variances = analysis_df_numeric.var().sort_values(ascending=False)\n",
        "                # Exclude target from top predictor selection if it was included in variance calculation\n",
        "                top_predictor_cols_viz = col_variances[col_variances.index != target_variable].head(min(3, len(col_variances)-1)).index.tolist() # Top 3 or fewer predictors\n",
        "            else:\n",
        "                top_predictor_cols_viz = [] # Cannot calculate variance with 1 or fewer rows\n",
        "\n",
        "            num_plots = 1 + len(top_predictor_cols_viz) # Target + selected predictors\n",
        "            if num_plots > 0:\n",
        "                plt.figure(figsize=(min(15, num_plots * 5), 5)) # Adjust figure width based on number of plots\n",
        "\n",
        "                # Target distribution\n",
        "                ax1 = plt.subplot(1, num_plots, 1)\n",
        "                sns.histplot(analysis_df[target_variable].dropna(), kde=True, ax=ax1) # Plot non-NaN data for distribution\n",
        "                ax1.set_title(f'Distribution of\\n{target_variable}')\n",
        "\n",
        "                # Top predictors distributions\n",
        "                for i, col in enumerate(top_predictor_cols_viz):\n",
        "                    ax = plt.subplot(1, num_plots, i+2)\n",
        "                    sns.histplot(analysis_df[col].dropna(), kde=True, ax=ax) # Plot non-NaN data\n",
        "                    ax.set_title(f'Distribution of\\n{col}')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"Not enough numeric data for distribution plots.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No numeric data available for distribution plots.\")\n",
        "\n",
        "\n",
        "        # Plot Correlation Heatmap (select a subset of columns if too many)\n",
        "        # Select top N predictors by correlation with the target for heatmap\n",
        "        analysis_df_numeric_all = analysis_df[predictor_cols + [target_variable]].select_dtypes(include=np.number)\n",
        "        if not analysis_df_numeric_all.empty and analysis_df_numeric_all.shape[0] > 1:\n",
        "             # Calculate correlations, drop NaNs first for this calculation\n",
        "             corr_matrix_calc = analysis_df_numeric_all.dropna().corr()\n",
        "             # Handle case where correlation cannot be calculated (e.g., all values same)\n",
        "             if not corr_matrix_calc.empty and target_variable in corr_matrix_calc.columns:\n",
        "                correlation_with_target = corr_matrix_calc[target_variable].abs().sort_values(ascending=False)\n",
        "                # Exclude target itself from top predictor selection\n",
        "                top_corr_predictor_cols = correlation_with_target[correlation_with_target.index != target_variable].head(min(15, len(correlation_with_target)-1)).index.tolist() # Select top 15 or fewer\n",
        "\n",
        "                if top_corr_predictor_cols: # Ensure list is not empty\n",
        "                    plt.figure(figsize=(12, 10))\n",
        "                    # Use the correlation matrix calculated from non-NaN data\n",
        "                    sns.heatmap(corr_matrix_calc.loc[top_corr_predictor_cols + [target_variable], top_corr_predictor_cols + [target_variable]],\n",
        "                                annot=False, cmap='coolwarm') # Set annot=True if you want to see values\n",
        "                    plt.title('Correlation Heatmap (Top Predictors)')\n",
        "                    plt.show()\n",
        "                else:\n",
        "                     print(\"Not enough varied numeric predictor data available for correlation heatmap.\")\n",
        "             else:\n",
        "                 print(\"Correlation calculation failed for heatmap (check for constant columns).\")\n",
        "        else:\n",
        "             print(\"No numeric data available for correlation heatmap.\")\n",
        "\n",
        "\n",
        "        # Plot Predictions vs Actuals for ML Models (only if models were fitted and results stored)\n",
        "        if 'y_test_df' in locals() and y_test_df is not None and not y_test_df.empty:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            # Plot for Random Forest if prediction column exists\n",
        "            if 'Predicted_RF' in y_test_df.columns:\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.scatter(y_test_df['Actual'], y_test_df['Predicted_RF'], alpha=0.5)\n",
        "                # Add a perfect prediction line\n",
        "                min_val = y_test_df['Actual'].min()\n",
        "                max_val = y_test_df['Actual'].max()\n",
        "                plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2) # Diagonal line\n",
        "                plt.xlabel(\"Actual\")\n",
        "                plt.ylabel(\"Predicted (Random Forest)\")\n",
        "                plt.title(\"Random Forest: Actual vs Predicted\")\n",
        "                plt.grid(True)\n",
        "            else:\n",
        "                plt.subplot(1, 2, 1).set_title('Random Forest Predictions N/A')\n",
        "\n",
        "\n",
        "            # Plot for XGBoost if prediction column exists\n",
        "            if 'Predicted_XGB' in y_test_df.columns:\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.scatter(y_test_df['Actual'], y_test_df['Predicted_XGB'], alpha=0.5)\n",
        "                # Add a perfect prediction line\n",
        "                min_val = y_test_df['Actual'].min()\n",
        "                max_val = y_test_df['Actual'].max()\n",
        "                plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2) # Diagonal line\n",
        "                plt.xlabel(\"Actual\")\n",
        "                plt.ylabel(\"Predicted (XGBoost)\")\n",
        "                plt.title(\"XGBoost: Actual vs Predicted\")\n",
        "                plt.grid(True)\n",
        "            else:\n",
        "                 plt.subplot(1, 2, 2).set_title('XGBoost Predictions N/A')\n",
        "\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Skipping Prediction vs Actuals plots (ML models not fitted or test data unavailable).\")\n",
        "\n",
        "\n",
        "        # Plot Feature Importance (only if models were fitted and importance stored)\n",
        "        if 'rf_feature_importance' in locals() and rf_feature_importance is not None or ('xgb_feature_importance' in locals() and xgb_feature_importance is not None):\n",
        "             # Check if either DataFrame is non-empty before plotting\n",
        "             if (rf_feature_importance is not None and not rf_feature_importance.empty) or (xgb_feature_importance is not None and not xgb_feature_importance.empty):\n",
        "                plt.figure(figsize=(12, 6))\n",
        "\n",
        "                if rf_feature_importance is not None and not rf_feature_importance.empty:\n",
        "                    plt.subplot(1, 2, 1)\n",
        "                    sns.barplot(x='Importance', y='Feature', data=rf_feature_importance.head(10)) # Top 10 features\n",
        "                    plt.title('Random Forest Top 10 Feature Importance')\n",
        "                else:\n",
        "                    plt.subplot(1, 2, 1).set_title('Random Forest Feature Importance N/A')\n",
        "\n",
        "\n",
        "                if xgb_feature_importance is not None and not xgb_feature_importance.empty:\n",
        "                    plt.subplot(1, 2, 2)\n",
        "                    sns.barplot(x='Importance', y='Feature', data=xgb_feature_importance.head(10)) # Top 10 features\n",
        "                    plt.title('XGBoost Top 10 Feature Importance')\n",
        "                else:\n",
        "                    plt.subplot(1, 2, 2).set_title('XGBoost Feature Importance N/A')\n",
        "\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "             else:\n",
        "                print(\"Skipping Feature Importance plots (No feature importance data available).\")\n",
        "\n",
        "        else:\n",
        "             print(\"Skipping Feature Importance plots (ML models not fitted).\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during visualization: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print traceback for debugging visualization errors\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping visualizations as data loading/imputation failed or analysis data is empty.\")\n",
        "\n",
        "\n",
        "print(\"--- Analysis and Visualization Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4zvJSywp0Sjq",
        "outputId": "b2dd1638-05cf-4ea9-81f8-0037a453f287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Setup and Imports ---\n",
            "Required libraries already installed or successfully imported.\n",
            "--- Setup and Imports Complete ---\n",
            "\n",
            "--- Starting Data Loading and Merging ---\n",
            "Successfully loaded data from '/content/PLS_SEM_SE_Pre.csv'.\n",
            "Successfully loaded data from '/content/ESG_panel_processed.csv'.\n",
            "Both input files loaded successfully. Proceeding with merging.\n",
            "Cleaned column names.\n",
            "Merged data shape: (312, 51)\n",
            "Shape after dropping rows with missing target: (312, 51)\n",
            "\n",
            "--- Data Loading and Merging Complete ---\n",
            "Analysis DataFrame Head:\n",
            "   Country  Year  Entrepreneurial_intentions  Perceived_opportunities  \\\n",
            "0   Brazil  2023                       48.71                    65.37   \n",
            "1   Canada  2023                       14.26                    62.57   \n",
            "2    Chile  2023                       53.11                    59.42   \n",
            "3    China  2023                        5.58                    69.21   \n",
            "4  Croatia  2023                       21.64                    64.09   \n",
            "\n",
            "   Perceived_capabilities  Fear_failure_rate  Entrepreneurial_TEA  \\\n",
            "0                   65.89              46.90                18.62   \n",
            "1                   56.66              54.61                19.76   \n",
            "2                   75.67              40.92                31.05   \n",
            "3                   55.77              64.54                 6.82   \n",
            "4                   73.60              45.90                13.15   \n",
            "\n",
            "   Established_Business_Ownership  Entrepreneurial_Employee_Activity  \\\n",
            "0                           11.87                                0.0   \n",
            "1                            7.77                                0.0   \n",
            "2                            5.29                                0.0   \n",
            "3                            4.17                                0.0   \n",
            "4                            5.19                                0.0   \n",
            "\n",
            "   Motivational_Index  ...  population_density_people_per_sq_km_land_area  \\\n",
            "0                 0.0  ...                                            0.0   \n",
            "1                 0.0  ...                                            0.0   \n",
            "2                 0.0  ...                                            0.0   \n",
            "3                 0.0  ...                                            0.0   \n",
            "4                 0.0  ...                                            0.0   \n",
            "\n",
            "   poverty_headcount_ratio_at_national_poverty_lines_population  \\\n",
            "0                                                0.0              \n",
            "1                                                0.0              \n",
            "2                                                0.0              \n",
            "3                                                0.0              \n",
            "4                                                0.0              \n",
            "\n",
            "   proportion_bodies_water_with_good_ambient_water_quality  \\\n",
            "0                                                0.0         \n",
            "1                                                0.0         \n",
            "2                                                0.0         \n",
            "3                                                0.0         \n",
            "4                                                0.0         \n",
            "\n",
            "   ratio_female_to_male_labor_force_participation_rate_modeled_ilo_estimate  \\\n",
            "0                                              72.80                          \n",
            "1                                              88.20                          \n",
            "2                                              72.80                          \n",
            "3                                              83.95                          \n",
            "4                                              83.12                          \n",
            "\n",
            "   renewable_electricity_output_total_electricity_output  \\\n",
            "0                                                0.0       \n",
            "1                                                0.0       \n",
            "2                                                0.0       \n",
            "3                                                0.0       \n",
            "4                                                0.0       \n",
            "\n",
            "   renewable_energy_consumption_total_final_energy_consumption  \\\n",
            "0                                                0.0             \n",
            "1                                                0.0             \n",
            "2                                                0.0             \n",
            "3                                                0.0             \n",
            "4                                                0.0             \n",
            "\n",
            "   research_and_development_expenditure_gdp  rule_law_estimate  \\\n",
            "0                                       0.0                0.0   \n",
            "1                                       0.0                0.0   \n",
            "2                                       0.0                0.0   \n",
            "3                                       0.0                0.0   \n",
            "4                                       0.0                0.0   \n",
            "\n",
            "   school_enrollment_primary_and_secondary_gross_gender_parity_index_gpi  \\\n",
            "0                                                0.0                       \n",
            "1                                                0.0                       \n",
            "2                                                0.0                       \n",
            "3                                                0.0                       \n",
            "4                                                0.0                       \n",
            "\n",
            "   voice_and_accountability_estimate  \n",
            "0                                0.0  \n",
            "1                                0.0  \n",
            "2                                0.0  \n",
            "3                                0.0  \n",
            "4                                0.0  \n",
            "\n",
            "[5 rows x 51 columns]\n",
            "\n",
            "Target Variable: Entrepreneurial_intentions\n",
            "Predictor Variables (48): ['Perceived_opportunities', 'Perceived_capabilities', 'Fear_failure_rate', 'Entrepreneurial_TEA', 'Established_Business_Ownership', 'Entrepreneurial_Employee_Activity', 'Motivational_Index', 'Female_Male_TEA', 'Female_Male_Opportunity_Driven_TEA', 'High_Job_Creation_Expectation', 'Innovation', 'Business_Services_Sector', 'High_Status_Successful_Entrepreneurs', 'Entrepreneurship_Good_Career_Choice', 'coastal_protection', 'control_corruption_estimate', 'economic_and_social_rights_performance_score', 'electricity_production_from_coal_sources_total', 'energy_imports_net_energy_use', 'energy_intensity_level_primary_energy_mj_2017_ppp_gdp', 'energy_use_kg_oil_equivalent_per_capita', 'fertility_rate_total_births_per_woman', 'food_production_index_2014_2016_100', 'fossil_fuel_energy_consumption_total', 'gdp_growth_annual', 'gini_index', 'government_expenditure_on_education_total_government_expenditure', 'hospital_beds_per_1_000_people', 'income_share_held_by_lowest_20', 'individuals_using_the_internet_population', 'land_surface_temperature', 'level_water_stress_freshwater_withdrawal_as_a_proportion_available_freshwater_resources', 'life_expectancy_at_birth_total_years', 'literacy_rate_adult_total_people_ages_15_and_above', 'people_using_safely_managed_drinking_water_services_population', 'people_using_safely_managed_sanitation_services_population', 'political_stability_and_absence_violence_terrorism_estimate', 'population_ages_65_and_above_total_population', 'population_density_people_per_sq_km_land_area', 'poverty_headcount_ratio_at_national_poverty_lines_population', 'proportion_bodies_water_with_good_ambient_water_quality', 'ratio_female_to_male_labor_force_participation_rate_modeled_ilo_estimate', 'renewable_electricity_output_total_electricity_output', 'renewable_energy_consumption_total_final_energy_consumption', 'research_and_development_expenditure_gdp', 'rule_law_estimate', 'school_enrollment_primary_and_secondary_gross_gender_parity_index_gpi', 'voice_and_accountability_estimate']\n",
            "\n",
            "--- Starting Imputation ---\n",
            "No missing values found in columns to impute. Imputation skipped.\n",
            "--- Imputation Complete ---\n",
            "\n",
            "--- Starting VIF Analysis ---\n",
            "VIF Results:\n",
            "                                             Variable         VIF\n",
            "0                                               const  374.076643\n",
            "33               life_expectancy_at_birth_total_years   65.927089\n",
            "17       economic_and_social_rights_performance_score   45.291301\n",
            "46                                  rule_law_estimate   45.203942\n",
            "9                  Female_Male_Opportunity_Driven_TEA   37.861495\n",
            "16                        control_corruption_estimate   35.739170\n",
            "22              fertility_rate_total_births_per_woman   26.004601\n",
            "38      population_ages_65_and_above_total_population   25.115465\n",
            "48                  voice_and_accountability_estimate   10.247776\n",
            "11                                         Innovation    8.609834\n",
            "47  school_enrollment_primary_and_secondary_gross_...    7.657057\n",
            "36  people_using_safely_managed_sanitation_service...    7.656167\n",
            "29                     income_share_held_by_lowest_20    7.571295\n",
            "37  political_stability_and_absence_violence_terro...    7.465349\n",
            "26                                         gini_index    6.530498\n",
            "24               fossil_fuel_energy_consumption_total    6.436509\n",
            "23                food_production_index_2014_2016_100    6.339582\n",
            "35  people_using_safely_managed_drinking_water_ser...    5.691881\n",
            "2                              Perceived_capabilities    5.496983\n",
            "6                   Entrepreneurial_Employee_Activity    5.429451\n",
            "42  ratio_female_to_male_labor_force_participation...    5.311370\n",
            "30          individuals_using_the_internet_population    4.994244\n",
            "21            energy_use_kg_oil_equivalent_per_capita    4.879389\n",
            "31                           land_surface_temperature    4.828733\n",
            "45           research_and_development_expenditure_gdp    4.615597\n",
            "44  renewable_energy_consumption_total_final_energ...    4.302823\n",
            "32  level_water_stress_freshwater_withdrawal_as_a_...    4.270803\n",
            "12                           Business_Services_Sector    4.237312\n",
            "1                             Perceived_opportunities    3.934426\n",
            "20  energy_intensity_level_primary_energy_mj_2017_...    3.902099\n",
            "27  government_expenditure_on_education_total_gove...    3.803499\n",
            "15                                 coastal_protection    3.776107\n",
            "28                     hospital_beds_per_1_000_people    3.731230\n",
            "4                                 Entrepreneurial_TEA    3.642544\n",
            "7                                  Motivational_Index    3.552573\n",
            "39      population_density_people_per_sq_km_land_area    3.444451\n",
            "14                Entrepreneurship_Good_Career_Choice    3.408174\n",
            "13               High_Status_Successful_Entrepreneurs    2.808804\n",
            "43  renewable_electricity_output_total_electricity...    2.760200\n",
            "18     electricity_production_from_coal_sources_total    2.549289\n",
            "40  poverty_headcount_ratio_at_national_poverty_li...    2.425618\n",
            "25                                  gdp_growth_annual    2.306042\n",
            "8                                     Female_Male_TEA    2.121195\n",
            "10                      High_Job_Creation_Expectation    2.119159\n",
            "34  literacy_rate_adult_total_people_ages_15_and_a...    1.889972\n",
            "5                      Established_Business_Ownership    1.673720\n",
            "3                                   Fear_failure_rate    1.651318\n",
            "19                      energy_imports_net_energy_use    1.651071\n",
            "41  proportion_bodies_water_with_good_ambient_wate...    1.331471\n",
            "\n",
            "VIF Interpretation:\n",
            "Warning: The following variables have VIF > 10, indicating potential multicollinearity:\n",
            "                                         Variable        VIF\n",
            "33           life_expectancy_at_birth_total_years  65.927089\n",
            "17   economic_and_social_rights_performance_score  45.291301\n",
            "46                              rule_law_estimate  45.203942\n",
            "9              Female_Male_Opportunity_Driven_TEA  37.861495\n",
            "16                    control_corruption_estimate  35.739170\n",
            "22          fertility_rate_total_births_per_woman  26.004601\n",
            "38  population_ages_65_and_above_total_population  25.115465\n",
            "48              voice_and_accountability_estimate  10.247776\n",
            "Consider removing one or more of these highly correlated predictors, combining them, or using dimension reduction techniques.\n",
            "--- VIF Analysis Complete ---\n",
            "\n",
            "--- Starting Panel Data Regression (PanelOLS) ---\n",
            "\n",
            "PanelOLS Results Summary:\n",
            "                              PanelOLS Estimation Summary                               \n",
            "========================================================================================\n",
            "Dep. Variable:     Entrepreneurial_intentions   R-squared:                        0.5431\n",
            "Estimator:                           PanelOLS   R-squared (Between):              0.0918\n",
            "No. Observations:                         312   R-squared (Within):               0.5431\n",
            "Date:                        Thu, May 01 2025   R-squared (Overall):              0.1660\n",
            "Time:                                14:07:06   Log-likelihood                   -801.11\n",
            "Cov. Estimator:                    Unadjusted                                           \n",
            "                                                F-statistic:                      5.4983\n",
            "Entities:                                  42   P-value                           0.0000\n",
            "Avg Obs:                               7.4286   Distribution:                  F(48,222)\n",
            "Min Obs:                               1.0000                                           \n",
            "Max Obs:                              10.0000   F-statistic (robust):             5.4983\n",
            "                                                P-value                           0.0000\n",
            "Time periods:                              10   Distribution:                  F(48,222)\n",
            "Avg Obs:                               31.200                                           \n",
            "Min Obs:                               25.000                                           \n",
            "Max Obs:                               37.000                                           \n",
            "                                                                                        \n",
            "                                                                    Parameter Estimates                                                                    \n",
            "===========================================================================================================================================================\n",
            "                                                                                         Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "const                                                                                      -32.828     17.676    -1.8573     0.0646     -67.662      2.0054\n",
            "Perceived_opportunities                                                                     0.0276     0.0345     0.7999     0.4246     -0.0404      0.0955\n",
            "Perceived_capabilities                                                                      0.2196     0.0555     3.9558     0.0001      0.1102      0.3290\n",
            "Fear_failure_rate                                                                          -0.0983     0.0460    -2.1362     0.0338     -0.1890     -0.0076\n",
            "Entrepreneurial_TEA                                                                         0.6744     0.1167     5.7775     0.0000      0.4444      0.9045\n",
            "Established_Business_Ownership                                                             -0.3812     0.1523    -2.5037     0.0130     -0.6813     -0.0812\n",
            "Entrepreneurial_Employee_Activity                                                           0.2115     0.2055     1.0291     0.3045     -0.1935      0.6165\n",
            "Motivational_Index                                                                         -0.2446     0.1904    -1.2848     0.2002     -0.6199      0.1306\n",
            "Female_Male_TEA                                                                            -3.8430     2.1182    -1.8143     0.0710     -8.0174      0.3313\n",
            "Female_Male_Opportunity_Driven_TEA                                                          5.1371     3.0928     1.6610     0.0981     -0.9579      11.232\n",
            "High_Job_Creation_Expectation                                                               0.2515     0.0394     6.3873     0.0000      0.1739      0.3291\n",
            "Innovation                                                                                 -0.0483     0.0449    -1.0758     0.2832     -0.1367      0.0402\n",
            "Business_Services_Sector                                                                    0.1052     0.0528     1.9932     0.0475      0.0012      0.2092\n",
            "High_Status_Successful_Entrepreneurs                                                        0.1067     0.0338     3.1529     0.0018      0.0400      0.1734\n",
            "Entrepreneurship_Good_Career_Choice                                                        -0.0651     0.0351    -1.8569     0.0647     -0.1343      0.0040\n",
            "coastal_protection                                                                         -0.0281     0.0159    -1.7633     0.0792     -0.0595      0.0033\n",
            "control_corruption_estimate                                                                -3.3853     2.1654    -1.5633     0.1194     -7.6527      0.8821\n",
            "economic_and_social_rights_performance_score                                               -0.8906     1.3607    -0.6545     0.5135     -3.5721      1.7910\n",
            "electricity_production_from_coal_sources_total                                             -0.0153     0.0227    -0.6750     0.5004     -0.0600      0.0294\n",
            "energy_imports_net_energy_use                                                              -0.0073     0.0056    -1.2852     0.2001     -0.0184      0.0039\n",
            "energy_intensity_level_primary_energy_mj_2017_ppp_gdp                                       0.6873     0.3455     1.9895     0.0479      0.0065      1.3681\n",
            "energy_use_kg_oil_equivalent_per_capita                                                    -0.0003     0.0003    -1.0686     0.2864     -0.0010      0.0003\n",
            "fertility_rate_total_births_per_woman                                                       2.0661     1.8375     1.1244     0.2620     -1.5550      5.6872\n",
            "food_production_index_2014_2016_100                                                        -0.0284     0.0235    -1.2059     0.2291     -0.0748      0.0180\n",
            "fossil_fuel_energy_consumption_total                                                        0.0114     0.0196     0.5790     0.5632     -0.0273      0.0501\n",
            "gdp_growth_annual                                                                          -0.0140     0.1035    -0.1354     0.8924     -0.2179      0.1899\n",
            "gini_index                                                                                  0.0420     0.0492     0.8549     0.3935     -0.0548      0.1389\n",
            "government_expenditure_on_education_total_government_expenditure                            0.0173     0.0849     0.2035     0.8389     -0.1500      0.1845\n",
            "hospital_beds_per_1_000_people                                                              0.1571     0.1847     0.8507     0.3958     -0.2069      0.5212\n",
            "income_share_held_by_lowest_20                                                             -0.1406     0.2690    -0.5229     0.6016     -0.6707      0.3894\n",
            "individuals_using_the_internet_population                                                  -0.0282     0.0184    -1.5267     0.1283     -0.0645      0.0082\n",
            "land_surface_temperature                                                                   -0.0460     0.0631    -0.7287     0.4669     -0.1703      0.0784\n",
            "level_water_stress_freshwater_withdrawal_as_a_proportion_available_freshwater_resources     0.0004     0.0052     0.0778     0.9381     -0.0099      0.0107\n",
            "life_expectancy_at_birth_total_years                                                        0.0131     0.0704     0.1859     0.8527     -0.1257      0.1519\n",
            "literacy_rate_adult_total_people_ages_15_and_above                                         -0.0167     0.0096    -1.7458     0.0822     -0.0356      0.0022\n",
            "people_using_safely_managed_drinking_water_services_population                              0.0079     0.0326     0.2436     0.8078     -0.0563      0.0722\n",
            "people_using_safely_managed_sanitation_services_population                                  0.0523     0.0324     1.6153     0.1077     -0.0115      0.1161\n",
            "political_stability_and_absence_violence_terrorism_estimate                                 0.0959     1.4645     0.0655     0.9479     -2.7902      2.9820\n",
            "population_ages_65_and_above_total_population                                              -0.0121     0.1802    -0.0671     0.9466     -0.3672      0.3430\n",
            "population_density_people_per_sq_km_land_area                                               0.0053     0.0055     0.9627     0.3368     -0.0055      0.0161\n",
            "poverty_headcount_ratio_at_national_poverty_lines_population                               -0.0756     0.0471    -1.6031     0.1103     -0.1685      0.0173\n",
            "proportion_bodies_water_with_good_ambient_water_quality                                    -0.0034     0.0118    -0.2909     0.7714     -0.0268      0.0199\n",
            "ratio_female_to_male_labor_force_participation_rate_modeled_ilo_estimate                    0.3363     0.2214     1.5190     0.1302     -0.1000      0.7725\n",
            "renewable_electricity_output_total_electricity_output                                       0.0205     0.0203     1.0084     0.3144     -0.0195      0.0605\n",
            "renewable_energy_consumption_total_final_energy_consumption                                -0.0626     0.0432    -1.4481     0.1490     -0.1477      0.0226\n",
            "research_and_development_expenditure_gdp                                                   -0.3033     0.4913    -0.6172     0.5377     -1.2715      0.6650\n",
            "rule_law_estimate                                                                           2.2197     2.8158     0.7883     0.4314     -3.3293      7.7688\n",
            "school_enrollment_primary_and_secondary_gross_gender_parity_index_gpi                      -1.9220     1.3899    -1.3828     0.1681     -4.6609      0.8170\n",
            "voice_and_accountability_estimate                                                          -0.3468     1.7943    -0.1933     0.8469     -3.8827      3.1892\n",
            "===========================================================================================================================================================\n",
            "\n",
            "F-test for Poolability: 8.9918\n",
            "P-value: 0.0000\n",
            "Distribution: F(41,222)\n",
            "\n",
            "Included effects: Entity\n",
            "\n",
            "PanelOLS Interpretation Notes:\n",
            "- Dependent Variable: Entrepreneurial_intentions\n",
            "- Coefficients: Show the estimated effect of each predictor on the target, controlling for entity fixed effects.\n",
            "- P-values (P>|t|): Indicate statistical significance (typically < 0.05).\n",
            "- R-squared (Within): Measures the proportion of within-entity variance explained.\n",
            "- F-statistic (Within): Tests the overall significance of the predictors in explaining within-entity variation.\n",
            "- Entity Effects: Account for unobserved, time-invariant country-specific factors.\n",
            "--- Panel Data Regression Complete ---\n",
            "\n",
            "--- Starting Machine Learning Models ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'select_dtypes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a45a039de499>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# Use the imputed data DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mX_ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictor_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0my_ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_variable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# Ensure no NaNs remain in X or y - Should be handled by imputation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'select_dtypes'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge dataset"
      ],
      "metadata": {
        "id": "0gwiYYDLda1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv # Import csv for potential quoting options later if needed\n",
        "\n",
        "print(\"Script started...\")\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    # --- TRY LOADING WB DATA WITH COMMA FIRST ---\n",
        "    # The error suggests the WB file might not be tab-separated. Let's try comma.\n",
        "    try:\n",
        "        print(\"Attempting to load New_Entrepreneur_WB.csv with comma separator...\")\n",
        "        # Try comma separator\n",
        "        df_wb = pd.read_csv('New_Entrepreneur_WB.csv', sep=',', engine='python')\n",
        "        print(\"Successfully loaded New_Entrepreneur_WB.csv using comma separator.\")\n",
        "    except Exception as e_comma:\n",
        "        print(f\"Comma separator failed for WB data: {e_comma}\")\n",
        "        print(\"Attempting to load New_Entrepreneur_WB.csv with tab separator as fallback...\")\n",
        "        # Fallback to tab separator if comma fails\n",
        "        df_wb = pd.read_csv('New_Entrepreneur_WB.csv', sep='\\t', engine='python')\n",
        "        print(\"Successfully loaded New_Entrepreneur_WB.csv using tab separator.\")\n",
        "    # --- END OF WB LOADING ATTEMPTS ---\n",
        "\n",
        "    print(f\"WB data shape: {df_wb.shape}\")\n",
        "    # print(\"WB data head:\\n\", df_wb.head()) # Uncomment for debugging if needed\n",
        "\n",
        "    # Load PLS-SEM data (Assuming Comma-separated - this seems correct based on no error here)\n",
        "    df_pls = pd.read_csv('PLS_SEM_SE_Pre.csv', sep=',', engine='python')\n",
        "    print(\"Successfully loaded PLS_SEM_SE_Pre.csv\")\n",
        "    print(f\"PLS data shape: {df_pls.shape}\")\n",
        "    # print(\"PLS data head:\\n\", df_pls.head()) # Uncomment for debugging if needed\n",
        "\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: File not found: {e}\")\n",
        "    print(\"Please ensure 'New_Entrepreneur_WB.csv' and 'PLS_SEM_SE_Pre.csv' are in the same directory as the script.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    # This catches errors from the fallback tab attempt or PLS loading\n",
        "    print(f\"An critical error occurred during file loading: {e}\")\n",
        "    print(\"Please check the format (delimiters, quoting) of your CSV files, especially 'New_Entrepreneur_WB.csv'.\")\n",
        "    exit() # Exit if loading fails fundamentally\n",
        "\n",
        "# --- 2. Initial Cleaning and Standardization ---\n",
        "\n",
        "# Function to clean column names (lowercase, replace spaces/special chars with _)\n",
        "def clean_col_names(df):\n",
        "    cols = df.columns\n",
        "    new_cols = []\n",
        "    for col in cols:\n",
        "        # Remove leading/trailing whitespace\n",
        "        col = col.strip()\n",
        "        # Replace spaces and special characters (except _) with underscore\n",
        "        col = re.sub(r'[^A-Za-z0-9_]+', '_', col)\n",
        "        # Make lowercase\n",
        "        col = col.lower()\n",
        "        # Handle potential multiple underscores\n",
        "        col = re.sub(r'_+', '_', col)\n",
        "        # Remove leading/trailing underscores\n",
        "        col = col.strip('_')\n",
        "        new_cols.append(col)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "print(\"\\nCleaning column names...\")\n",
        "# Now df_wb should be defined if loading succeeded with either separator\n",
        "df_wb = clean_col_names(df_wb)\n",
        "df_pls = clean_col_names(df_pls)\n",
        "print(\"Column names cleaned.\")\n",
        "# print(\"Cleaned WB columns:\", df_wb.columns.tolist())\n",
        "# print(\"Cleaned PLS columns:\", df_pls.columns.tolist())\n",
        "\n",
        "\n",
        "# --- Cleaning World Bank Data (df_wb) ---\n",
        "print(\"\\nCleaning World Bank data...\")\n",
        "\n",
        "# Standardize Country names\n",
        "country_replacements_wb = {\n",
        "    r'\\*|\\xa0': '',  # Remove asterisks and non-breaking spaces (often appear from copy-paste)\n",
        "    r'\\s+$': '', # Remove trailing spaces\n",
        "    r'^\\s+': '', # Remove leading spaces\n",
        "    'Antigua and Barbuda': 'Antigua and Barbuda', # Example: Ensure consistency if needed later\n",
        "    'Korea, Rep.': 'South Korea',\n",
        "    'China\\*\\*\\*': 'China', # Handle specific cases if needed after general cleaning\n",
        "    'Egypt, Arab Rep.': 'Egypt',\n",
        "    'Hong Kong SAR, China': 'Hong Kong',\n",
        "    'Iran, Islamic Rep.': 'Iran',\n",
        "    'North Macedonia, Rep': 'North Macedonia',\n",
        "    'Russian Federation': 'Russia',\n",
        "    'Slovak Republic': 'Slovakia',\n",
        "    'St.': 'St', # Abbreviate St. consistently if needed\n",
        "    'Syrian Arab Republic': 'Syria',\n",
        "    'Türkiye': 'Turkey',\n",
        "    'United States': 'United States', # Ensure consistent naming\n",
        "    'Puerto Rico, US': 'Puerto Rico',\n",
        "    'Venezuela, RB': 'Venezuela',\n",
        "    'Yemen, Rep.': 'Yemen',\n",
        "    'Viet Nam': 'Vietnam',\n",
        "    'Congo, Dem. Rep.': 'Congo, DR', # Use consistent abbreviations if desired\n",
        "    'Congo, Rep.': 'Congo',\n",
        "    'Central African Republic': 'Central African Republic', #Keep long names if consistent\n",
        "    'Dominican Republic': 'Dominican Republic',\n",
        "    'São Tomé and Príncipe': 'Sao Tome and Principe', # Remove accents for easier matching\n",
        "    'Côte d’Ivoire': 'Cote d\\'Ivoire',\n",
        "    'Kyrgyz Republic': 'Kyrgyzstan' # Common name\n",
        "}\n",
        "# Ensure 'country' column exists before trying to replace\n",
        "if 'country' in df_wb.columns:\n",
        "    df_wb['country'] = df_wb['country'].astype(str).replace(country_replacements_wb, regex=True).str.strip()\n",
        "    print(\"WB Country names standardized.\")\n",
        "else:\n",
        "    print(\"Warning: 'country' column not found in df_wb. Cannot standardize country names.\")\n",
        "\n",
        "\n",
        "# Clean numeric columns (remove commas, convert to numeric)\n",
        "numeric_cols_wb = ['adult_population', 'number_entrepreneur_llc']\n",
        "for col in numeric_cols_wb:\n",
        "    if col in df_wb.columns:\n",
        "        if df_wb[col].dtype == 'object':\n",
        "            print(f\"Cleaning numeric column (WB): {col}\")\n",
        "            # Replace comma and any non-numeric character except '.' and '-' before coercion\n",
        "            df_wb[col] = df_wb[col].astype(str).str.replace(',', '', regex=False)\n",
        "            # Added cleaning for potential spaces within numbers\n",
        "            df_wb[col] = df_wb[col].astype(str).str.replace(r'\\s+', '', regex=True)\n",
        "            df_wb[col] = pd.to_numeric(df_wb[col], errors='coerce')\n",
        "        elif pd.api.types.is_numeric_dtype(df_wb[col]):\n",
        "             print(f\"Column {col} is already numeric.\")\n",
        "        else:\n",
        "             print(f\"Column {col} is not object or numeric, type: {df_wb[col].dtype}. Skipping numeric cleaning.\")\n",
        "    else:\n",
        "        print(f\"Warning: Expected numeric column '{col}' not found in WB data.\")\n",
        "\n",
        "\n",
        "# Convert 'new_business_density_rate' to numeric\n",
        "if 'new_business_density_rate' in df_wb.columns:\n",
        "    print(\"Converting 'new_business_density_rate' to numeric...\")\n",
        "    df_wb['new_business_density_rate'] = pd.to_numeric(df_wb['new_business_density_rate'], errors='coerce')\n",
        "else:\n",
        "     print(\"Warning: Column 'new_business_density_rate' not found in WB data.\")\n",
        "\n",
        "# Convert 'year' to numeric (integer)\n",
        "if 'year' in df_wb.columns:\n",
        "    print(\"Converting 'year' to integer...\")\n",
        "    df_wb['year'] = pd.to_numeric(df_wb['year'], errors='coerce').astype('Int64') # Use nullable integer\n",
        "else:\n",
        "    print(\"Warning: Column 'year' not found in WB data.\")\n",
        "\n",
        "print(\"WB data cleaning finished.\")\n",
        "# print(\"WB data info after cleaning:\\n\")\n",
        "# df_wb.info()\n",
        "# print(\"WB data head after cleaning:\\n\", df_wb.head())\n",
        "# print(\"WB Missing values after cleaning:\\n\", df_wb.isnull().sum())\n",
        "\n",
        "\n",
        "# --- Cleaning PLS-SEM Data (df_pls) ---\n",
        "print(\"\\nCleaning PLS-SEM data...\")\n",
        "\n",
        "# Standardize Country names (ensure they match WB format after cleaning)\n",
        "country_replacements_pls = {\n",
        "    r'\\*|\\xa0': '',  # Remove asterisks and non-breaking spaces\n",
        "    r'\\s+$': '', # Remove trailing spaces\n",
        "    r'^\\s+': '', # Remove leading spaces\n",
        "    'South Korea': 'South Korea', # Already standardized above\n",
        "    'United States': 'United States',\n",
        "    'Puerto Rico': 'Puerto Rico',\n",
        "     'Iran, Islamic Rep.': 'Iran',\n",
        "     'Korea, Rep.': 'South Korea', # Just in case it appears here too\n",
        "     'Viet Nam': 'Vietnam',\n",
        "     'São Tomé and Príncipe': 'Sao Tome and Principe',\n",
        "     'Côte d’Ivoire': 'Cote d\\'Ivoire',\n",
        "     'Kyrgyz Republic': 'Kyrgyzstan'\n",
        "     # Add more replacements if needed based on inspection\n",
        "}\n",
        "# Ensure 'country' column exists\n",
        "if 'country' in df_pls.columns:\n",
        "    df_pls['country'] = df_pls['country'].astype(str).replace(country_replacements_pls, regex=True).str.strip()\n",
        "    print(\"PLS Country names standardized.\")\n",
        "else:\n",
        "    print(\"Warning: 'country' column not found in df_pls. Cannot standardize country names.\")\n",
        "\n",
        "\n",
        "# Convert 'year' to numeric (integer)\n",
        "if 'year' in df_pls.columns:\n",
        "    print(\"Converting 'year' to integer...\")\n",
        "    df_pls['year'] = pd.to_numeric(df_pls['year'], errors='coerce').astype('Int64')\n",
        "else:\n",
        "    print(\"Warning: Column 'year' not found in PLS data.\")\n",
        "\n",
        "\n",
        "# Ensure all other PLS columns are numeric (except country and year)\n",
        "print(\"Converting PLS columns to numeric...\")\n",
        "pls_numeric_cols = [col for col in df_pls.columns if col not in ['country', 'year']]\n",
        "for col in pls_numeric_cols:\n",
        "     if col in df_pls.columns:\n",
        "        if df_pls[col].dtype == 'object': # Only convert if it's currently an object type\n",
        "             print(f\"Converting PLS column to numeric: {col}\")\n",
        "             df_pls[col] = pd.to_numeric(df_pls[col], errors='coerce')\n",
        "        elif pd.api.types.is_numeric_dtype(df_pls[col]):\n",
        "            # Already numeric, do nothing\n",
        "            pass\n",
        "        else:\n",
        "            print(f\"PLS column {col} has non-numeric, non-object type: {df_pls[col].dtype}. Skipping conversion.\")\n",
        "     else:\n",
        "          print(f\"Warning: Expected numeric column '{col}' not found in PLS data during numeric conversion check.\")\n",
        "\n",
        "\n",
        "print(\"PLS data cleaning finished.\")\n",
        "# print(\"PLS data info after cleaning:\\n\")\n",
        "# df_pls.info()\n",
        "# print(\"PLS data head after cleaning:\\n\", df_pls.head())\n",
        "# print(\"PLS Missing values after cleaning:\\n\", df_pls.isnull().sum())\n",
        "\n",
        "\n",
        "# --- 3. Merge Data ---\n",
        "print(\"\\nMerging datasets...\")\n",
        "# Check if essential columns exist before merging\n",
        "if 'country' not in df_wb.columns or 'year' not in df_wb.columns:\n",
        "    print(\"Error: Cannot merge because 'country' or 'year' column is missing from WB data after cleaning.\")\n",
        "    exit()\n",
        "if 'country' not in df_pls.columns or 'year' not in df_pls.columns:\n",
        "    print(\"Error: Cannot merge because 'country' or 'year' column is missing from PLS data after cleaning.\")\n",
        "    exit()\n",
        "\n",
        "# Use an outer merge to keep all rows initially, identify gaps\n",
        "merged_df = pd.merge(df_wb, df_pls, on=['country', 'year'], how='outer', suffixes=('_wb', '_pls'))\n",
        "print(f\"Merged data shape: {merged_df.shape}\")\n",
        "# print(\"Merged data info:\\n\")\n",
        "# merged_df.info()\n",
        "# print(\"Merged data head:\\n\", merged_df.head())\n",
        "\n",
        "# --- 4. Imputation ---\n",
        "print(\"\\nHandling missing values (Imputation)...\")\n",
        "print(\"Missing values BEFORE imputation:\\n\", merged_df.isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "# Identify all numeric columns in the merged dataframe\n",
        "numeric_cols_merged = merged_df.select_dtypes(include=np.number).columns.tolist()\n",
        "# Exclude 'year' if it's considered numeric but shouldn't be imputed\n",
        "if 'year' in numeric_cols_merged:\n",
        "    numeric_cols_merged.remove('year')\n",
        "\n",
        "print(f\"\\nNumeric columns identified for potential imputation: {len(numeric_cols_merged)}\")\n",
        "\n",
        "# Simple Imputation Strategy: Fill NaN in numeric columns with the column's median\n",
        "# Note: This is a basic strategy. More sophisticated methods (like group-wise median,\n",
        "# KNNImputer, IterativeImputer) might be better depending on the analysis needs.\n",
        "imputed_count = 0\n",
        "for col in numeric_cols_merged:\n",
        "    if merged_df[col].isnull().any():\n",
        "        median_val = merged_df[col].median()\n",
        "        # Handle case where median might be NaN (if all values were NaN)\n",
        "        if pd.isna(median_val):\n",
        "            median_val = 0 # Default to 0 if median cannot be calculated\n",
        "            print(f\"Warning: Could not calculate median for column '{col}'. Imputing with 0.\")\n",
        "        merged_df[col].fillna(median_val, inplace=True)\n",
        "        # print(f\"Imputed column '{col}' with median value: {median_val:.2f}\") # Reduce verbosity\n",
        "        imputed_count += 1\n",
        "\n",
        "if imputed_count == 0:\n",
        "    print(\"No missing numeric values found to impute.\")\n",
        "else:\n",
        "     print(f\"\\nImputed {imputed_count} numeric columns with their respective medians (or 0 if median failed).\")\n",
        "\n",
        "print(\"\\nMissing values AFTER imputation (numeric columns):\\n\", merged_df[numeric_cols_merged].isnull().sum())\n",
        "\n",
        "# Handle potential remaining non-numeric NaNs (like 'country' if outer merge didn't match)\n",
        "# For fsQCA/PLS-SEM, rows with missing Country/Year are usually unusable.\n",
        "initial_rows = merged_df.shape[0]\n",
        "merged_df.dropna(subset=['country', 'year'], inplace=True)\n",
        "rows_dropped = initial_rows - merged_df.shape[0]\n",
        "if rows_dropped > 0:\n",
        "    print(f\"\\nDropped {rows_dropped} rows with missing 'country' or 'year' (likely due to unmatched merge).\")\n",
        "\n",
        "# --- 5. Final Preparation (Data Types) ---\n",
        "# Ensure year is integer type after potential merge/imputation effects\n",
        "if 'year' in merged_df.columns:\n",
        "     merged_df['year'] = merged_df['year'].astype(int) # Convert back to standard int if no NaNs remain\n",
        "\n",
        "# Ensure other numeric columns have appropriate float types\n",
        "for col in numeric_cols_merged: # Re-iterate over originally identified numeric cols\n",
        "    if col in merged_df.columns: # Check if column still exists\n",
        "         if not pd.api.types.is_float_dtype(merged_df[col]) and not pd.api.types.is_integer_dtype(merged_df[col]):\n",
        "              print(f\"Converting column {col} to float.\")\n",
        "              # Try converting to float, coercing errors if any value became non-numeric somehow\n",
        "              merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
        "              if merged_df[col].isnull().any():\n",
        "                   print(f\"Warning: Column {col} introduced NaNs during final float conversion. Re-imputing with 0.\")\n",
        "                   # Handle unexpected NaNs (e.g., fill with 0 or re-impute median)\n",
        "                   merged_df[col].fillna(0, inplace=True) # Or use .median() again\n",
        "\n",
        "print(\"\\nFinal data types check:\")\n",
        "merged_df.info()\n",
        "\n",
        "# Sort for better readability (optional)\n",
        "merged_df.sort_values(by=['country', 'year'], inplace=True)\n",
        "\n",
        "# --- 6. Save Prepared Data ---\n",
        "output_base_filename = 'merged_entrepreneur_data_prepared'\n",
        "excel_filename = f'{output_base_filename}.xlsx'\n",
        "csv_filename = f'{output_base_filename}.csv'\n",
        "\n",
        "print(f\"\\nSaving prepared data to {excel_filename} and {csv_filename}...\")\n",
        "try:\n",
        "    merged_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "    merged_df.to_csv(csv_filename, index=False)\n",
        "    print(\"Files saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during file saving: {e}\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_70tz_-deX1",
        "outputId": "603acbb3-4823-4227-f556-5018f6a5dd0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script started...\n",
            "Attempting to load New_Entrepreneur_WB.csv with comma separator...\n",
            "Successfully loaded New_Entrepreneur_WB.csv using comma separator.\n",
            "WB data shape: (2365, 5)\n",
            "Successfully loaded PLS_SEM_SE_Pre.csv\n",
            "PLS data shape: (582, 17)\n",
            "\n",
            "Cleaning column names...\n",
            "Column names cleaned.\n",
            "\n",
            "Cleaning World Bank data...\n",
            "WB Country names standardized.\n",
            "Cleaning numeric column (WB): adult_population\n",
            "Cleaning numeric column (WB): number_entrepreneur_llc\n",
            "Converting 'new_business_density_rate' to numeric...\n",
            "Converting 'year' to integer...\n",
            "WB data cleaning finished.\n",
            "\n",
            "Cleaning PLS-SEM data...\n",
            "PLS Country names standardized.\n",
            "Converting 'year' to integer...\n",
            "Converting PLS columns to numeric...\n",
            "PLS data cleaning finished.\n",
            "\n",
            "Merging datasets...\n",
            "Merged data shape: (2551, 20)\n",
            "\n",
            "Handling missing values (Imputation)...\n",
            "Missing values BEFORE imputation:\n",
            " entrepreneurial_intentions              1969\n",
            "fear_failure_rate                       1969\n",
            "perceived_capabilities                  1969\n",
            "perceived_opportunities                 1969\n",
            "innovation                              1969\n",
            "female_male_tea                         1969\n",
            "female_male_opportunity_driven_tea      1969\n",
            "high_job_creation_expectation           1969\n",
            "established_business_ownership          1969\n",
            "entrepreneurial_tea                     1969\n",
            "entrepreneurial_employee_activity       1969\n",
            "motivational_index                      1969\n",
            "high_status_successful_entrepreneurs    1969\n",
            "business_services_sector                1969\n",
            "entrepreneurship_good_career_choice     1969\n",
            "number_entrepreneur_llc                  193\n",
            "new_business_density_rate                193\n",
            "adult_population                         190\n",
            "year                                       4\n",
            "country                                    0\n",
            "dtype: int64\n",
            "\n",
            "Numeric columns identified for potential imputation: 18\n",
            "\n",
            "Imputed 18 numeric columns with their respective medians (or 0 if median failed).\n",
            "\n",
            "Missing values AFTER imputation (numeric columns):\n",
            " adult_population                        0\n",
            "number_entrepreneur_llc                 0\n",
            "new_business_density_rate               0\n",
            "perceived_opportunities                 0\n",
            "perceived_capabilities                  0\n",
            "fear_failure_rate                       0\n",
            "entrepreneurial_intentions              0\n",
            "entrepreneurial_tea                     0\n",
            "established_business_ownership          0\n",
            "entrepreneurial_employee_activity       0\n",
            "motivational_index                      0\n",
            "female_male_tea                         0\n",
            "female_male_opportunity_driven_tea      0\n",
            "high_job_creation_expectation           0\n",
            "innovation                              0\n",
            "business_services_sector                0\n",
            "high_status_successful_entrepreneurs    0\n",
            "entrepreneurship_good_career_choice     0\n",
            "dtype: int64\n",
            "\n",
            "Dropped 4 rows with missing 'country' or 'year' (likely due to unmatched merge).\n",
            "\n",
            "Final data types check:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b78bf17539d6>:255: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2547 entries, 0 to 2546\n",
            "Data columns (total 20 columns):\n",
            " #   Column                                Non-Null Count  Dtype  \n",
            "---  ------                                --------------  -----  \n",
            " 0   country                               2547 non-null   object \n",
            " 1   year                                  2547 non-null   int64  \n",
            " 2   adult_population                      2547 non-null   float64\n",
            " 3   number_entrepreneur_llc               2547 non-null   float64\n",
            " 4   new_business_density_rate             2547 non-null   float64\n",
            " 5   perceived_opportunities               2547 non-null   float64\n",
            " 6   perceived_capabilities                2547 non-null   float64\n",
            " 7   fear_failure_rate                     2547 non-null   float64\n",
            " 8   entrepreneurial_intentions            2547 non-null   float64\n",
            " 9   entrepreneurial_tea                   2547 non-null   float64\n",
            " 10  established_business_ownership        2547 non-null   float64\n",
            " 11  entrepreneurial_employee_activity     2547 non-null   float64\n",
            " 12  motivational_index                    2547 non-null   float64\n",
            " 13  female_male_tea                       2547 non-null   float64\n",
            " 14  female_male_opportunity_driven_tea    2547 non-null   float64\n",
            " 15  high_job_creation_expectation         2547 non-null   float64\n",
            " 16  innovation                            2547 non-null   float64\n",
            " 17  business_services_sector              2547 non-null   float64\n",
            " 18  high_status_successful_entrepreneurs  2547 non-null   float64\n",
            " 19  entrepreneurship_good_career_choice   2547 non-null   float64\n",
            "dtypes: float64(18), int64(1), object(1)\n",
            "memory usage: 417.9+ KB\n",
            "\n",
            "Saving prepared data to merged_entrepreneur_data_prepared.xlsx and merged_entrepreneur_data_prepared.csv...\n",
            "Files saved successfully.\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to Prepare WB new Entrepreneurship LLC data and GEM for fsQCA"
      ],
      "metadata": {
        "id": "bmhJHIKuniVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Script started...\")\n",
        "\n",
        "# --- Configuration ---\n",
        "input_filename = 'merged_entrepreneur_data_prepared.csv'\n",
        "output_filename = 'calibrated_fsQCA_data.csv'\n",
        "\n",
        "# !!! CRITICAL: DEFINE YOUR CALIBRATION PARAMETERS HERE !!!\n",
        "# Structure: 'column_name_to_calibrate': (threshold_for_0, threshold_for_0.5, threshold_for_1)\n",
        "# Replace 'column_name_...' with actual columns from your merged file.\n",
        "# Replace the numeric tuples with YOUR anchors based on theory/evidence.\n",
        "# --- These are PLACEHOLDERS - YOU MUST CHANGE THEM ---\n",
        "calibration_params = {\n",
        "    # Example WB Variables (replace with actual names if needed & decide thresholds)\n",
        "    'adult_population': (100000, 10000000, 100000000), # Placeholder: e.g., <100k=0, 10M=0.5, >100M=1\n",
        "    'number_entrepreneur_llc': (100, 5000, 50000),      # Placeholder: e.g., <100=0, 5k=0.5, >50k=1\n",
        "    'new_business_density_rate': (0.1, 2.0, 10.0),     # Placeholder: e.g., <0.1=0, 2.0=0.5, >10=1\n",
        "\n",
        "    # Example PLS Variables (Use the cleaned names from the previous step)\n",
        "    # Add ALL conditions and the outcome variable you intend to use in fsQCA\n",
        "    'perceived_opportunities': (20, 50, 80),          # Placeholder: e.g., <20%=0, 50%=0.5, >80%=1\n",
        "    'perceived_capabilities': (30, 60, 85),           # Placeholder\n",
        "    'fear_failure_rate': (20, 40, 60),             # Placeholder - Note: High score might mean LOW fear conceptually for fsQCA\n",
        "    'entrepreneurial_intentions': (5, 20, 40),        # Placeholder\n",
        "    'entrepreneurial_tea': (3, 10, 25),               # Placeholder (Total Early-stage Entrepreneurial Activity)\n",
        "    'established_business_ownership': (2, 8, 15),   # Placeholder\n",
        "    'innovation': (10, 30, 50),                    # Placeholder (Assuming this is % innovation)\n",
        "    'high_status_successful_entrepreneurs': (50, 75, 90), # Placeholder\n",
        "    'entrepreneurship_good_career_choice': (50, 70, 85) # Placeholder\n",
        "\n",
        "    # --- ADD ALL OTHER VARIABLES (CONDITIONS & OUTCOME) YOU NEED FOR fsQCA HERE ---\n",
        "    # 'your_outcome_variable': (threshold_0, threshold_0.5, threshold_1),\n",
        "    # 'another_condition': (threshold_0, threshold_0.5, threshold_1),\n",
        "}\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "# --- Calibration Function (Direct Method) ---\n",
        "def calibrate_direct(series, thresh_0, thresh_05, thresh_1, col_name=\"\"):\n",
        "    \"\"\"\n",
        "    Calibrates a pandas Series using the direct method with 3 anchors.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The data series to calibrate.\n",
        "        thresh_0 (float): The threshold for full non-membership (score = 0).\n",
        "        thresh_05 (float): The threshold for the crossover point (score = 0.5).\n",
        "        thresh_1 (float): The threshold for full membership (score = 1).\n",
        "        col_name (str): Optional name of the column for warning messages.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The calibrated series with scores between 0 and 1.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not (thresh_0 < thresh_05 < thresh_1):\n",
        "        print(f\"Warning: Anchors for '{col_name}' are not strictly increasing ({thresh_0}, {thresh_05}, {thresh_1}). Calibration might be invalid.\")\n",
        "        # Decide how to handle: return NaNs, original data, or attempt anyway?\n",
        "        # Returning NaNs might be safest to flag the issue.\n",
        "        return pd.Series([np.nan] * len(series), index=series.index)\n",
        "\n",
        "    # Ensure denominators are not zero\n",
        "    denom_upper = thresh_1 - thresh_05\n",
        "    denom_lower = thresh_05 - thresh_0\n",
        "    if denom_upper == 0 or denom_lower == 0:\n",
        "         print(f\"Warning: Crossover anchor is identical to full membership or non-membership anchor for '{col_name}' ({thresh_0}, {thresh_05}, {thresh_1}). Cannot calibrate.\")\n",
        "         return pd.Series([np.nan] * len(series), index=series.index)\n",
        "\n",
        "\n",
        "    # Define conditions for np.select\n",
        "    conditions = [\n",
        "        series >= thresh_1,                       # Fully in (or above)\n",
        "        series <= thresh_0,                       # Fully out (or below)\n",
        "        (series >= thresh_05) & (series < thresh_1), # Above crossover, below full membership\n",
        "        (series > thresh_0) & (series < thresh_05)   # Below crossover, above full non-membership\n",
        "        # Note: Values exactly equal to thresh_05 will be handled by the third condition.\n",
        "    ]\n",
        "\n",
        "    # Define corresponding choices (calibration formulas)\n",
        "    choices = [\n",
        "        1.0,\n",
        "        0.0,\n",
        "        0.5 + 0.5 * (series - thresh_05) / denom_upper,\n",
        "        0.5 * (series - thresh_0) / denom_lower\n",
        "    ]\n",
        "\n",
        "    # Apply calibration using np.select\n",
        "    # Default is np.nan for values exactly equal to thresh_05 if condition logic had gaps,\n",
        "    # or for original NaNs, or unexpected values. Should handle original NaNs correctly.\n",
        "    calibrated_series = np.select(conditions, choices, default=np.nan)\n",
        "\n",
        "    # Convert the result back to a pandas Series with the original index\n",
        "    return pd.Series(calibrated_series, index=series.index)\n",
        "\n",
        "# --- Load Data ---\n",
        "print(f\"\\nLoading data from {input_filename}...\")\n",
        "try:\n",
        "    df = pd.read_csv(input_filename)\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file not found at {input_filename}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading {input_filename}: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Apply Calibration ---\n",
        "print(\"\\nStarting calibration process...\")\n",
        "calibrated_df = df.copy() # Work on a copy to keep original + calibrated\n",
        "\n",
        "for col, anchors in calibration_params.items():\n",
        "    if col in calibrated_df.columns:\n",
        "        print(f\"Calibrating column: '{col}' with anchors {anchors}\")\n",
        "        thresh_0, thresh_05, thresh_1 = anchors\n",
        "        calibrated_col_name = f\"{col}_cal\" # Create a new name for the calibrated column\n",
        "\n",
        "        # Perform calibration\n",
        "        calibrated_series = calibrate_direct(calibrated_df[col], thresh_0, thresh_05, thresh_1, col_name=col)\n",
        "\n",
        "        # Add the calibrated series to the dataframe\n",
        "        calibrated_df[calibrated_col_name] = calibrated_series\n",
        "\n",
        "        # Check for NaNs introduced during calibration (e.g., due to bad anchors)\n",
        "        if calibrated_series.isnull().any():\n",
        "             original_nan_count = calibrated_df[col].isnull().sum()\n",
        "             new_nan_count = calibrated_series.isnull().sum()\n",
        "             if new_nan_count > original_nan_count:\n",
        "                  print(f\"  -> Warning: NaNs were introduced or increased during calibration for '{col}'. Check anchors or original data.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' specified for calibration not found in the dataframe. Skipping.\")\n",
        "\n",
        "print(\"\\nCalibration process finished.\")\n",
        "\n",
        "# Display info about the new dataframe\n",
        "print(\"\\nCalibrated dataframe info:\")\n",
        "calibrated_df.info()\n",
        "# print(\"\\nFirst 5 rows of calibrated data (selected columns):\")\n",
        "# cols_to_show = ['country', 'year'] + list(calibration_params.keys()) + [f\"{c}_cal\" for c in calibration_params.keys() if c in df.columns]\n",
        "# print(calibrated_df[cols_to_show].head())\n",
        "\n",
        "# --- Save Calibrated Data ---\n",
        "print(f\"\\nSaving calibrated data to {output_filename}...\")\n",
        "try:\n",
        "    calibrated_df.to_csv(output_filename, index=False)\n",
        "    print(\"Calibrated file saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred saving the calibrated file: {e}\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA6jxaUAnxrQ",
        "outputId": "4e9c0ac6-14a6-4010-9af2-d4cf35bc0af9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script started...\n",
            "\n",
            "Loading data from merged_entrepreneur_data_prepared.csv...\n",
            "Data loaded successfully.\n",
            "Original shape: (2547, 20)\n",
            "\n",
            "Starting calibration process...\n",
            "Calibrating column: 'adult_population' with anchors (100000, 10000000, 100000000)\n",
            "Calibrating column: 'number_entrepreneur_llc' with anchors (100, 5000, 50000)\n",
            "Calibrating column: 'new_business_density_rate' with anchors (0.1, 2.0, 10.0)\n",
            "Calibrating column: 'perceived_opportunities' with anchors (20, 50, 80)\n",
            "Calibrating column: 'perceived_capabilities' with anchors (30, 60, 85)\n",
            "Calibrating column: 'fear_failure_rate' with anchors (20, 40, 60)\n",
            "Calibrating column: 'entrepreneurial_intentions' with anchors (5, 20, 40)\n",
            "Calibrating column: 'entrepreneurial_tea' with anchors (3, 10, 25)\n",
            "Calibrating column: 'established_business_ownership' with anchors (2, 8, 15)\n",
            "Calibrating column: 'innovation' with anchors (10, 30, 50)\n",
            "Calibrating column: 'high_status_successful_entrepreneurs' with anchors (50, 75, 90)\n",
            "Calibrating column: 'entrepreneurship_good_career_choice' with anchors (50, 70, 85)\n",
            "\n",
            "Calibration process finished.\n",
            "\n",
            "Calibrated dataframe info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2547 entries, 0 to 2546\n",
            "Data columns (total 32 columns):\n",
            " #   Column                                    Non-Null Count  Dtype  \n",
            "---  ------                                    --------------  -----  \n",
            " 0   country                                   2547 non-null   object \n",
            " 1   year                                      2547 non-null   int64  \n",
            " 2   adult_population                          2547 non-null   float64\n",
            " 3   number_entrepreneur_llc                   2547 non-null   float64\n",
            " 4   new_business_density_rate                 2547 non-null   float64\n",
            " 5   perceived_opportunities                   2547 non-null   float64\n",
            " 6   perceived_capabilities                    2547 non-null   float64\n",
            " 7   fear_failure_rate                         2547 non-null   float64\n",
            " 8   entrepreneurial_intentions                2547 non-null   float64\n",
            " 9   entrepreneurial_tea                       2547 non-null   float64\n",
            " 10  established_business_ownership            2547 non-null   float64\n",
            " 11  entrepreneurial_employee_activity         2547 non-null   float64\n",
            " 12  motivational_index                        2547 non-null   float64\n",
            " 13  female_male_tea                           2547 non-null   float64\n",
            " 14  female_male_opportunity_driven_tea        2547 non-null   float64\n",
            " 15  high_job_creation_expectation             2547 non-null   float64\n",
            " 16  innovation                                2547 non-null   float64\n",
            " 17  business_services_sector                  2547 non-null   float64\n",
            " 18  high_status_successful_entrepreneurs      2547 non-null   float64\n",
            " 19  entrepreneurship_good_career_choice       2547 non-null   float64\n",
            " 20  adult_population_cal                      2547 non-null   float64\n",
            " 21  number_entrepreneur_llc_cal               2547 non-null   float64\n",
            " 22  new_business_density_rate_cal             2547 non-null   float64\n",
            " 23  perceived_opportunities_cal               2547 non-null   float64\n",
            " 24  perceived_capabilities_cal                2547 non-null   float64\n",
            " 25  fear_failure_rate_cal                     2547 non-null   float64\n",
            " 26  entrepreneurial_intentions_cal            2547 non-null   float64\n",
            " 27  entrepreneurial_tea_cal                   2547 non-null   float64\n",
            " 28  established_business_ownership_cal        2547 non-null   float64\n",
            " 29  innovation_cal                            2547 non-null   float64\n",
            " 30  high_status_successful_entrepreneurs_cal  2547 non-null   float64\n",
            " 31  entrepreneurship_good_career_choice_cal   2547 non-null   float64\n",
            "dtypes: float64(30), int64(1), object(1)\n",
            "memory usage: 636.9+ KB\n",
            "\n",
            "Saving calibrated data to calibrated_fsQCA_data.csv...\n",
            "Calibrated file saved successfully.\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ]
}